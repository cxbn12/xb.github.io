https://juejin.cn/post/7593607642553892890

## 每周AI论文速递（251229-260102）

mHC: Manifold-Constrained Hyper-Connections
mHC: 流形约束的超连接

近来，以超连接 (HC) 为代表的研究，通过扩展残差流宽度并多样化连接模式，对过去十年间确立的、普遍存在的残差连接范式进行了拓展。尽管这带来了显著的性能提升，但连接模式的多样化从根本上损害了残差连接固有的恒等映射特性，进而导致严重的训练不稳定性、受限的可扩展性，并产生了显著的内存访问开销。为应对这些挑战，我们提出了流形约束的超连接 (mHC)。这是一个通用框架，它将 HC 的残差连接空间投影到特定流形上，以恢复恒等映射特性，同时融合了严格的基础设施优化以确保效率。实证实验表明，mHC 能有效进行大规模训练，带来切实的性能提升与卓越的可扩展性。我们预计，mHC 作为 HC 的一种灵活且实用的扩展，将有助于更深入地理解拓扑架构设计，并为基础模型的演进提供有前景的方向。

Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding
用于改进长上下文理解的 Mindscape-Aware 检索增强生成

人类理解长而复杂的文本，依赖于对内容的整体语义表征。这种全局视角有助于组织先验知识、解释新信息，并整合分散在文档中的证据，这正体现了心理学中所揭示的人类 Mindscape-Aware（心智景观感知）能力。当前的检索增强生成 (RAG) 系统缺乏这种指导，因此在处理长上下文任务时面临困难。本文提出了 Mindscape-Aware RAG (MiA-RAG)，这是首个为基于大语言模型的 RAG 系统赋予显式全局上下文感知能力的方法。MiA-RAG 通过分层摘要构建一个全局语义表征（即心智景观），并以此为基础指导检索和生成过程。这使得检索器能够形成信息更丰富的查询嵌入，同时使生成器能够在连贯的全局上下文中对检索到的证据进行推理。我们在多种长上下文和双语基准测试上评估了 MiA-RAG 在基于证据的理解和全局语义整合方面的性能。结果表明，MiA-RAG consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.

InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion
InsertAnywhere: 融合4D场景几何与扩散模型以实现逼真的视频对象插入

基于扩散模型的视频生成技术近期取得了显著进展，为可控视频编辑带来了新的可能。然而，由于对4D场景的理解有限，以及对遮挡和光照效应的处理不足，实现逼真的视频对象插入 (VOI) 仍然面临挑战。本文提出了 InsertAnywhere，这是一个新的 VOI 框架，能够实现几何一致的对象放置和外观忠实的视频合成。我们的方法首先采用一个4D感知的掩码生成模块，该模块重建场景几何，并在视频帧间传播用户指定的对象位置，同时确保时间连贯性和遮挡一致性。在此空间基础之上，我们扩展了一个基于扩散的视频生成模型，以联合合成插入的对象及其周围的局部变化（如光照和阴影）。为了进行有监督训练，我们引入了 ROSE++，这是一个光照感知的合成数据集，通过将 ROSE 对象移除数据集转换为三元组（包含对象移除后的视频、对象存在时的视频以及由 VLM 生成的参考图像）而构建。大量实验表明，我们的框架能够在多样化的真实世界场景中生成几何合理且视觉连贯的对象插入效果，其性能显著优于现有的研究模型和商业模型。

Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss
通过辅助损失耦合混合专家模型中的专家与路由器

混合专家 (Mixture-of-Experts, MoE) 模型缺乏明确的约束来确保路由器的决策与专家的能力良好匹配，这最终会限制模型性能。为解决此问题，我们提出了专家-路由器耦合 (expert-router coupling, ERC) 损失，这是一种轻量级辅助损失，旨在将路由器的决策与专家能力紧密耦合。我们的方法将每个专家的路由器嵌入 (router embedding) 视为分配给该专家的 Token 的代理 Token (proxy token)，并将扰动后的路由器嵌入输入专家以获取其内部激活 (internal activations)。ERC 损失对这些激活施加了两项约束：(1) 每个专家对其自身代理 Token 的激活必须高于对其他任何专家代理 Token 的激活。(2) 每个代理 Token 在其对应专家处激发的激活必须强于在其他任何专家处激发的激活。这些约束共同作用，确保每个路由器嵌入能准确表征其对应专家的能力，同时使每个专家专注于处理实际被路由至它的 Token。ERC 损失计算高效，仅需处理 n^2 个激活（n 为专家数量）。这意味着一个与批次大小无关的固定开销，不同于先前那些计算成本随 Token 数量（通常每批次达数百万）增长的耦合方法。通过对参数量从 30亿到 150亿的 MoE-LLMs 进行预训练，并基于数万亿 Token 进行广泛分析，我们验证了 ERC 损失的有效性。此外，ERC 损失还能在训练过程中对专家的专业化程度进行灵活控制和定量追踪，从而为理解 MoE 模型提供了宝贵洞见。

Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models
Youtu-LLM: 释放轻量级大语言模型的原生智能体潜力

我们推出 Youtu-LLM，这是一个轻量级但功能强大的语言模型，它成功兼顾了高计算效率与原生智能体智能。与通常依赖知识蒸馏的小型模型不同，Youtu-LLM (1.96B) 采用从头预训练的方式，旨在系统性地发展其推理与规划能力。其关键技术进展如下：(1) 支持长上下文的紧凑架构：该模型基于密集的多潜在注意力 (MLA) 架构和一种新颖的 STEM 导向词汇表构建，支持长达 128k 的上下文窗口。这一设计使其能够在极小的内存开销下实现稳健的长上下文推理与状态追踪，非常适合长程智能体任务和推理任务。(2) 结构化的 "常识-STEM-智能体" 课程学习：我们构建了一个约 11T token 的大规模语料库，并采用多阶段训练策略。通过逐步将预训练数据的分布从通用常识转向复杂的 STEM 及智能体任务，我们确保模型获得的是深层的认知能力，而非浅层的任务对齐。(3) 可扩展的智能体中期训练：针对智能体中期训练，我们采用了多样化的数据构建方案，在数学、代码和工具使用等领域合成了丰富多样的行动轨迹。这些高质量数据使模型能够有效地内化规划与反思行为。广泛的评估表明，Youtu-LLM 为参数量小于 20 亿的大语言模型树立了新的性能标杆。在通用基准测试中，其性能可与更大的模型相媲美；而在智能体专项任务上，它则显著超越了现有的 SOTA 基线模型，这证明轻量级模型同样可以具备强大的内在智能体能力。

Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling
基于超图记忆改进多步 RAG 以进行长上下文复杂关系建模

多步检索增强生成 (RAG) 已成为一项广泛采用的策略，用于增强大语言模型 (LLM) 在需要全局理解和深度推理任务上的性能。许多 RAG 系统集成了工作记忆模块以整合检索到的信息。然而，现有的记忆设计主要充当被动存储器，仅用于积累孤立的事实，以压缩冗长输入并通过演绎生成新的子查询。这种静态特性忽略了原始事实间关键的高阶关联，而这些事实的组合往往能为后续步骤提供更强的指导。因此，其表示能力以及对多步推理和知识演进的影响有限，导致在长上下文处理中出现推理碎片化和全局理解能力薄弱的问题。我们提出了 HGMem，一种基于超图的记忆机制，它将记忆的概念从简单的存储扩展为一种动态、富有表现力的结构，以支持复杂推理和全局理解。在我们的方法中，记忆被表示为一个超图，其超边对应不同的记忆单元，从而能够在记忆内部逐步形成高阶交互。该机制围绕核心问题连接事实与思路，演进为一个一体化、情境化的知识结构，为后续步骤的深度推理提供有力支撑。我们在多个专为测试全局理解能力设计的挑战性数据集上评估了 HGMem。大量实验和深入分析表明，我们的方法能持续提升多步 RAG 的性能，并在多种任务上显著优于各强基线系统。

LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation
LiveTalk: 通过改进的同策略蒸馏实现实时多模态交互式视频扩散

利用扩散模型进行实时视频生成，对于构建通用多模态交互式 AI 系统至关重要。然而，扩散模型通过迭代过程、结合双向注意力对所有视频帧进行同步去噪，这阻碍了实时交互。虽然现有的蒸馏方法可以使模型具备自回归特性并减少采样步数以缓解此问题，但这些方法主要集中于文生视频任务，导致人机交互显得不自然且效率低下。本文旨在实现一种基于多模态上下文（包括文本、图像和音频）的实时交互式视频扩散模型，以弥合这一差距。我们观察到，领先的同策略蒸馏方法 Self Forcing 在多模态条件输入下会面临挑战（出现闪烁、黑帧和质量下降等视觉伪影）。为此，我们研究了一种改进的蒸馏方案，该方案着重优化条件输入的质量，以及同策略优化的初始化和调度策略。在 HDTF、AVSpeech 和 CelebV-HQ 等多模态条件（音频、图像和文本）驱动的虚拟形象视频生成基准测试中，我们蒸馏得到的模型，在推理成本和延迟降低 20 倍的前提下，其视觉质量与相似或更大规模的全步长双向基线模型相当。此外，我们将该模型与音频语言模型以及长视频推理技术 Anchor-Heavy Identity Sinks 相结合，构建了 LiveTalk——一个实时多模态交互式虚拟形象系统。在我们精心构建的多轮交互基准上进行的系统级评估显示，LiveTalk 在多轮视频连贯性和内容质量方面均优于 Sora2、Veo3 等最先进的模型，同时将响应延迟从 1-2 分钟大幅降低至实时生成水平，从而实现了流畅无缝的人机多模态交互。

Yume-1.5: A Text-Controlled Interactive World Generation Model
Yume-1.5: 一个文本控制的交互式世界生成模型

近期的一些方法展现了利用扩散模型生成交互式、可探索世界的潜力。然而，这些方法大多面临关键挑战，例如参数量过大、依赖冗长的推理步骤以及历史上下文快速增长，这些问题严重限制了实时性能，并且缺乏文本控制生成能力。为解决这些挑战，我们提出了 \method，这是一个新颖的框架，用于从单张图像或文本提示生成逼真、交互且连续的世界。\method 通过一个精心设计的框架实现此目标，该框架支持基于键盘对生成的世界进行探索。该框架包含三个核心组件：(1) 一个集成了统一上下文压缩与线性注意力的长视频生成框架；(2) 一个由双向注意力蒸馏和增强文本嵌入方案驱动的实时流式加速策略；(3) 一种用于生成世界事件的文本控制方法。相关代码库已提供在补充材料中。

Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem
任其流动：基于 ROCK 与 ROLL 的智能体式构建，在开放智能体学习生态系统中打造 ROME 模型

智能体式构建 (Agentic crafting) 要求大语言模型 (LLM) 在现实环境中通过多轮操作来执行任务，具体包括采取行动、观察结果并迭代优化其生成的制品。尽管该能力至关重要，但开源社区目前仍缺乏一个系统化、端到端的生态系统来简化智能体的开发流程。为此，我们提出了智能体学习生态系统 (Agentic Learning Ecosystem, ALE)，这是一个旨在优化智能体大语言模型生产流水线的基础设施。ALE 包含三个核心组件：ROLL，一个用于权重优化的后训练 (post-training) 框架；ROCK，一个用于生成训练轨迹的沙盒环境管理器；以及 iFlow CLI，一个用于高效上下文工程 (context engineering) 的智能体框架。我们发布了 ROME (ROME is Obviously an Agentic Model)，这是一个基于 ALE 构建、并在超过一百万条轨迹上训练而成的开源智能体模型。我们的方法包含用于合成复杂行为的数据组合协议 (data composition protocols)，以及一种新颖的策略优化算法——基于交互的策略对齐 (Interaction-based Policy Alignment, IPA)。IPA 算法在语义交互块而非单个 Token 上分配信用，从而提升了长视野 (long-horizon) 训练的稳定性。在实证评估中，我们在一个结构化设置中对 ROME 进行了测试，并推出了 Terminal Bench Pro 基准测试，该基准在规模和污染控制方面均有改进。ROME 在 SWE-bench Verified 和 Terminal Bench 等多个基准测试中均展现出强劲性能，这证明了 ALE 基础设施的有效性。

## 每周AI论文速递（260105-260109）

GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization
GDPO: 面向多奖励RL优化的组奖励解耦归一化策略优化
随着语言模型能力日益增强，用户不仅期望其提供准确的响应，还希望它们能在多样化的场景中表现出符合不同人类偏好的行为。为此，强化学习 (RL) 训练框架已开始整合多个奖励信号，每个奖励对应一种特定偏好，以引导模型产生这些期望行为。然而，近期研究默认在多奖励设置下直接采用组相对策略优化 (GRPO)，而未深入探究其适用性。本文证明，直接应用GRPO对不同rollout奖励组合进行归一化，会导致这些组合的优势值坍缩为相同数值，从而降低训练信号的分辨率，导致收敛结果次优，甚至在部分情况下引发早期训练失败。为此，我们提出了组奖励解耦归一化策略优化 (GDPO)。这一新策略优化方法通过解耦各奖励的归一化过程，解决了上述问题，能更真实地保留奖励间的相对差异，从而实现更精确的多奖励优化，并大幅提升训练稳定性。我们在工具调用、数学推理和代码推理三个任务上对比了GDPO与GRPO，评估了包括正确性指标（准确率、错误率）和约束遵循指标（格式、长度）。在所有实验设置下，GDPO均一致优于GRPO，证明了其在多奖励强化学习优化中的有效性和泛化能力。

NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos
NeoVerse: 利用真实世界单目视频增强 4D 世界模型
本文提出 NeoVerse，一个多功能 4D 世界模型，能够执行 4D 重建、新视角轨迹视频生成以及丰富的下游任务。我们首先指出，当前 4D 世界建模方法普遍存在可扩展性局限，其根源在于依赖昂贵且专用的多视图 4D 数据，或训练预处理流程繁琐。相比之下，NeoVerse 基于一个核心设计理念，使得整个流程能够轻松扩展至多样化的真实世界单目视频。具体而言，NeoVerse 具备免姿态前馈 4D 重建、在线单目退化模式模拟以及其他协调一致的技术。这些设计使 NeoVerse 具备了多功能性以及对多种领域的泛化能力。同时，NeoVerse 在标准重建与生成基准测试中取得了最先进的性能。项目页面详见 neoverse-4d.github.io。

Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization
Youtu-Agent: 通过自动化生成与混合策略优化提升智能体生产力
现有的大语言模型 (LLM) 智能体框架面临两大挑战：配置成本高昂与能力固化。构建高质量智能体通常需要在工具集成和提示工程上投入大量人工，而已部署的智能体若不进行代价高昂的微调，则难以适应动态环境。为解决这些问题，我们提出了 Youtu-Agent，这是一个专为 LLM 智能体自动化生成与持续进化设计的模块化框架。Youtu-Agent 具备结构化配置系统，实现了执行环境、工具包与上下文管理的解耦，从而支持灵活复用与自动化组装。我们引入了两种生成范式：面向标准任务的 工作流 模式，以及面向复杂、非规范化需求的 元智能体 模式，后者能够自动生成工具代码、提示词及配置。此外，Youtu-Agent 构建了一套混合策略优化系统：(1) 智能体实践 模块，使智能体能够通过上下文内优化积累经验、提升性能，且无需更新模型参数；(2) 智能体强化学习 模块，可与分布式训练框架集成，支持以端到端、大规模的方式对任意 Youtu-Agent 进行可扩展且稳定的强化学习。实验表明，使用开源权重模型时，Youtu-Agent 在 WebWalkerQA (71.47%) 和 GAIA (72.8%) 基准上取得了领先性能。我们的自动化生成流程工具合成成功率超过 81%，而实践模块在 AIME 2024 和 2025 上分别将性能提升了 2.7% 和 5.4%。此外，我们的智能体强化学习训练在 7B 参数规模的 LLM 上实现了 40% 的加速，同时性能持续稳定提升，在数学及通用/多跳问答基准测试中，编码/推理与搜索能力分别最高提升了 35% 和 21%。

InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields
InfiniDepth: 基于神经隐式场的任意分辨率细粒度深度估计
现有的深度估计方法本质上受限于在离散的图像网格上预测深度。这种表示形式限制了其向任意输出分辨率的扩展能力，并影响了几何细节的还原。本文提出了 InfiniDepth，该方法将深度表示为神经隐式场。通过一个简单而有效的局部隐式解码器，我们可以在连续的二维坐标处查询深度值，从而实现任意分辨率下的细粒度深度估计。为了更全面地评估本方法的性能，我们从五款不同的游戏中构建了一个高质量的 4K 合成基准数据集，该数据集涵盖了具有丰富几何与外观细节的多样化场景。大量实验表明，无论是在合成数据还是真实世界数据的基准测试上，InfiniDepth 在相对深度估计和度量深度估计任务中均达到了最先进的性能，尤其在精细细节区域的表现尤为突出。此外，该方法也能显著提升大视角变化下新视图合成任务的效果，所生成的结果质量更高，且空洞和伪影更少。

LTX-2: Efficient Joint Audio-Visual Foundation Model
LTX-2: 高效的联合视听基础模型
当前的文本到视频扩散模型虽能生成高质量的视频序列，但通常是无声的，缺乏音频所能提供的语义、情感及氛围线索。为此，我们提出了 LTX-2，这是一个开源的基础模型，能够以统一的方式生成高质量且时间同步的视听内容。LTX-2 采用非对称双流 Transformer 架构，其中视频流包含 140 亿参数，音频流包含 50 亿参数。两个流通过双向视听交叉注意力层进行耦合，该层包含时间位置嵌入以及用于共享时间步条件化的跨模态 AdaLN。此架构不仅实现了统一视听模型的高效训练与推理，还为视频生成分配了比音频生成更多的模型容量。
我们采用了多语言文本编码器，以支持对更广泛提示词的理解，并引入了一种模态感知的无分类器引导机制，从而提升了视听对齐效果与可控性。除了生成语音，LTX-2 还能生成丰富、连贯的音频轨道，这些音频能够贴合每个场景的角色、环境、风格与情感，并包含自然的背景音和拟音效果。在我们的评估中，该模型在开源系统中实现了最先进的视听质量与提示跟随性，同时仅需专有模型一小部分的计算成本和推理时间，便能达到与之相当的效果。所有模型权重和代码均已开源发布。

Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting
熵自适应微调：解决自信冲突以缓解遗忘
监督微调 (Supervised Fine-Tuning, SFT) 是领域适应的标准范式，但它常常以灾难性遗忘为代价。与此形成鲜明对比的是，基于策略的强化学习 (Reinforcement Learning, RL) 能有效保留模型的通用能力。我们探究了这种差异，并发现了一个根本性的分布不匹配问题：RL 与模型的内在信念保持一致，而 SFT 则迫使模型拟合外部监督信号。这种不匹配通常表现为一种"自信冲突" (Confident Conflicts) 的 Token，其特征是预测概率低但熵值也低。在这种情况下，模型对其自身预测高度自信，却被迫学习与之相悖的真实标签，从而引发破坏性的梯度更新。为解决此问题，我们提出了熵自适应微调 (Entropy-Adaptive Fine-Tuning, EAFT)。与仅依赖预测概率的方法不同，EAFT 利用 Token 级别的熵作为门控机制，以区分认知不确定性和知识冲突。这使得模型能够从不确定的样本中学习，同时抑制来自冲突数据的梯度。我们在数学、医学和 AI 智能体领域，对 Qwen 和 GLM 系列模型 (参数规模从 4B 到 32B) 进行了广泛实验，结果证实了我们的假设。EAFT 在始终达到与标准 SFT 相当的下游任务性能的同时，显著缓解了通用能力的衰退。

K-EXAONE Technical Report
K-EXAONE 技术报告
本技术报告介绍了 K-EXAONE，这是一个由 LG AI Research 开发的大规模多语言大语言模型。K-EXAONE 基于专家混合 (Mixture-of-Experts, MoE) 架构构建，总参数量为 236B，推理时激活参数量为 23B。它支持 256K Token 的上下文窗口，并涵盖六种语言：韩语、英语、西班牙语、德语、日语和越南语。我们在一个涵盖推理、智能体能力、通用能力、韩语能力及多语言能力的综合基准测试套件上对 K-EXAONE 进行了评估。在这些评估中，K-EXAONE 展现出了与同类规模的开源模型相当的性能。K-EXAONE 旨在通过推进人工智能技术来创造更美好的生活，其定位是一个强大的闭源 AI 基础模型，适用于广泛的工业与科研应用。

Evolving Programmatic Skill Networks
演化式程序化技能网络
我们研究在开放域具身环境中持续的技能获取问题，智能体需要构建、优化并重用其不断增长的可执行技能库。我们提出了程序化技能网络（PSN），该框架中的技能是可执行的符号程序，它们构成一个组合网络，并通过经验不断演化。PSN 定义了三个由大语言模型实例化的核心机制：(1) 用于对技能组合进行结构化故障定位的 REFLECT，(2) 具备成熟度感知更新门控的渐进式优化，该机制能稳定可靠技能，同时为不确定技能保持可塑性，以及 (3) 在回滚验证下的规范结构重构，以维持网络紧凑性。我们进一步指出，PSN 的学习动态在结构上与神经网络训练存在相似性。在 MineDojo 和 Crafter 环境上的实验表明，该方法在开放域任务分布上具有强大的技能重用能力、快速适应能力和优异的泛化性能。\footnote{我们计划开源代码。

Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits
大语言模型能预测自身的失败吗？基于内部路径的自我感知
大语言模型 (LLMs) 能够生成流畅且复杂的输出，但常常无法识别自身的错误和幻觉。现有方法通常依赖于外部评判器、多样本一致性或基于文本的自我批判，这些方法要么会产生额外的计算开销，要么与真实正确性的关联较弱。我们探讨一个问题：大语言模型能否通过检查推理过程中的内部状态来预测自身的失败？我们提出了 Gnosis，一种轻量级的自我感知机制，它使参数冻结的 LLMs 能够通过解码其隐藏状态和注意力模式的信号，进行内在的自我验证。Gnosis 被动地观察内部轨迹，将其压缩为固定资源占用的描述符，并以极低的推理开销预测正确性，仅增加约 500 万个参数，且其运行与序列长度无关。在数学推理、开放域问答和学术知识基准测试中，在参数规模从 17 亿到 200 亿不等的多个冻结骨干模型上，Gnosis 在准确性和校准度方面均持续优于性能强劲的内部基线模型和规模庞大的外部评判器。此外，它能够零样本泛化到部分（不完整）的生成结果，从而实现对失败生成路径的早期检测，并进行计算感知的控制。这些结果表明，可靠的正确性线索本就存在于生成过程之中，无需外部监督即可高效提取。

NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation
NextFlow: 统一序列建模赋能多模态理解与生成
我们提出了 NextFlow，这是一个统一的仅解码器自回归 Transformer 模型，在 6 万亿交织的文本-图像离散 Token 上训练而成。通过在统一的自回归架构内利用统一的视觉表示，NextFlow 原生具备了多模态理解与生成能力，实现了图像编辑、交织内容生成以及视频生成等功能。鉴于不同模态的本质差异——文本具有严格的顺序性，而图像则具有内在的层次性——我们为文本保留了下一 Token 预测，但对视觉生成采用了下一尺度预测。这有别于传统的光栅扫描方法，使得生成 1024x1024 分辨率图像仅需 5 秒，比同类自回归 (AR) 模型快数个数量级。我们通过一套稳健的训练方案解决了多尺度生成的不稳定性问题。此外，我们还引入了一种用于强化学习的前缀调优 (prefix-tuning) 策略。实验表明，NextFlow 在统一模型中取得了最先进的性能，并且在视觉质量上可与专门的扩散 (diffusion) 基线模型相匹敌。

MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization
MOSS Transcribe Diarize: 带说话人角色分离的精准转录
带说话人角色和时间戳的转录 (Speaker-Attributed, Time-Stamped Transcription, SATS) 旨在转录语音内容，并精确确定每位说话人的发言时间点，这对于会议转录尤其有价值。现有的 SATS 系统很少采用端到端方案，并且普遍存在上下文窗口有限、长距离说话人记忆能力弱以及无法输出时间戳等局限。为了克服这些不足，我们提出了 MOSS Transcribe Diarize，这是一个统一的多模态大语言模型，能够以端到端的方式联合完成带说话人角色和时间戳的转录。该模型在大量真实场景数据上进行训练，并配备了可处理长达 90 分钟音频输入的 128k 上下文窗口，因此具有良好的可扩展性和强大的泛化能力。在全面的评估中，它在多个公开及内部基准测试上的表现均优于当前最先进的商业系统。

Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation
Avatar Forcing：面向自然对话的实时交互式头部数字人生成
说话头生成技术旨在从静态肖像中创建逼真的数字人 (Avatar) ，以用于虚拟交流与内容创作。然而，现有模型尚无法营造出真正交互式交流的体验，其生成的响应往往是单向的，缺乏情感参与感。我们为实现真正交互式的数字人，识别出两个关键挑战：一是在因果约束条件下实时生成运动；二是在无需额外标注数据的情况下，学习富有表现力且鲜活自然的反应。为应对这些挑战，我们提出了 Avatar Forcing，这是一个用于交互式头部数字人生成的新框架，它通过扩散驱动 (Diffusion Forcing) 对实时用户-数字人交互进行建模。该设计使得数字人能够以低延迟处理实时多模态输入（包括用户的音频和动作），从而对语音、点头、笑声等言语与非言语线索做出即时反应。此外，我们引入了一种直接偏好优化方法，该方法利用通过丢弃用户条件构建的合成负样本，实现了对富有表现力交互的无标签学习。实验结果表明，我们的框架能够实现低延迟（约500毫秒）的实时交互，相比基线模型获得了6.8倍的加速，并生成了反应灵敏且富有表现力的数字人运动，在超过80%的评估中优于基线模型。

Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation
抑制幻觉：通过反事实视频生成增强 MLLMs 的视频理解能力
多模态大语言模型 (Multimodal Large Language Models, MLLMs) 在视频理解方面取得了显著进展。然而，它们存在一个关键缺陷：过度依赖语言先验，这可能导致视觉上无根据的幻觉，尤其是在处理违背常识的反事实视频时。这一局限源于文本与视频之间固有的数据不平衡，而由于收集和标注反事实数据成本高昂，该问题难以解决。为此，我们提出了 DualityForge，一个新颖的反事实数据合成框架。该框架利用可控的、基于扩散模型的视频编辑技术，将真实世界视频转化为反事实场景。通过将结构化的上下文信息嵌入视频编辑和问答生成过程，该框架能自动生成高质量的问答对以及用于对比训练的原始视频与编辑后视频配对。基于此，我们构建了 DualityVidQA，一个旨在减少 MLLM 幻觉的大规模视频数据集。此外，为充分利用配对数据的对比特性，我们提出了 Duality 归一化优势训练 (Duality-Normalized Advantage Training, DNA-Train)。这是一个两阶段的 SFT-RL 训练机制，其中强化学习阶段应用了成对的 ℓ1\ell_1ℓ1​ 优势归一化，从而实现更稳定、高效的策略优化。在 DualityVidQA-Test 上的实验表明，我们的方法能显著降低模型在反事实视频上的幻觉，相比 Qwen2.5-VL-7B 基线获得了 24.0% 的相对提升。此外，我们的方法在幻觉评测和通用基准测试上均取得了显著进步，显示出强大的泛化能力。我们将开源数据集和代码。

Recursive Language Models
递归语言模型
我们从推理时扩展的角度，研究如何让大语言模型 (LLMs) 能够处理任意长度的提示。我们提出了递归语言模型 (RLMs)，这是一种通用的推理策略。它将长提示视为外部环境的一部分，使大语言模型能够以编程方式对提示片段进行检查、分解，并递归地调用自身进行处理。我们发现，在四个不同的长上下文任务中，RLMs 能够成功处理长度超出模型原始上下文窗口两个数量级的输入。并且，即使是对于较短的提示，其输出质量也显著优于基础大语言模型和常见的长上下文扩展框架，同时每次查询的成本与之相当（甚至更低）。

DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer
DreamID-V：通过扩散 Transformer 弥合图像到视频鸿沟以实现高保真人脸交换
视频人脸交换 (VFS) 旨在将源身份无缝注入目标视频，同时精确保持原始的姿态、表情、光照、背景及动态信息。现有方法在维持时序一致性的同时，往往难以兼顾身份相似度与属性保留。为应对这一挑战，我们提出了一个综合性框架，旨在将图像人脸交换 (IFS) 的优势无缝迁移至视频领域。我们首先引入了一种新颖的数据流水线 SyncID-Pipe，它预训练了一个身份锚定视频合成器，并将其与 IFS 模型相结合，构建用于显式监督的双向 ID 四元组。基于此配对数据，我们提出了首个基于扩散 Transformer 的框架 DreamID-V，其核心是一个模态感知条件模块，用于区分性地注入多模态条件。同时，我们提出了一种合成到真实的课程学习机制以及一种身份一致性强化学习策略，以提升在复杂场景下的视觉真实感与身份一致性。针对现有基准测试有限的问题，我们引入了 IDBench-V，这是一个涵盖多样化场景的综合基准测试集。大量实验表明，DreamID-V 性能优于现有最先进方法，并展现出出色的通用性，能够无缝适配各类与人脸交换相关的任务。

## 每周AI论文速递（260112-260116）

Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning
观看、推理与搜索：面向智能体视频推理的开放网络视频深度研究基准
在现实世界的视频问答场景中，视频本身通常仅提供局部视觉线索，而可验证的答案广泛分布于开放网络。因此，模型需要协同完成跨帧线索提取、迭代式检索以及基于多跳推理的验证。为弥补这一差距，我们构建了首个视频深度研究基准 VideoDR。VideoDR 的核心是视频条件化的开放域视频问答，要求模型执行跨帧视觉锚点提取、交互式网络检索，并对视频与网络联合证据进行多跳推理。通过严格的人工标注与质量控制，我们构建了涵盖六个语义领域的高质量视频深度研究样本集。我们评估了在流程式 (Workflow) 与智能体式 (Agentic) 两种范式下的多个闭源及开源多模态大语言模型。结果表明，智能体式 (Agentic) 范式并非总是优于流程式 (Workflow) 范式：其性能提升取决于模型在长链检索过程中维持初始视频锚点的能力。进一步分析指出，目标漂移 (goal drift) 和长程一致性 (long-horizon consistency) 是核心瓶颈。总之，VideoDR 为研究开放网络环境下的视频智能体提供了一个系统性基准，并揭示了下一代视频深度研究智能体所面临的关键挑战。

BabyVision: Visual Reasoning Beyond Language
BabyVision: 超越语言的视觉推理
人类在掌握语言能力之前很久便已发展出核心视觉技能，然而，当代的多模态大语言模型 (Multimodal LLMs, MLLMs) 仍严重依赖语言先验来弥补其薄弱的视觉理解能力。我们发现了一个关键事实：最先进的多模态大语言模型在人类（即使是3岁儿童）都能轻松解决的基本视觉任务上持续表现不佳。为了系统地探究这一差距，我们提出了 BabyVision 基准，旨在评估多模态大语言模型独立于语言知识的核心视觉能力。BabyVision 涵盖广泛的任务，包含388个测试项，划分为四个关键类别下的22个子类。实证结果与人工评估表明，领先的多模态大语言模型性能显著低于人类基线。Gemini3-Pro-Preview 的得分为49.7，落后于6岁儿童的表现，且远低于成人平均分94.1。这些结果表明，尽管当前的多模态大语言模型在知识密集型评估中表现出色，但它们仍然缺乏基本的视觉原语。BabyVision 的进展是迈向人类水平视觉感知与推理能力的一步。我们还通过提出 BabyVision-Gen 和自动评估工具包，探索了利用生成模型解决视觉推理问题。我们的代码和基准数据已在 github.com/UniPat-AI/B… 发布，以供复现。

Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization
基于地图思考：用于地理定位的强化并行地图增强智能体
图像地理定位任务旨在利用视觉线索，预测图像在地球上的拍摄位置。现有的大型视觉语言模型 (LVLM) 方法利用了世界知识、思维链 (Chain-of-Thought) 推理和智能体 (Agent) 能力，但忽略了人类常用的一种策略——使用地图。在本工作中，我们首先赋予模型 基于地图思考 的能力，并将其形式化为一个地图中的智能体循环。我们为此开发了一个两阶段优化方案：首先是智能体强化学习 (RL)，随后是并行测试时缩放 (TTS)。RL 阶段旨在增强模型的智能体能力，以提高其采样效率；而并行 TTS 阶段则允许模型在做出最终预测前并行探索多条候选路径，这对地理定位任务至关重要。为了在最新且真实（非合成）的图像上评估我们的方法，我们进一步提出了 MAPBench，这是一个完全基于真实世界图像的综合性地理定位训练与评估基准。实验结果表明，我们的方法在大多数指标上优于现有的开源和闭源模型。具体而言，与启用 Google 搜索/地图定位模式的 Gemini-3-Pro 相比，我们的方法将 Acc@500m 指标从 8.0% 显著提升至 22.1%。

STEP3-VL-10B Technical Report
STEP3-VL-10B 技术报告
我们推出 STEP3-VL-10B，这是一个轻量级开源基础模型，旨在重新权衡紧凑效率与前沿多模态智能能力。STEP3-VL-10B 通过两项战略转变实现：首先，在 1.2T 多模态 Token 上采用统一且全部参数可训练的预训练策略，将语言对齐的感知编码器与 Qwen3-8B 解码器集成，以建立内在的视觉-语言协同；其次，采用一个大规模后训练流程，包含超过 1000 次强化学习迭代。关键在于，我们实施了并行协调推理 (PaCoRe) 来扩展测试时的计算规模，将资源分配给可扩展的感知推理，以探索并整合多样化的视觉假设。因此，尽管其模型规模紧凑，仅为 10B 参数，STEP3-VL-10B 的性能却媲美甚至超越了规模大 10 到 20 倍的模型（例如 GLM-4.6V-106B、Qwen3-VL-235B）以及顶级的闭源旗舰模型，如 Gemini 2.5 Pro 和 Seed-1.5-VL。它实现了同类最佳的绩效，在 MMBench 上取得 92.2% 的得分，在 MMMU 上取得 80.11% 的得分；同时在复杂推理任务中表现卓越，在 AIME2025 上达到 94.43%，在 MathVision 上达到 75.95%。我们发布了完整的模型套件，旨在为社区提供一个强大、高效且可复现的基准。

Urban Socio-Semantic Segmentation with Vision-Language Reasoning
基于视觉语言推理的城市社会语义分割
城市作为人类活动的中心，其区域包含丰富的语义实体。从卫星影像中分割这些多样的实体，对众多下游应用至关重要。当前先进的分割模型能够可靠地分割由物理属性定义的实体 (例如，建筑物、水体) ，但在处理社会语义类别 (例如，学校、公园) 时仍面临挑战。本研究通过视觉语言模型 (Vision-Language Model) 的推理能力，实现了社会语义分割。为此，我们提出了一个名为 SocioSeg 的城市社会语义分割数据集，该新资源包含卫星影像、数字地图以及按层级结构组织的社会语义实体的像素级标注。此外，我们提出了一种名为 SocioReasoner 的新型视觉语言推理框架，它通过跨模态识别与多阶段推理，模拟人类识别与标注社会语义实体的过程。我们采用强化学习来优化这一不可微的流程，从而有效激发视觉语言模型固有的推理能力。实验结果表明，我们的方法相较于现有最优模型取得了性能提升，并展现出强大的零样本泛化能力。我们的数据集与代码已公开于 github.com/AMAP-ML/Soc…

Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs
奖励稀缺性：面向大语言模型创造性问题解决的独特性感知强化学习
强化学习已成为大语言模型后训练，尤其是针对复杂推理任务的核心范式。然而，该方法常面临探索崩溃问题：模型策略过早地收敛于少数几种主导的推理模式。这虽然能提升 pass@1 指标，却限制了推理路径层面的多样性，从而抑制了 pass@k 指标的进一步增益。我们认为，问题的根源在于现有方法侧重于对局部 Token 行为进行正则化，而非鼓励解决方案集合的多样性。为此，我们提出了独特性感知强化学习。该方法定义了一个推理路径层面的优化目标，明确奖励那些采用罕见高级策略的正确解决方案。具体而言，我们利用一个基于大语言模型的评判器，将针对同一问题生成的不同推理路径按其高级解决策略（忽略表面差异）进行聚类，并依据聚类规模反比地重新调整策略优势值。如此一来，正确且新颖的策略将获得比冗余策略更高的奖励。在数学、物理和医学推理等多个基准测试上的实验表明，我们的方法能够在大规模采样预算下持续提升 pass@kkk 指标，并增大 pass@kkk 曲线下面积，同时不损害 pass@1 性能。该方法有效维持了探索过程，并能在更大规模上发掘出更多样化的解决方案策略。

DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation
DeepResearchEval：一个用于深度研究任务构建与智能体评估的自动化框架
深度研究系统已广泛应用于多步骤网络研究、分析与多源信息综合，但其评估仍面临挑战。现有基准测试往往需要大量标注来构建任务，依赖静态的评估维度，或在引文缺失时无法可靠地验证事实。为填补这些空白，我们提出了 DeepResearchEval，一个用于深度研究任务构建与智能体 (Agentic) 评估的自动化框架。在任务构建方面，我们设计了一个角色 (Persona) 驱动的流程，能够基于多样化的用户画像生成真实、复杂的研究任务，并应用一个两阶段过滤器（任务资格 (Task Qualification) 与搜索必要性 (Search Necessity)）来筛选出仅那些需要整合多源证据并进行外部检索的任务。在评估方面，我们提出了一个智能体流程，包含两个组件：一是自适应点式质量评估 (Adaptive Point-wise Quality Evaluation)，它能根据每个生成的任务动态推导出任务特定的评估维度、标准及权重；二是主动事实核查 (Active Fact-Checking)，它能通过网页搜索自主提取并验证报告中的陈述，即便在引文缺失的情况下也能进行。

Controlled Self-Evolution for Algorithmic Code Optimization
面向算法代码优化的受控自进化
自进化方法通过迭代式的“生成-验证-精炼”循环来提升代码生成质量。然而，现有方法探索效率低下，难以在有限预算内发现复杂度更优的解决方案。这种低效源于几个瓶颈：初始化偏差会将进化过程限制在较差的解空间区域；随机操作缺乏反馈引导，不可控；且跨任务的经验未能得到充分利用。为应对这些挑战，我们提出了受控自进化 (Controlled Self-Evolution, CSE)，其包含三个核心组件。多样化规划初始化通过生成结构各异的算法策略，以实现对解空间的广泛覆盖。遗传进化则采用反馈引导机制替代随机操作，从而支持有针对性的变异与组合式交叉。分层进化记忆能够在任务间与任务内两个层面，同时捕获成功与失败的经验。在EffiBench-X基准上的实验表明，CSE在使用不同大语言模型 (LLM) 骨干时，均稳定优于所有基线方法。此外，CSE在进化早期即展现出更高的效率，并能在此后的整个进化过程中持续改进。我们的代码已公开于 github.com/QuantaAlpha…

MMFormalizer: Multimodal Autoformalization in the Wild
MMFormalizer: 真实场景下的多模态自动形式化
自动形式化 (Autoformalization) 旨在将自然语言描述的数学内容转化为形式化语句，以实现机器推理。然而，在真实物理世界的开放场景下，由于其多模态特性，自动形式化面临着根本性挑战：物理学问题常常需要从视觉元素中推断出隐藏的约束条件 (例如质量或能量)。为此，我们提出了 MMFormalizer，它通过整合来自真实世界数学和物理领域的实体进行自适应基础 (adaptive grounding)，从而将自动形式化的范畴从纯文本扩展到了多模态。MMFormalizer 通过递归基础 (recursive grounding) 和公理组合 (axiom composition)，从感知基础 (perceptually grounded) 的基元出发，递归地构建形式化命题。其自适应的递归终止机制确保每一个抽象概念都有视觉证据支撑，并基于维度或公理基础 (dimensional or axiomatic grounding)。我们在一个新构建的基准测试 PhyX-AF 上评估了 MMFormalizer。该基准包含从 MathVerse、PhyX、综合几何 (Synthetic Geometry) 和解析几何 (Analytic Geometry) 中精心挑选的 115 个样本，涵盖了多样化的多模态自动形式化任务。结果表明，GPT-5 和 Gemini-3-Pro 等前沿模型在形式化编译和语义准确性方面取得了最高分，其中 GPT-5 在物理推理任务上表现尤为出色，而几何领域仍然是挑战最大的方向。总体而言，MMFormalizer 为统一的多模态自动形式化提供了一个可扩展的框架，有效连接了感知与形式推理。据我们所知，这是首个能够处理经典力学 (基于哈密顿量推导)、相对论、量子力学和热力学的多模态自动形式化方法。更多详细信息请访问我们的项目页面：MMFormalizer.github.io。

MAXS: Meta-Adaptive Exploration with LLM Agents
MAXS: 基于大语言模型智能体的元自适应探索
大语言模型 (LLM) 智能体通过多工具协作，具备内在的推理能力。然而，在智能体推理过程中，现有方法通常存在两个问题：(i) 由于缺乏前瞻性，导致局部短视的生成；(ii) 轨迹不稳定，即早期的微小错误可能演变为发散的推理路径。这些问题使得难以在全局有效性与计算效率之间取得平衡。为解决这两个问题，我们提出了 MAXS (Meta-Adaptive Exploration with LLM Agents，项目地址：github.com/exoskeleton…) ，这是一个基于 LLM 智能体的元自适应推理框架，能够灵活集成工具执行与推理规划。MAXS 采用前瞻策略，将推理路径向前推演若干步，以估计工具使用的优势值，并结合步骤一致性方差与步骤间趋势斜率，共同筛选出稳定、一致且高价值的推理步骤。此外，我们引入了轨迹收敛机制，一旦路径一致性达成，便停止进一步推演，从而控制计算成本，实现在多工具推理中平衡资源效率与全局有效性。我们在三个基础模型 (MiMo-VL-7B、Qwen2.5-VL-7B、Qwen2.5-VL-32B) 和五个数据集上进行了大量实验，结果表明 MAXS 在性能与推理效率上均持续优于现有方法。进一步的分析证实了我们前瞻策略与工具使用机制的有效性。

A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation
A^3-Bench：基于锚点与吸引子激活的记忆驱动科学推理基准
科学推理不仅依赖于逻辑推断，也需要激活先验知识与经验结构。记忆能够有效复用知识，提升推理的一致性与稳定性。然而，现有基准主要评估最终答案或逐步推理的连贯性，忽略了人类推理所依赖的 记忆驱动 机制，该机制通过激活锚点 (Anchor) 和吸引子 (Attractor)，并将其整合到多步推理中来实现。为填补这一空白，我们提出了 A³-Bench (a3-bench.github.io)，这是一个基于锚点与吸引子激活 (Anchor and Attractor Activation) 理论、旨在通过双尺度记忆驱动激活来评估科学推理的基准。
首先，我们采用 SAPM 流程 (即主题、锚点与吸引子、问题及记忆形成) 对跨领域的 2,198 个科学推理问题进行了标注。其次，我们引入了一个利用锚点和吸引子的双尺度记忆评估框架，并提出了 AAUI (锚点-吸引子利用指数) 指标以量化记忆激活率。最后，通过对多种基础模型及推理范式的实验，我们验证了 A³-Bench 的有效性，分析了记忆激活如何影响推理性能，从而深入理解了记忆驱动的科学推理机制。

PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning
PaCoRe: 学习通过并行协调推理扩展测试时计算量
我们提出了并行协调推理 (PaCoRe)，这是一个旨在克服当代语言模型核心局限性的训练与推理框架：即模型无法在固定上下文窗口下，将测试时计算量 (Test-Time Compute, TTC) 扩展到远超顺序推理的程度。PaCoRe 摒弃了传统的顺序范式，转而通过多轮消息传递架构协调的大规模并行探索来驱动 TTC。在每一轮中，系统启动多个并行推理轨迹，将其发现压缩成受上下文长度限制的消息，然后综合这些消息来指导下一轮推理，并最终生成答案。通过大规模、基于结果的强化学习进行端到端训练，模型掌握了 PaCoRe 所需的信息综合能力，能够将有效 TTC 扩展到数百万 token 的规模，同时不突破上下文长度限制。该方法在多个不同领域都带来了显著提升，尤其在数学推理方面超越了前沿系统：一个 80 亿参数的模型在 HMMT 2025 数据集上达到了 94.5% 的准确率，通过将有效 TTC 扩展至约两百万 token，超越了 GPT-5 的 93.2%。我们开源了模型检查点、训练数据以及完整的推理流水线，以促进后续研究。

MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences
MemGovern: 通过从经过治理的人类经验中学习增强代码智能体
尽管自主软件工程 (SWE) 智能体正在重塑编程范式，但它们目前存在一个“封闭世界”的局限：它们试图从零开始或仅依赖本地上下文来修复缺陷，而忽略了 GitHub 等平台上可用的海量历史人类经验。然而，现实世界中的问题跟踪数据往往是非结构化和碎片化的，这阻碍了智能体有效利用这些开放世界的经验。本文提出了 MemGovern 框架，旨在治理原始 GitHub 数据，并将其转化为智能体可操作的体验记忆。MemGovern 通过经验治理流程，将人类经验转换为便于智能体使用的经验卡片，并引入了一种智能体驱动的经验搜索策略，从而实现基于逻辑的人类专业知识检索。通过生成 13.5 万个经过治理的经验卡片，MemGovern 带来了显著的性能提升，在 SWE-bench Verified 基准测试中将问题解决率提高了 4.65%。作为一种插件式方案，MemGovern 为构建适配智能体的记忆基础设施提供了一种解决方案。

Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning
面向推理的协作式多智能体测试时强化学习
多智能体系统已发展成为许多实际应用中由大语言模型驱动的实用协作者，其鲁棒性得益于多样性和交叉验证。然而，多智能体强化学习 (MARL) 训练资源密集且不稳定：智能体间的协同适应会导致环境非平稳性，且奖励信号通常稀疏且方差高。为此，我们提出了 多智能体测试时强化学习 (MATTRL) 框架，该框架在推理阶段将结构化的文本经验注入多智能体的决策审议中。MATTRL 组建一个多专家团队进行多轮讨论，检索并整合测试时经验，最终达成共识以做出决策。我们还研究了信用分配机制，用于构建轮级经验池并将其重新注入对话流程。在医学、数学和教育等领域的多个挑战性基准测试上，MATTRL 的平均准确率相较于多智能体基线提升了 3.67%，相较于相应的单智能体基线提升了 8.67%。消融研究分析了不同的信用分配方案，并详细比较了它们对训练结果的影响。MATTRL 为无需额外调优、且能有效应对分布偏移的多智能体推理，提供了一条稳定、高效且有效的路径。

Motion Attribution for Video Generation
视频生成中的运动归因
尽管视频生成模型发展迅速，但数据如何影响运动仍不明确。我们提出了 Motive (MOTIon attribution for Video gEneration)，这是一个运动中心、基于梯度的数据归因框架，能够适应现代大规模高质量视频数据集和模型。我们利用该框架研究哪些微调片段会改善或损害时序动态。Motive 通过运动加权损失掩码将时序动态与静态外观分离，实现了高效且可扩展的运动特定影响力计算。在文本到视频模型上，Motive 能够识别出对运动有强烈影响的片段，并以此指导数据筛选工作，从而提升时间一致性与物理合理性。使用 Motive 精选出的高影响力数据进行微调，我们的方法在 VBench 基准上同时提升了运动平滑度与动态程度，相较于预训练基础模型，获得了 74.1% 的人类偏好胜率。据我们所知，这是首个在视频生成模型中归因于运动而非视觉外观的框架，并利用该归因结果来构建微调数据集。

Solar Open Technical Report
Solar Open 技术报告
我们介绍了 Solar Open，这是一个拥有 1020 亿参数、面向低资源语言的双语专家混合 (Mixture-of-Experts， MoE) 大语言模型。Solar Open 展示了一种通过解决三个相互关联的挑战来构建具有竞争力大语言模型的系统性方法。首先，为了应对低资源语言数据稀缺的问题以实现有效训练，我们合成了 4.5 万亿个高质量、领域特定且面向强化学习 (RL) 的 Token。其次，我们通过课程学习 (Curriculum Learning) 来协调这些数据，在总计 20 万亿 Token 的数据上，联合优化其构成、质量阈值和领域覆盖范围。第三，为了通过可扩展的强化学习来获得推理能力，我们应用了我们提出的 SnapPO 框架进行高效优化。在英语和韩语的各项基准测试中，Solar Open 都取得了具有竞争力的性能，这证明了该方法对于推动低资源语言 AI 发展的有效性。

VIBE: Visual Instruction Based Editor
VIBE: 基于视觉指令的编辑器
基于指令的图像编辑是生成式 AI (Generative AI) 中发展最迅速的领域之一。过去一年，该领域迈上了新台阶，涌现出数十个开源模型以及能力强大的商业系统。然而，目前仅有少数开源方法能达到实用级质量。此外，作为这些管线主流选择的扩散主干网络，通常体积庞大且计算成本高昂，难以适配许多部署和研究场景；其广泛使用的版本通常包含 60 亿至 200 亿参数。本文提出了一种紧凑、高吞吐的基于指令的图像编辑管线，它采用一个现代的 20 亿参数 Qwen3-VL 模型来指导编辑过程，并使用一个 16 亿参数的扩散模型 Sana1.5 进行图像生成。我们在架构、数据处理、训练配置和评估等方面的设计决策，均以低成本推理和严格的源一致性为目标，同时确保在此规模可行的主要编辑类别上保持高质量。在 ImgEdit 和 GEdit 基准测试上的评估表明，所提方法的性能匹配甚至超越了参数量大数倍、推理成本显著更高的基线模型，并且在需要保留输入图像的编辑任务上表现尤为突出，例如属性调整、对象移除、背景编辑和针对性替换。该模型可适配 24 GB 的 GPU 显存，在 NVIDIA H100 上以 BF16 精度生成高达 2K 分辨率的编辑图像仅需约 4 秒，且无需任何额外的推理优化或模型蒸馏。

Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning
面向卓越长链思维推理的分布对齐序列蒸馏
在本报告中，我们介绍了 DASD-4B-Thinking，这是一个轻量级但能力强大、完全开源的推理模型。在数学、科学推理和代码生成等具有挑战性的基准测试中，它在同等规模的开源模型中实现了 SOTA 性能，甚至优于一些更大的模型。我们首先批判性地重新审视了社区中广泛采用的一种蒸馏范式：基于教师模型生成回答进行监督微调 (SFT)，也称为序列级蒸馏 (Sequence-Level Distillation)。尽管近期一系列遵循此方案的工作展现了显著的效率和强大的实证性能，但它们主要立足于 SFT 的视角。因此，这些方法将重点放在了设计 SFT 数据过滤的启发式规则上，而在很大程度上忽略了蒸馏本身的核心原则——即让学生模型学习教师模型的完整输出分布，从而继承其泛化能力。具体而言，我们指出了当前实践中的三个关键局限：i) 对教师模型序列级分布的表征不足；ii) 教师模型的输出分布与学生模型的学习能力之间存在错配；iii) 教师强制训练 (Teacher-Forced Training) 与自回归推理 (Autoregressive Inference) 之间的差异导致的暴露偏差。总而言之，这些不足反映了在整个蒸馏过程中系统性缺乏明确的师生交互，使得蒸馏的精髓未能得到充分利用。为了解决这些问题，我们提出了几项方法学创新，它们共同构成了一个增强的序列级蒸馏训练流程。值得注意的是，DASD-4B-Thinking 仅使用 44.8 万训练样本就取得了有竞争力的结果——这比大多数现有开源工作所使用的样本量少了一个数量级。为了支持社区研究，我们公开发布了模型和训练数据集。

KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions
KnowMe-Bench: 用于终身数字伴侣的人物理解基准测试
现有的长期记忆基准测试大多使用多轮对话或合成的用户历史，这使得检索性能并不能完美地代表模型的人物理解能力。我们提出了 \BenchName，这是一个基于长篇自传叙事构建的可公开发布的基准测试。在这些叙事中，人物的行动、背景和内心思想为推断其稳定的动机和决策原则提供了丰富的证据。\BenchName~将每个叙事重构为一个具有闪回感知和时间锚定的序列，并通过一系列与证据关联的问题来评估模型，这些问题涵盖事实回忆、主观状态归因和原则级推理。在不同来源的叙事上，检索增强系统主要提升了事实准确性，但在需要时间定位的解释和更高层次的推理方面，错误仍然存在，这凸显了对超越检索的记忆机制的需求。我们的数据发布于 \href{KnowMeBench}{github.com/QuantaAlpha…

CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature
CaricatureGS: 基于高斯曲率夸张化3D高斯泼溅人脸
本文提出了一种用于人脸的照片级真实感、可控3D漫画化框架。我们首先采用一种基于内在高斯曲率的表面夸张技术，但当其与纹理结合时，渲染结果往往过于平滑。为解决此问题，我们求助于3D高斯泼溅 (3D Gaussian Splatting, 3DGS)，该技术近期已被证明能生成逼真的自由视点化身。给定一个多视角图像序列，我们提取FLAME网格，求解曲率加权的泊松方程，从而得到其夸张形式。然而，直接对3DGS中的高斯泼溅进行变形效果不佳，因此需要通过局部仿射变换将每一帧图像扭曲为其对应的夸张2D表示，以合成伪真值漫画图像。随后，我们设计了一种交替使用真实图像和合成图像进行监督的训练方案，使得单个高斯泼溅集合能够同时表征自然状态和夸张状态的化身。该方案提升了保真度，支持局部编辑，并允许对漫画夸张程度进行连续控制。为实现实时变形，我们引入了原始表面与夸张表面之间的高效插值方法，并进一步分析表明，该方法与闭式解之间的偏差是有界的。在定量与定性评估中，我们的方法均优于现有工作，能够生成几何可控、具有照片级真实感的漫画化身。

ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking
ArenaRL: 通过基于锦标赛的相对排名实现开放式智能体的强化学习规模化
强化学习已显著提升了大语言模型智能体在结果可验证任务上的性能，但在解空间广阔的开放式智能体任务（例如复杂旅行规划）上仍举步维艰。由于此类任务缺乏客观的真值，当前的强化学习算法主要依赖于为单个响应分配标量分数的奖励模型。我们认为，这种逐点评分法存在固有的区分度崩溃问题：奖励模型难以辨别不同轨迹间的细微优势，导致组内分数被压缩至一个狭窄区间。因此，有效的奖励信号被奖励模型本身的噪声所主导，进而引发优化停滞。为解决此问题，我们提出了 ArenaRL，这是一种将评估方式从逐点标量评分转变为组内相对排名的强化学习范式。ArenaRL 引入了一种感知任务过程的成对评估机制，采用多级评分标准为轨迹分配细粒度的相对分数。此外，我们构建了一个组内对抗竞技场，并设计了一套基于锦标赛的排名方案，以获取稳定的优势信号。实证结果表明，所构建的采用种子排位的单败淘汰制，在仅需 O(N) 复杂度的情况下，其优势估计精度与需要 O(N^2) 复杂度的完全成对比较几乎相当，从而在效率与精度之间达到了最优平衡。再者，为弥补开放式智能体缺乏全流程评测基准的不足，我们构建了 Open-Travel 和 Open-DeepResearch 两个高质量基准，它们具备覆盖监督微调、强化训练及多维评估的完整流程。大量实验表明，ArenaRL 显著优于标准强化学习基线，能够使大语言模型智能体为复杂的现实世界任务生成更为稳健的解决方案。

The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning
思想的分子结构：映射长链思维推理的拓扑结构
大语言模型 (LLMs) 常常难以通过模仿人类或非长链思维 (Long CoT) 大语言模型来学习有效的长链思维 (Long CoT) 推理。为探究其原因，我们提出，在统一视角下，有效且可学习的长链思维轨迹具有类似分子的稳定结构，这些结构由三种相互作用构成：深度推理 (类共价键) 、自我反思 (类氢键) 和自我探索 (类范德华力) 。对蒸馏轨迹的分析表明，这些结构源自长链思维微调过程，而非对关键词的简单模仿。我们引入了有效语义异构体的概念，并证明只有那些能促进快速熵收敛的化学键才能支持稳定的长链思维学习，而不同结构之间的竞争则会损害训练效果。基于这些发现，我们提出了 Mole-Syn，一种基于分布转移图的方法，用于指导合成有效的长链思维结构，从而在多个基准测试中提升模型性能并增强强化学习的稳定性。

User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale
面向用户的大规模多轮对话生成与工具使用
近期，将大型推理模型 (Large Reasoning Models, LRMs) 作为自主智能体的范式转变，极大地提升了对复杂多轮工具使用能力的需求。然而，现有数据集与数据生成方法受限于静态、预定义的工具集，难以应对开放式人机协作场景的复杂性。为此，我们首先构建了一个框架，用于大规模自动化生成面向任务的多轮对话。该框架利用基于 LRM 的模拟器动态生成高价值、领域特定的工具，以解决指定任务。但我们发现，纯粹面向任务的设计往往导致"仅限任务解决"的轨迹，即智能体以最少的交互完成目标，无法生成现实场景中常见的多轮次对话。为弥补这一不足，我们转向了面向用户的模拟范式。通过将任务生成与一个专用的用户模拟器解耦——该模拟器模仿人类行为规则，如增量式提出请求和逐轮提供反馈——我们能够促成更真实、更扩展的多轮对话，从而反映现实世界问题解决的迭代特性。我们的生成流水线作为一个多功能、即插即用的模块运行，可从任意状态启动生成，确保了在产出扩展工具使用数据时的高可扩展性。此外，通过支持在单条轨迹内完成多个任务，该流程能够生成高密度数据集，以反映现实世界人机交互的多方面需求。

Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning
Fast-ThinkAct: 基于可语言化潜在规划的高效视觉-语言-动作推理
视觉-语言-动作 (Vision-Language-Action, VLA) 任务要求对复杂视觉场景进行推理，并在动态环境中执行自适应的动作。尽管近期关于推理型 VLA 的研究表明，显式的思维链 (Chain-of-Thought, CoT) 能够提升泛化能力，但其冗长的推理轨迹导致了较高的推理延迟。我们提出了 Fast-ThinkAct，一个高效的推理框架，它通过可语言化的潜在推理，实现了紧凑且高性能的规划。Fast-ThinkAct 通过从教师模型进行知识蒸馏，学习利用潜在 CoT 进行高效推理。该方法由一个偏好引导的目标驱动，旨在对齐操作轨迹，从而迁移用于具身控制的语言和视觉规划能力。这实现了推理增强的策略学习，能够有效地将紧凑的推理与动作执行相衔接。在多种具身操作与推理基准上进行的大量实验表明，Fast-ThinkAct 取得了优异的性能，与最先进的推理型 VLA 相比，推理延迟最高可降低 89.3%，同时仍能有效进行长时程规划、少样本适应和故障恢复

## 每周AI论文速递（260119-260123）
Agentic Reasoning for Large Language Models
面向大语言模型的智能体推理
推理是支撑推断、问题解决与决策制定的基本认知过程。尽管大语言模型 (LLMs) 在封闭环境设定下展现出强大的推理能力，但在开放、动态的环境中却表现欠佳。智能体推理 (Agentic Reasoning) 标志着一种范式转变，它将大语言模型重构为能够通过持续交互进行规划、行动和学习的自主智能体。本综述从三个互补的维度来梳理智能体推理。首先，我们通过三个层级来刻画环境动态：基础智能体推理，它在稳定环境中建立核心的单智能体能力，包括规划、工具使用和搜索；自我进化智能体推理，它研究智能体如何通过反馈、记忆和适应来完善这些能力；以及集体多智能体推理，它将智能延伸至涉及协调、知识共享和共同目标的协作场景。在这些层级中，我们区分了上下文推理（通过结构化编排来扩展测试时的交互）与训练后推理（通过强化学习和监督微调来优化行为）。我们进一步回顾了跨越现实世界应用与基准测试的代表性智能体推理框架，涵盖科学、机器人、医疗保健、自主研究与数学等领域。本综述将各类智能体推理方法综合成一个连接思维与行动的统一路线图，并概述了开放的挑战与未来方向，包括个性化、长周期交互、世界模型建模、可扩展的多智能体训练以及实际部署的治理机制。

Your Group-Relative Advantage Is Biased
群体相对优势估计存在偏差
基于验证器奖励的强化学习 (Reinforcement Learning from Verifier Rewards, RLVR) 已成为对大语言模型进行推理任务后训练的一种广泛应用方法，其中基于群体的方法，如 GRPO 及其变体，得到了广泛采用。这些方法依赖于群体相对优势估计 (group-relative advantage estimation) 来避免使用学习到的评论家，但其理论性质仍不明确。
本工作揭示了基于群体的强化学习的一个根本问题：群体相对优势估计量相对于真实（期望）优势存在固有偏差。我们首次进行了理论分析，证明该估计量会系统性地低估困难提示的优势，同时高估简单提示的优势，从而导致探索与利用的失衡。为解决此问题，我们提出了历史感知自适应难度加权 (History-Aware Adaptive Difficulty Weighting, HA-DW)，这是一种自适应重加权方案，能够根据一个动态变化的难度锚点和训练过程动态来调整优势估计。在五个数学推理基准测试上的理论分析与实验均表明，将 HA-DW 集成到 GRPO 及其变体中能持续提升性能。我们的结果表明，纠正有偏差的优势估计对于实现稳健、高效的 RLVR 训练至关重要。

Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization
Being-H0.5: 面向跨形态泛化的以人为中心机器人学习规模化
我们提出了 Being-H0.5，这是一个基础视觉-语言-动作（Vision-Language-Action, VLA）模型，旨在实现跨多样化机器人平台的鲁棒跨形态泛化。针对现有 VLA 模型常受限于形态异构性与数据稀缺的问题，我们提出了一种以人为中心的学习范式，将人类交互轨迹视为物理交互的通用“母语”。为此，我们推出了 UniHand-2.0，这是迄今为止规模最大的具身预训练方案，包含了跨越 30 种不同机器人形态的超过 35,000 小时多模态数据。我们的方法引入了一个统一动作空间，将异构的机器人控制映射到语义对齐的槽位，从而使低资源机器人能够从人类数据和高资源平台中自举学习技能。基于此以人为中心的基础，我们设计了一个统一的序列建模与多任务预训练范式，以弥合人类演示与机器人执行之间的差距。在架构上，Being-H0.5 采用了一种混合 Transformer（Mixture-of-Transformers）设计，其核心是新颖的混合流（Mixture-of-Flow, MoF）框架，用于将共享的运动基元与专门的形态特定专家解耦。最后，为确保跨形态策略在现实世界中的稳定性，我们引入了流形保持门控（Manifold-Preserving Gating）以应对感知变化下的鲁棒性挑战，以及通用异步分块（Universal Async Chunking）以实现跨不同延迟与控制特性的机器人形态的通用分块控制。实验结果表明，Being-H0.5 在模拟基准测试（如 LIBERO (98.9%) 和 RoboCasa (53.9%)）上取得了最先进的性能，同时在五个机器人平台上展现出强大的跨形态能力。

EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience
EvoCUA: 通过从可扩展合成经验中学习演进计算机使用智能体
原生计算机使用智能体 (CUA) 的发展是多模态 AI 领域的一次重大飞跃。然而，其潜力目前受限于静态数据扩展的瓶颈。主要依赖对静态数据集进行被动模仿的现有范式，难以捕捉长程计算机任务中固有的复杂因果动态。本文中，我们提出了 EvoCUA，一个原生计算机使用智能体模型。与静态模仿不同，EvoCUA 将数据生成与策略优化整合为一个自我维持的演进循环。为缓解数据稀缺问题，我们开发了一个可验证的合成引擎，能自主生成多样化任务并附带可执行的验证器。为实现大规模经验获取，我们设计了一个可扩展的基础设施，可协调数万个异步沙盒模拟运行。基于这些大规模轨迹，我们提出了一种迭代演进学习策略，以高效吸收这些经验。该机制通过识别能力边界来动态调节策略更新——强化成功的行为模式，同时通过错误分析与自我纠正将失败轨迹转化为丰富的监督信号。在 OSWorld 基准测试上的实证评估表明，EvoCUA 实现了 56.7% 的成功率，创造了新的开源模型最佳性能。值得注意的是，EvoCUA 显著优于此前最佳的开源模型 OpenCUA-72B (45.0%)，并且超越了领先的闭源权重模型，如 UI-TARS-2 (53.1%)。关键的是，我们的结果证明了该方法的泛化能力：这种由经验学习驱动的演进范式，在不同规模的基础模型上均能实现一致的性能提升，从而为增强原生智能体能力开辟了一条稳健且可扩展的路径。

ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development
ABC-Bench: 面向真实世界开发环境的智能体后端编码基准测试
大语言模型 (LLMs) 向自主 AI 智能体 (AI Agent) 的演进，已将人工智能 (AI) 编码的范畴从局部代码生成，扩展至复杂的、仓库级别的、由执行驱动的问题求解。然而，现有的基准测试主要评估静态上下文中的代码逻辑，忽视了真实世界工程项目中动态的、全流程的需求，尤其是在需要严格环境配置与服务部署的后端开发领域。为弥补这一不足，我们提出了 ABC-Bench，这是一个专为在真实、可执行的工作流中评估智能体后端编码能力而设计的基准测试。通过一个可扩展的自动化流水线，我们从开源仓库中构建了 224 个实际任务，涵盖 8 种编程语言和 19 个框架。与以往的评估不同，ABC-Bench 规定智能体必须管理从仓库探索到部署容器化服务的完整开发生命周期，并且要通过外部的端到端 API 测试。我们的大量评估结果表明，即便是最先进的模型，在面对这些综合性任务时也难以提供稳定可靠的性能，这凸显了当前模型能力与实际后端工程需求之间存在的显著差距。我们的代码开源在 github.com/OpenMOSS/AB…

HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding
HERMES: 将 KV 缓存作为分层内存以实现高效流式视频理解
多模态大语言模型 (Multimodal Large Language Models, MLLMs) 的最新进展，在离线视频理解任务上取得了显著进步。然而，将其能力扩展至流式视频输入仍面临挑战，因为现有模型难以在保持稳定理解性能的同时，兼顾实时响应与较低的 GPU 内存开销。为应对这一挑战，我们提出了 HERMES，一种新颖的免训练架构，旨在实现对视频流的实时、准确理解。基于对注意力机制的机理探究，我们将 KV 缓存 (KV Cache) 概念化为一个分层内存框架，该框架能以多种粒度封装视频信息。在推理过程中，HERMES 通过重用紧凑的 KV 缓存，在有限资源下实现了高效的流式理解。值得注意的是，HERMES 在用户查询到达时无需任何辅助计算，从而确保了连续视频流交互的实时响应能力，其首次令牌生成时间 (Time To First Token, TTFT) 比之前的 SOTA 方法快 10 倍。即使与均匀采样相比，视频令牌数量减少了高达 68%，HERMES 在所有基准测试中仍取得了相当或更优的准确率，在流式数据集上的性能提升最高达 11.4%。

Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey
基于大语言模型的软件工程问题解决：进展与前沿综述
问题解决是现实软件开发中一项不可或缺的复杂软件工程任务，现已成为人工智能面临的一项重大挑战。SWE-bench等基准测试的建立表明，此项任务对大语言模型而言极具难度，从而极大地推动了自主编码智能体的演进。本文对这一新兴领域进行了系统性综述。首先，我们考察了数据构建流程，包括自动收集与合成方法。接着，我们全面分析了相关方法，范围从具备模块化组件的免训练框架，到基于训练的技术（如监督微调和强化学习）。随后，我们对数据质量和智能体行为进行了批判性分析，并探讨了实际应用。最后，我们指出了当前面临的关键挑战，并展望了未来有前景的研究方向。我们在 github.com/DeepSoftwar… 维护了一个开源仓库，作为该领域的动态资源。

The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models
灵活性陷阱：为何任意顺序会限制扩散语言模型的推理潜力
扩散大语言模型 (Diffusion Large Language Models, dLLMs) 打破了传统大语言模型严格的从左到右约束，允许以任意顺序生成 Token。直观上看，这种灵活性意味着其解空间严格包含了固定的自回归轨迹，理论上能为数学和编码等通用任务释放更强大的推理潜力。因此，许多研究都利用强化学习 (Reinforcement Learning, RL) 来挖掘 dLLMs 的推理能力。本文揭示了一个反直觉的事实：在当前形式下，任意顺序生成非但没有扩大，反而缩小了 dLLMs 的推理边界。我们发现，dLLMs 倾向于利用这种顺序灵活性来规避对探索至关重要的高不确定性 Token，从而导致解空间过早坍缩。这一发现挑战了现有 dLLMs 强化学习方法的前提，这些方法通常为了保持这种灵活性而引入了相当大的复杂性，例如处理组合轨迹和难解的似然问题。我们证明，通过有意放弃任意顺序，转而应用标准的组相对策略优化 (Group Relative Policy Optimization, GRPO)，可以更有效地激发推理能力。我们的方法 JustGRPO 设计极其简洁，却效果惊人（例如，在 GSM8K 上达到 89.1% 的准确率），同时完全保留了 dLLMs 的并行解码能力。项目页面：nzl-thu.github.io/the-flexibi…

RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation
RubricHub: 通过自动化由粗到细生成构建的全面高区分度评分标准数据集
具有可验证奖励的强化学习 (Reinforcement Learning with Verifiable Rewards, RLVR) 在数学等推理密集型领域已取得重大进展。然而，由于缺乏真实标签，优化开放式生成任务仍面临挑战。基于评分标准的评估虽为验证提供了一种结构化替代方案，但现有方法受限于可扩展性瓶颈和粗糙的评判标准，导致了监督性能瓶颈。为解决此问题，我们提出了一种自动化的由粗到细评分标准生成框架。该框架协同利用原则引导的合成、多模型聚合与难度演化，能够生成全面且高区分度的评判标准，从而捕捉生成内容中的细微差别。基于此框架，我们发布了 RubricHub，这是一个大规模（约 11 万条）且覆盖多领域的数据集。我们通过一个两阶段的后训练流程验证了其有效性，该流程包含基于评分标准的拒绝采样微调 (Rubric-based Rejection Sampling Fine-Tuning, RuFT) 和强化学习 (Rubric-based Reinforcement Learning, RuRL)。实验结果表明，RubricHub 能显著提升模型性能：经后训练的 Qwen3-14B 模型在 HealthBench 基准上取得了最先进 (state-of-the-art, SOTA) 的性能（69.3 分），超越了 GPT-5 等专有的前沿模型。相关代码与数据即将发布。

LLM-in-Sandbox Elicits General Agentic Intelligence
LLM-in-Sandbox 激发通用智能体能力
我们提出了 LLM-in-Sandbox 方法，使大语言模型能够在代码沙盒（即虚拟计算机）内进行探索，从而在非代码领域激发通用智能。我们首先证明，强大的大语言模型无需额外训练，即可展现出利用代码沙盒处理非代码任务的泛化能力。例如，大语言模型能够自主访问外部资源以获取新知识，利用文件系统处理长上下文，并执行脚本来满足特定格式要求。我们进一步表明，这些 AI 智能体能力可以通过 LLM-in-Sandbox 强化学习（LLM-in-Sandbox-RL）得到增强，该方法仅使用非智能体行为数据来训练模型进行沙盒探索。实验表明，LLM-in-Sandbox 在免训练和训练后两种设置下，均能实现稳健的泛化，其能力覆盖数学、物理、化学、生物医学、长上下文理解及指令遵循等多个领域。最后，我们从计算和系统两个角度分析了 LLM-in-Sandbox 的效率，并将其开源为一个 Python 软件包，以促进实际应用部署。

BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries
BayesianVLA: 基于潜在动作查询的视觉-语言-动作模型贝叶斯分解
视觉-语言-动作 (Vision-Language-Action, VLA) 模型在机器人操作任务中展现出潜力，但其泛化能力常受限于新指令或复杂的多任务场景。我们指出当前训练范式存在一个关键缺陷：目标驱动的数据收集导致了数据集偏差。在此类数据集中，仅凭视觉观察就足以高度预测出语言指令，致使指令与动作之间的条件互信息趋于零，我们将此现象称为信息坍缩 (Information Collapse)。其结果是，模型退化为仅依赖视觉的策略，忽略了语言约束，从而在分布外 (Out-of-Distribution, OOD) 场景中失效。为解决此问题，我们提出了 BayesianVLA，这是一个通过贝叶斯分解来确保模型遵循指令的新框架。通过引入可学习的潜在动作查询 (Latent Action Queries)，我们构建了一个双分支架构，分别估计仅视觉先验 p(a∣v)p(a \mid v)p(a∣v) 和语言条件后验 π(a∣v,ℓ)π(a \mid v, \ell)π(a∣v,ℓ)。随后，我们优化策略以最大化动作与指令之间的条件点互信息 (Pointwise Mutual Information, PMI)。该目标有效地抑制了视觉捷径，并奖励那些能明确体现语言指令的动作。BayesianVLA 无需额外数据即可显著提升泛化性能。在 SimplerEnv 和 RoboCasa 上进行的大量实验证明了其显著的性能提升，其中在极具挑战性的 OOD SimplerEnv 基准测试上实现了 11.3% 的性能增益，验证了我们的方法能够稳健地将语言关联到动作。

Toward Efficient Agents: Memory, Tool learning, and Planning
迈向高效智能体：记忆、工具学习与规划
近年来，将大语言模型扩展为智能体 (AI Agent) 系统的研究兴趣日益浓厚。尽管智能体的有效性在持续提升，但对于实际部署至关重要的效率却常被忽视。因此，本文从智能体的三个核心组件——记忆、工具学习和规划——出发，研究其效率问题，并综合考虑延迟、Token 消耗、步骤数等成本。为了对智能体系统本身的效率进行全面研究，我们回顾了近期的一系列方法。这些方法在具体实现上各异，但在高级设计原则上往往趋同，包括但不限于：通过压缩和管理来限制上下文、设计强化学习奖励以最小化工具调用，以及采用受控搜索机制来提升效率。我们将对这些原则进行详细讨论。
相应地，我们从两个互补的维度来刻画效率：一是在固定成本预算下比较其有效性；二是在达到可比有效性水平时比较其成本消耗。这种权衡关系也可以从有效性与成本之间的帕累托前沿 (Pareto frontier) 来理解。基于此视角，我们还审视了面向效率的基准评测：通过总结针对上述组件的评估方案，并整合来自基准研究和方法论文献中常报告的各项效率指标。
此外，我们讨论了该领域面临的关键挑战与未来研究方向，旨在为相关研究提供有价值的见解。

MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents
MMDeepResearch-Bench：多模态深度研究智能体基准
深度研究智能体 (Deep Research Agents, DRAs) 通过多步骤搜索与信息合成来生成包含丰富引用的报告。然而，现有基准主要面向纯文本场景或短格式多模态问答，缺乏对端到端多模态证据使用的评估。为此，我们提出了 MMDeepResearch-Bench (MMDR-Bench)，这是一个包含 21 个领域、共计 140 项由专家精心设计任务的基准。每项任务提供一个图文数据包 (image-text bundle)，用于评估模型的多模态理解能力以及基于引用的报告生成能力。与以往的设置相比，MMDR-Bench 强调具备明确证据使用的报告式合成，要求模型必须将视觉内容与有来源支撑的论断相关联，并在叙述、引用和视觉参考之间保持一致性。我们进一步提出了一套统一且可解释的评估流程：用于评估报告质量的公式化-LLM自适应评估 (Formula-LLM Adaptive Evaluation, FLAE)，用于评估引用与证据对齐的可信检索对齐引用评估 (Trustworthy Retrieval-Aligned Citation Evaluation, TRACE)，以及用于检查文本-视觉一致性的多模态支持对齐完整性检查 (Multimodal Support-Aligned Integrity Check, MOSAIC)。每个评估环节都能产生细粒度的指标，支持进行超越单一总分的精细化错误诊断。我们在 25 个前沿模型上进行了实验，结果揭示了生成质量、引用规范性与多模态基础之间的系统性权衡。这些发现突出表明，仅能生成流畅的文本并不能保证对证据的忠实使用，并且多模态一致性仍然是深度研究智能体面临的一个关键瓶颈。

The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents
毒苹果效应：通过 AI 智能体技术扩张对中介市场进行战略操纵
AI 智能体 (AI Agents) 融入经济市场，从根本上改变了战略互动的格局。我们在三个经典的博弈论场景中，研究了扩展可用技术集合所带来的经济影响，这些场景包括：议价 (资源分配)、谈判 (非对称信息交易) 以及说服 (战略信息传递)。研究发现，仅仅增加可供选择的 AI 智能体，就能显著改变均衡收益和监管结果，这常常会激励监管机构主动开发和发布新技术。与之相对，我们识别出一种被称为“毒苹果”效应的战略现象：某个智能体可能会发布一项新技术，而这项技术最终既不会被其自身采用，也不会被其对手采用，其唯一目的在于操纵监管机构，使其选择有利于该智能体的市场设计方案。这种战略性发布行为，以牺牲对手利益和违背监管机构公平目标为代价，提升了发布者自身的福利。我们的研究结果表明，静态的监管框架易受技术扩张的操纵，因此需要能够适应 AI 能力动态发展的市场设计。

