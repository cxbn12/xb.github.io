https://juejin.cn/post/7593607642553892890

## 每周AI论文速递（251201-251205）

From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence
从代码基础模型到智能体与应用：代码智能实用指南
大语言模型 (LLMs) 从根本上改变了自动化软件开发，实现了将自然语言描述直接转换为功能代码，并通过诸如 GitHub Copilot (Microsoft)、Cursor (Anysphere)、Trae (ByteDance) 和 Claude Code (Anthropic) 等工具推动了商业应用。该领域已从基于规则的系统演进为基于 Transformer 的架构，在 HumanEval 等基准测试上的性能从个位数成功率提升至超过 95%。在本工作中，我们提供了一份关于代码大语言模型的全面综述与实用指南（包含一系列分析和探索性实验），系统性地审视了从数据管理到后训练的完整模型生命周期，涉及高级提示范式、代码预训练、监督微调、强化学习以及自主编码智能体。我们评估了通用大语言模型（GPT-4、Claude、LLaMA）与代码专用大语言模型（StarCoder、Code LLaMA、DeepSeek-Coder、QwenCoder）的代码能力，并批判性地探讨了相关技术、设计决策及其权衡。此外，我们阐明了学术研究（例如基准测试与任务）与实际部署（例如软件相关的代码任务）之间的差距，涵盖代码正确性、安全性、大型代码库的上下文感知以及与开发工作流的集成，并将有前景的研究方向与实际需求相关联。最后，我们通过一系列实验，对代码预训练、监督微调和强化学习进行了全面分析，内容涵盖缩放定律、框架选择、超参数敏感性、模型架构和数据集比较。

DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models
DeepSeek-V3.2: 推动开源大语言模型的前沿
我们推出 DeepSeek-V3.2，这是一个将高效计算与卓越的推理及智能体性能融为一体的模型。DeepSeek-V3.2 的关键技术突破如下：(1) DeepSeek 稀疏注意力 (DSA)：我们引入了 DSA，这是一种高效的注意力机制，能在长上下文场景中显著降低计算复杂度，同时保持模型性能。(2) 可扩展的强化学习框架：通过采用稳健的强化学习协议并扩展后训练阶段的计算规模，DeepSeek-V3.2 的性能与 GPT-5 相当。值得注意的是，我们的高计算配置变体 DeepSeek-V3.2-Speciale 超越了 GPT-5，其推理能力与 Gemini-3.0-Pro 并驾齐驱，在 2025 年国际数学奥林匹克竞赛 (IMO) 和国际信息学奥林匹克竞赛 (IOI) 中均取得了金牌级别的成绩。(3) 大规模面向智能体的任务合成流水线：为了将推理能力融入工具使用场景，我们开发了一种新颖的合成流水线，能够系统性地大规模生成训练数据。该方法支持可扩展的智能体后训练，从而在复杂、交互式的环境中，显著提升了模型的泛化能力和遵循指令的鲁棒性。

LongVT: 通过原生工具调用实现“长视频思维”
LongVT: 通过原生工具调用实现“长视频思维”
大模态模型 (Large Multimodal Models, LMMs) 在结合文本思维链进行视频推理方面展现出巨大潜力。然而，它们仍易产生幻觉，尤其是在处理那些证据稀疏且时间分布分散的长视频时。受人类理解长视频方式（先全局概览，再细查相关片段）的启发，我们提出了 LongVT，一个端到端的智能体框架。该框架通过交错进行的“多模态工具调用思维链”，实现了“用长视频进行思考”。具体而言，我们将 LMMs 固有的时间定位能力作为一种原生视频裁剪工具，用于聚焦特定视频片段并重新采样更细粒度的视频帧。这种从全局到局部的推理循环会持续进行，直至答案能够基于检索到的视觉证据得到确认。针对长视频推理任务中细粒度问答 (QA) 数据稀缺的问题，我们构建并将发布一个名为 VideoSIAH 的数据套件，以支持模型的训练与评估。具体来说，我们的训练数据集包含：24.79 万个用于工具集成冷启动监督微调的样本、1600 个用于智能体强化学习的样本，以及 1.54 万个用于智能体强化微调的样本。我们的评估基准包含 1280 个 QA 对，这些数据通过一个半自动的数据流水线精心构建，并经过了人工验证。通过精心设计的三阶段训练策略和广泛的实证评估，LongVT 在四个具有挑战性的长视频理解与推理基准测试中，均持续超越现有的强基线模型。我们的代码、数据及模型检查点均已公开，地址为：github.com/EvolvingLMM…

Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer
Z-Image: 基于单流扩散Transformer的高效图像生成基础模型
当前，高性能图像生成领域主要由专有系统主导，例如 Nano Banana Pro 和 Seedream 4.0。而领先的开源替代方案，如 Qwen-Image、Hunyuan-Image-3.0 和 FLUX.2，则普遍参数量庞大 (200亿至800亿)，导致其在消费级硬件上进行推理和微调都极不现实。为弥补这一空白，我们提出了 Z-Image，这是一个参数规模为 60亿 的高效基础生成模型。它基于可扩展单流扩散Transformer (Scalable Single-Stream Diffusion Transformer, S3-DiT) 架构构建，旨在挑战“不计成本堆叠规模”的行业范式。通过对整个模型生命周期进行系统性优化——从精心构建的数据基础设施到高效精简的训练方案——我们仅消耗了 31.4万 H800 GPU 小时 (约合 63万美元) 便完成了完整的训练流程。我们结合奖励后训练的少步蒸馏方案进一步得到了 Z-Image-Turbo，该版本不仅能在企业级 H800 GPU 上实现亚秒级推理延迟，还能兼容显存小于 16GB 的消费级硬件。此外，我们的全预训练范式也支持高效训练出 Z-Image-Edit，这是一个具备出色指令遵循能力的图像编辑模型。定性与定量实验均表明，我们的模型在多个维度上的性能均可比肩乃至超越领先的竞争对手。尤为突出的是，Z-Image 在生成逼真图像和渲染双语文本方面表现卓越，其效果足以媲美顶尖的商业模型。这证明了，即使大幅降低计算开销，同样能够实现最先进的性能。为促进可获取、低成本且性能领先的生成模型发展，我们已公开代码、模型权重并提供在线演示。

Live Avatar: 流式实时音频驱动的无限长度虚拟形象生成
Live Avatar: 流式实时音频驱动的无限长度虚拟形象生成
现有基于扩散模型的视频生成方法，从根本上受限于顺序计算和长时不一致性问题，制约了其在实时流式音频驱动虚拟形象合成中的实际采用。我们提出了 Live Avatar，一个算法-系统协同设计框架，能够利用一个 140 亿参数的扩散模型，实现高效、高保真且无限长度的虚拟形象生成。我们的方法引入了时间步强制流水线并行 (Timestep-forcing Pipeline Parallelism, TPP)，这是一种分布式推理范式，它跨多个 GPU 对去噪步骤进行流水线处理，从而有效打破自回归瓶颈，确保稳定、低延迟的实时流生成。为了进一步增强时间一致性，并缓解身份漂移和颜色伪影问题，我们提出了滚动锚定帧机制 (Rolling Sink Frame Mechanism, RSFM)，该机制通过利用缓存的参考图像动态重校准外观，以维持序列保真度。此外，我们采用自强制分布匹配蒸馏技术，在不牺牲视觉质量的前提下，实现了大规模模型的因果性、可流式化适配。Live Avatar 展现了最先进的性能，在 5 块 H800 GPU 上实现了 20 FPS 的端到端生成速度。据我们所知，这是首个在此参数量级上实现实用化、实时、高保真虚拟形象生成的工作。我们的研究为在工业级长视频合成应用中部署先进扩散模型确立了一种新范式。

DAComp: 全数据智能生命周期数据智能体基准测试
DAComp: 全数据智能生命周期数据智能体基准测试
现实中的企业数据智能工作流涵盖两大环节：一是将原始数据源转化为可供分析表格的数据工程，二是将这些表格转化为决策洞察的数据分析。为此，我们提出了DAComp基准，它包含210个任务，旨在模拟这些复杂的工作流。数据工程（DE）任务要求对工业级数据模式进行仓库层面的工程操作，包括从零开始设计和构建多阶段SQL管道，以及在需求演进时对现有系统进行改造。数据分析（DA）任务则提出开放式的业务问题，要求进行战略规划、通过迭代编码进行探索性分析、解读中间结果，并最终综合出可执行的建议。工程类任务采用基于执行的多指标评估体系进行评分。开放式任务则由一个经过实验验证的、可靠的LLM-judge进行评估，该评估器遵循一套分层且精心设计的评分标准。
我们的实验表明，即使是当前最先进的AI智能体在DAComp上也表现欠佳。数据工程任务的表现尤其低下，成功率不足20%，这暴露了在整体管道编排（而不仅仅是代码生成）方面存在关键瓶颈。数据分析任务的平均得分也低于40%，突显了智能体在开放式推理方面存在严重不足，并证明工程能力与分析能力是两种截然不同的技能。通过清晰地诊断这些局限，DAComp为开发真正适用于企业环境的、能力全面的自主数据智能体提供了一个严谨且贴近现实的测试平台。我们的数据与代码公开于 da-comp.github.io。

Qwen3-VL Technical Report
Qwen3-VL 技术报告
我们推出 Qwen3-VL，这是 Qwen 系列迄今为止能力最强的视觉语言模型 (Vision-Language Model)，在广泛的多模态基准测试中均取得了卓越性能。该模型原生支持高达 256K token 的交错多模态上下文，能够无缝融合文本、图像与视频输入。模型系列包含密集 (2B/4B/8B/32B) 与专家混合 (Mixture-of-Experts, MoE) (30B-A3B/235B-A22B) 两种架构变体，以满足不同场景下对延迟与质量的权衡需求。Qwen3-VL 的核心优势体现在三个方面：(i) 显著增强的纯文本理解能力，在多项测试中超越了同等规模的纯文本骨干模型；(ii) 强大的长上下文理解能力，其原生 256K token 窗口同时支持纯文本及交错的多模态输入，可对长文档和视频内容实现准确的信息保持、检索与交叉引用；(iii) 先进的跨模态推理能力，在单图、多图及视频任务上均表现优异，在 MMMU 及视觉数学基准 (如 MathVista 和 MathVision) 等综合评估中取得了领先的性能表现。在模型架构层面，我们进行了三项关键升级：(i) 引入增强型交错-MRoPE (interleaved-MRoPE)，以提升对图像与视频的时空建模能力；(ii) 集成 DeepStack 模块，通过有效利用多层次 ViT 特征来强化视觉与语言的对齐；(iii) 为视频任务设计了基于文本的时间对齐机制，从 T-RoPE 演进为显式的文本时间戳对齐，从而实现更精准的时序定位。在相近的 token 预算与延迟约束下，Qwen3-VL 在密集与 MoE 架构中均展现出卓越的性能。我们期待 Qwen3-VL 能够成为实际工作流中，支撑图像推理、智能体决策以及多模态代码智能的基础引擎。

ToolOrchestra: 通过高效的模型与工具编排增强智能
ToolOrchestra: 通过高效的模型与工具编排增强智能
大语言模型 (LLM) 是强大的通用系统，但在解决诸如“人类终极考试” (Humanity's Last Exam, HLE) 这类深刻且复杂的问题时，仍面临概念上的挑战和较高的计算成本。我们的研究表明，通过小型编排器来管理其他模型与多样化工具，不仅能够突破智能水平的极限，还能提升解决复杂 AI 智能体 (AI Agent) 任务的效率。本文提出了 ToolOrchestra，一种用于训练小型编排器以协调智能工具的方法。该方法明确采用强化学习，并设计了基于结果、效率和用户偏好的奖励机制。基于 ToolOrchestra，我们训练出了 Orchestrator，这是一个拥有 80 亿参数的模型。对于给定的查询，Orchestrator 能以比以往的工具使用智能体更低的成本获得更高的准确率，同时其工具选择行为能与用户偏好保持一致。在 HLE 基准测试中，Orchestrator 取得了 37.1% 的得分，超越了 GPT-5 (35.1%)，并且效率是后者的 2.5 倍。在 tau2-Bench 和 FRAMES 基准上，Orchestrator 以显著优势领先于 GPT-5，而成本仅约为后者的 30%。深入分析表明，Orchestrator 在多项指标下实现了性能与成本的最佳权衡，并且对未见过的工具展现出强大的泛化能力。这些结果证明，利用轻量级编排模型组合多样化工具，相比现有方法不仅效率更高，而且效果更好，从而为构建实用、可扩展的工具增强推理系统铺平了道路。

Envision: 面向因果世界过程洞察的统一理解与生成基准
Envision: 面向因果世界过程洞察的统一理解与生成基准
当前的多模态模型旨在通过统一理解与生成来克服单模态表示的局限，通常采用文本到图像 (T2I) 任务来确保语义一致性。然而，其在训练和评估中对静态单图像生成的依赖，导致了对静态模式匹配与语义融合的过拟合，同时也从根本上限制了模型对随时间演变的动态过程进行建模的能力。为应对这些局限，我们提出了 Envision——一个用于链式文本到多图像生成的因果事件进展基准。该基准基于世界知识，并以时空因果关系为结构，它整合了现有的评估维度，并包含了涵盖六个科学与人文领域的 1000 个四阶段提示。为了将评估从单图像扩展到序列帧，并检验模型是否在遵循因果-时间约束的同时真正内化了世界知识，我们引入了 Envision-Score，这是一个综合了多维一致性、物理合理性与美学质量的整体评估指标。对 15 个模型 (10 个专用 T2I 模型，5 个统一模型) 的全面评估表明：专用 T2I 模型在美学渲染上表现熟练，但缺乏深层的世界知识。统一多模态模型则弥补了这一不足，在因果叙事连贯性上持续优于专用模型。然而，即便是这些统一架构，其性能仍落后于闭源模型，且难以克服时空一致性的核心挑战。这表明，专注于因果孤立的单图像会阻碍多帧推理与生成，导致模型偏向于静态模式匹配而非动态世界建模，最终限制了世界知识的内化与内容的生成。

Stabilizing Reinforcement Learning with LLMs: Formulation and Practices
使用大语言模型稳定强化学习：公式化与方法
本文提出了一种基于大语言模型 (LLM) 的强化学习 (RL) 新公式，阐释了为何以及何种条件下，可以通过 REINFORCE 等策略梯度方法中的代理 token 级别目标函数来优化真实的序列级别奖励。具体而言，通过一阶近似分析，我们证明，只有当训练-推断差异与策略滞后性均被最小化时，该代理目标函数的有效性才能得到保证。这一见解从原理上解释了几种广泛采用的 RL 训练稳定化技术的关键作用，包括重要性采样校正、梯度裁剪，以及特别针对混合专家 (Mixture-of-Experts, MoE) 模型的路由重放。通过在一个总计消耗数十万 GPU 小时的 300 亿参数 MoE 模型上进行大量实验，我们发现：对于同策略 (on-policy) 训练，带有重要性采样校正的基本策略梯度算法能实现最高的训练稳定性。而当引入异策略 (off-policy) 更新以加速收敛时，结合梯度裁剪与路由重放对于缓解因策略滞后性引起的不稳定性至关重要。值得注意的是，一旦训练趋于稳定，无论采用何种冷启动初始化方式，持续优化总能获得相当的最终性能。我们希望，所分享的见解与开发的稳定 RL 训练方案能促进未来的研究。

DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning
DeepSeekMath-V2: 迈向可自我验证的数学推理
大语言模型在数学推理方面取得了显著进展。数学推理是人工智能的一个重要测试平台，其进一步发展可能影响科学研究。通过采用奖励最终正确答案的强化学习来扩展推理能力，大语言模型在一年内从性能不佳提升到在 AIME、HMMT 等定量推理竞赛中达到性能饱和。然而，这种方法存在根本性局限。追求更高的最终答案准确率无法解决一个关键问题：正确答案并不能保证推理过程正确。此外，许多数学任务（如定理证明）需要严格的逐步推导，而非仅仅给出数值答案，这使得基于最终答案的奖励机制无法适用。为了突破深度推理的瓶颈，我们认为有必要对数学推理的全面性和严谨性进行验证。自我验证对于扩展测试阶段的计算资源尤为重要，特别是对于那些没有已知解的开放性问题。为实现可自我验证的数学推理，我们研究了如何训练一个准确、可靠的大语言模型验证器用于定理证明。接着，我们以该验证器作为奖励模型来训练一个证明生成器，并激励生成器在最终确定证明前，尽可能多地识别并解决自身证明中的问题。为防止生成器能力增强导致生成与验证之间的差距缩小，我们提出通过增加验证阶段的计算资源，自动标注新产生的、难以验证的证明，从而创建训练数据以持续改进验证器。由此得到的模型 DeepSeekMath-V2 展现出强大的定理证明能力：在增加测试阶段计算资源的情况下，它在 IMO 2025 和 CMO 2024 上获得了金牌级分数，并在 Putnam 2024 上取得了接近满分的 118/120 分。

Nex-N1: 通过大规模环境构建的统一生态系统训练的智能体模型
Nex-N1: 通过大规模环境构建的统一生态系统训练的智能体模型
大语言模型 (LLMs) 从被动响应者向自主智能体的演进，要求学习范式发生根本性转变——即从静态模仿转向激励驱动的决策。然而，由于缺乏能够为有效策略学习生成高质量交互信号的可扩展基础设施，这一转变受到了严重阻碍。为解决此问题，我们提出了一种综合性方法，旨在系统化地提升交互环境的多样性和复杂性。该方法通过解决三个相互独立且互补的维度来实现规模化构建：(1) 复杂性：NexAU，一个灵活的智能体框架，支持通过简单配置构建复杂的智能体层次结构；(2) 多样性：NexA4A 能够从自然语言自动生成多样化的智能体层次结构，从而覆盖无限领域；(3) 保真度：NexGAP 通过集成动态的真实世界环境来合成具身轨迹，以此弥合仿真与现实之间的鸿沟。我们基于该基础设施所建立的多样且复杂的交互环境对 Nex-N1 进行了训练。在 SWE-bench 和 tau2 等基准测试上的实证结果表明，Nex-N1 始终优于最先进的开源模型，并且在复杂的智能体任务上，其性能可与前沿的专有模型相竞争。我们开源了 Nex 生态系统及模型权重，以推动相关领域的进一步研究。

MultiShotMaster: 可控多镜头视频生成框架
MultiShotMaster: 可控多镜头视频生成框架
当前的视频生成技术擅长生成单镜头片段，但难以生成具有叙事性的多镜头视频。这类视频需要灵活的镜头安排、连贯的叙事逻辑以及超越文本描述的控制能力。为应对这些挑战，我们提出了 MultiShotMaster，一个具备高可控性的多镜头视频生成框架。我们通过集成两种新颖的 RoPE (Rotary Position Embedding) 变体，扩展了一个预训练的单镜头模型。首先，我们引入了多镜头叙事 RoPE，它在镜头切换时施加显式的相位偏移，从而在维持时序叙事的前提下实现灵活的镜头编排。其次，我们设计了时空位置感知 RoPE，以融入参考 token 和 grounding (接地) 信号，从而实现基于时空位置的参考信息注入。此外，为克服数据稀缺问题，我们构建了一套自动化数据标注流水线，用于从视频数据中解析出多镜头视频片段、对应字幕、跨镜头 grounding 信号以及参考图像。我们的框架利用其内在的架构特性来支持多镜头视频生成，主要特点包括：文本驱动的镜头间一致性、支持运动控制的自定义主体（如物体或角色），以及背景驱动的自定义场景。镜头数量和时长均可灵活配置。大量实验表明，我们的框架在性能和可控性方面均表现优异。

Deep Research: A Systematic Survey
深度研究：一项系统性综述
大语言模型（LLMs）已迅速从文本生成工具演变为强大的问题求解器。然而，许多开放性任务要求具备批判性思维、整合多源信息并产生可验证的输出，这超出了单次提示（single-shot prompting）或标准检索增强生成（RAG）的能力范围。近期，大量研究开始探索深度研究（Deep Research， DR），其核心目标是将大语言模型的推理能力与搜索引擎等外部工具相结合，从而赋能大语言模型成为能够完成复杂、开放式任务的研究智能体（research agents）。本综述对深度研究系统进行了全面而系统的梳理，内容涵盖清晰的路线图、基础组件、实际实现技术、关键挑战以及未来方向。具体而言，我们的主要贡献如下：（i）形式化了一个三阶段路线图，并明确了深度研究与相关范式的区别；（ii）介绍了四个关键组件：查询规划、信息获取、记忆管理与答案生成，并为每个组件提供了细粒度的子类别划分；（iii）总结了包括提示工程、监督微调以及智能体强化学习在内的优化技术；（iv）整合了相关的评估标准与开放挑战，旨在为未来的发展提供指导与便利。鉴于深度研究领域正在快速发展，我们将持续维护并更新本综述，以同步反映该领域的最新进展。

How Far Are We from Genuinely Useful Deep Research Agents?
我们距离真正有用的深度研究智能体还有多远？
深度研究智能体 (Deep Research Agents, DRAs) 旨在通过迭代的信息检索与综合，自动生成分析师级别的报告。然而，现有的大多数 DRAs 仅在问答基准上进行验证，而生成综合性报告的研究仍被忽视。更严重的是，当前用于报告综合的基准存在任务复杂性和评估指标主观性强的问题，这既无法反映真实用户需求，也限制了生成报告的实际效用。为弥补这些不足，我们提出了细粒度深度研究基准 (Fine-grained DEepResearch bench, FINDER)。该基准经过增强，包含 100 个人工精心策划的研究任务，并配有 419 个结构化检查项，用以标准化报告的结构、分析深度和事实依据。基于主流 DRAs 生成的大约 1,000 份报告，我们进一步提出了深度研究失败分类法 (Deep rEsearch Failure Taxonomy, DEFT)，这是首个专门针对深度研究智能体的失败模式分类体系。DEFT 涵盖了推理、检索和生成三个方面的 14 种细粒度失败模式，其构建基于扎根理论，并采用了人工与大语言模型协同标注以及标注者间一致性检验。我们的实验结果表明，当前 DRAs 的主要挑战并非任务理解，而是证据整合、验证以及在推理过程中保持稳健的规划能力。

TUNA: 构建统一视觉表示以实现原生统一多模态模型
TUNA: 构建统一视觉表示以实现原生统一多模态模型
统一多模态模型 (UMMs) 旨在单个框架内同时实现多模态理解与生成。我们提出了 TUNA，一个原生 UMM，它通过将 VAE 编码器与表示编码器级联，构建了一个统一的连续视觉表示。这一统一的表示空间支持对图像和视频进行端到端处理，以同时服务于理解和生成任务。与先前采用解耦表示的 UMMs 相比，TUNA 的统一视觉空间避免了因使用独立编码器而导致的表示格式不匹配问题，从而在理解和生成任务上均优于解耦方案。此外，我们发现，性能更强的预训练表示编码器能够在所有多模态任务上持续带来更优的性能，这凸显了表示编码器的重要性。最后，在这种统一框架下，联合使用理解和生成数据进行训练，能使两项任务相互促进而非相互干扰。我们在多模态理解与生成基准测试上进行了大量实验，结果表明，TUNA 在图像与视频理解、图像与视频生成以及图像编辑任务上均取得了最先进的性能，充分证明了其统一表示设计的有效性和可扩展性。
引导式大语言模型自进化：最小化人类监督
引导式大语言模型自进化：最小化人类监督
人工智能 (AI) 自进化 (self-evolution) 一直被视作通向超级智能的途径，即模型能够从其自身学习经验中自主地获取、优化并内化知识。然而，实际应用中，无引导的自进化系统往往很快陷入性能平台期，甚至在训练过程中发生退化。这些失败源于概念漂移 (concept drift)、多样性崩溃 (diversity collapse) 和错误进化 (mis-evolution) 等问题，因为模型会不断强化自身偏见，并收敛到低熵 (low-entropy) 行为。为了在最小化对人类监督依赖的同时，实现模型稳定、可控的自进化，我们提出了 R-Few：一个引导式的自我对弈 (Self-Play) 挑战者-求解器 (Challenger-Solver) 框架。该框架通过基于上下文的锚定 (in-context grounding) 和混合训练 (mixed training)，引入了轻量级的人类监督。在每一轮迭代中，挑战者 (Challenger) 会采样一小部分人工标注的示例，用以指导合成问题的生成；而求解器 (Solver) 则遵循一个基于难度的在线课程学习 (online, difficulty-based curriculum) 策略，对人工示例和合成示例进行联合训练。在数学和通用推理基准测试中，R-Few 实现了持续且迭代的性能提升。例如，Qwen3-8B-Base 模型在数学任务上比 R-Zero 提升了 3.0 分，其性能与 General-Reasoner 相当，而后者的训练使用了多达 20 倍以上的人工标注数据。消融研究 (Ablation studies) 证实了基于锚定的挑战者训练 (grounded challenger training) 和基于课程的求解器训练 (curriculum-based solver training) 具有互补作用；进一步分析表明，R-Few 有效缓解了概念漂移，从而产生了更稳定、更可控的协同进化 (co-evolutionary) 动态。

MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory
MG-Nav：基于稀疏空间记忆的双尺度视觉导航
我们提出了 MG-Nav (Memory-Guided Navigation，记忆引导导航)，这是一个用于零样本视觉导航的双尺度框架，它将全局记忆引导规划与局部几何增强控制相统一。其核心是稀疏空间记忆图 (Sparse Spatial Memory Graph, SMG)，这是一种紧凑的、以区域为中心的记忆结构。其中每个节点聚合了多视角关键帧和对象语义信息，不仅能捕获外观与空间结构，还保留了视角多样性。在全局层面，智能体首先在 SMG 上进行定位，然后通过图像到实例的混合检索，规划出一条以目标为条件的节点路径，从而生成一系列可达的路径点，为长程导航提供引导。在局部层面，一个导航基础策略以点目标模式执行这些路径点，并采用障碍物感知控制；当从最终节点向视觉目标导航时，则切换至图像目标模式。为了进一步提升视角对齐和目标识别的能力，我们引入了 VGGT-adapter。这是一个基于预训练 VGGT 模型构建的轻量级几何模块，能够在共享的 3D 感知空间中对齐观测特征与目标特征。MG-Nav 以不同的频率执行全局规划和局部控制，并利用周期性的重新定位来纠正误差。在 HM3D Instance-Image-Goal 和 MP3D Image-Goal 基准测试上的实验表明，MG-Nav 实现了最先进的零样本性能，并且在动态场景重排和未见过的场景条件下仍能保持鲁棒性。
视频生成中的重力问题如何解决？利用可验证奖励对模型进行牛顿定律后训练
视频生成中的重力问题如何解决？利用可验证奖励对模型进行牛顿定律后训练
当前的视频扩散模型虽能生成视觉上逼真的片段，却常常违背基本物理定律，例如物体漂浮、加速度异常、碰撞行为不一致等，这揭示了视觉真实感与物理真实感之间存在的持续差距。为此，我们提出了 NewtonRewards\texttt{NewtonRewards}NewtonRewards，这是首个基于 可验证奖励\textit{可验证奖励}可验证奖励、以物理学原理为根基的视频生成后训练框架。该框架不依赖人类或视觉语言模型 (VLM) 的反馈，而是利用冻结的工具模型从生成视频中提取 可测量的代理量\textit{可测量的代理量}可测量的代理量：以光流作为速度的代理，以高级外观特征作为质量的代理。基于这些代理量，我们通过两种互补的奖励机制来显式地强化牛顿力学结构：一是强制执行恒定加速度动力学的牛顿运动学约束奖励，二是防止出现平凡退化解的质量守恒奖励。我们使用新构建的大规模基准数据集 NewtonBench-60K\texttt{NewtonBench-60K}NewtonBench-60K，在五种牛顿运动基元（自由落体、水平抛射、抛物线抛射、斜坡下滑与上滑）上对 NewtonRewards\texttt{NewtonRewards}NewtonRewards 进行了评估。在所有基元的视觉与物理指标上，NewtonRewards\texttt{NewtonRewards}NewtonRewards 均能持续提升生成视频的物理合理性、运动平滑度与时间连贯性，效果优于已有的后训练方法。此外，在高度、速度、摩擦力等参数发生分布外偏移时，该方法依然能保持强劲的性能。我们的研究结果表明，基于物理学的可验证奖励为实现物理感知的视频生成提供了一条可扩展的路径。

REASONEDIT: 面向推理增强的图像编辑模型
REASONEDIT: 面向推理增强的图像编辑模型
图像编辑模型近期取得了显著进展。一种常见的架构设计是将多模态大语言模型 (Multimodal Large Language Model, MLLM) 编码器与扩散解码器相结合，例如 Step1X-Edit 和 Qwen-Image-Edit 等系统。在这些系统中，MLLM 负责编码参考图像和编辑指令，但其参数在训练过程中保持冻结。本工作表明，释放 MLLM 的推理能力可以进一步拓展图像编辑模型的性能边界。具体而言，我们探索了两种推理机制——思维 (Thinking) 与反思 (Reflection)，以提升模型对指令的理解能力和编辑准确性。基于此，我们提出了一个思维-编辑-反思循环框架：思维机制利用 MLLM 的世界知识来解析抽象指令，而反思机制则评估编辑结果、自动纠正非预期的修改，并确定停止迭代的时机。大量实验验证了我们推理方法的有效性。当基于 Step1X-Edit 初始化我们的 DiT 模型 (即 ReasonEdit-S) 时，在 ImgEdit (+4.3%)、GEdit (+4.7%) 和 Kris (+8.2%) 基准上均取得了显著提升。此外，当与 Qwen-Image-Edit 结合构建 ReasonEdit-Q 时，其在 GEdit 和 Kris 基准上的表现也超越了此前所有的开源方法。

## 每周AI论文速递（251208-251212）

Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance
Wan-Move: 通过潜在轨迹引导实现运动可控的视频生成
我们提出了 Wan-Move，一个简单且可扩展的框架，旨在为视频生成模型引入运动控制能力。现有的运动可控方法通常面临控制粒度粗糙和可扩展性有限的问题，使其输出难以满足实际应用需求。通过实现精确且高质量的运动控制，我们缩小了这一差距。我们的核心思想是直接使原始的条件特征具备运动感知能力，从而指导视频合成。具体而言，我们首先使用密集点轨迹来表示物体运动，以实现对场景的细粒度控制。接着，我们将这些轨迹映射到潜在空间，并沿每条轨迹传播第一帧的特征，从而生成一个对齐的时空特征图，该图定义了每个场景元素应如何运动。此特征图作为更新得到的潜在条件，可以无缝集成到现成的图像到视频模型（例如 Wan-I2V-14B）中，作为运动引导，而无需改变任何模型架构。该方法无需辅助运动编码器，并使得基础模型的微调具备良好的可扩展性。通过大规模训练，Wan-Move 能够生成 5 秒、480p 的视频，用户研究表明其运动可控性可与 Kling 1.5 Pro 的商业版 Motion Brush 相媲美。为了支持全面评估，我们进一步设计了 MoveBench，这是一个精心构建的基准测试，包含多样化的内容类别和经过混合验证的标注。其显著特点在于数据量更大、视频时长更长以及高质量的运动标注。在 MoveBench 和公共数据集上进行的大量实验一致证明了 Wan-Move 卓越的运动生成质量。代码、模型和基准数据均已公开。

Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform
Visionary: 基于 WebGPU 高斯泼溅平台的世界模型载体
神经渲染，特别是 3D 高斯泼溅 (3D Gaussian Splatting, 3DGS) 技术，已迅速发展成为构建世界模型的关键组件。然而，现有的查看器解决方案往往较为分散、笨重或受传统流水线限制，导致部署难度大，且对动态内容与生成模型的支持有限。为此，我们提出了 Visionary，一个开放的、基于 Web 的原生平台，用于实时渲染各类高斯泼溅与网格数据。该平台基于高效的 WebGPU 渲染器构建，并集成了每帧 ONNX 推理能力，从而在保持轻量级、“即点即用”浏览器体验的同时，实现了动态神经处理。Visionary 引入了一个标准化的高斯生成器接口，不仅支持标准 3DGS 渲染，还允许以即插即用的方式，在每帧生成或更新高斯分布。这种推理能力也使得前馈式生成后处理得以应用。此外，平台还提供了一个 three.js 库插件，其简洁的 TypeScript API 便于无缝集成到现有 Web 应用程序中。实验表明，在相同的 3DGS 资源下，得益于基于 GPU 的图元排序，Visionary 相比现有 Web 查看器实现了更优的渲染效率。目前，它已支持多种变体，包括基于 MLP 的 3DGS、4DGS、神经化身以及风格转换或增强网络。通过在浏览器中直接统一推理与渲染，Visionary 显著降低了 3DGS 系列方法的复现、比较与部署门槛，为重建式与生成式两种范式提供了一个统一的世界模型载体。

Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning
原生并行推理器：通过自蒸馏强化学习实现并行推理
我们提出了原生并行推理器 (Native Parallel Reasoner, NPR)，这是一个免教师框架，能够使大语言模型 (LLMs) 自我进化出真正的并行推理能力。NPR 通过三项关键创新，将模型从顺序式处理转变为原生并行认知：1) 一种自蒸馏渐进训练范式，无需外部监督，即可从“冷启动”格式发现过渡到严格的拓扑约束；2) 一种新颖的并行感知策略优化 (Parallel-Aware Policy Optimization, PAPO) 算法，该算法直接在执行图内部优化分支策略，使模型能够通过试错进行自适应分解；以及 3) 一个稳健的 NPR 引擎，它重构了 SGLang 的内存管理与流程控制，从而支持稳定、大规模并行强化学习训练。在八项推理基准测试上，基于 Qwen3-4B 训练的 NPR 实现了高达 24.5% 的性能提升和高达 4.6 倍的推理加速。与先前常需回退至自回归解码的基线方法不同，NPR 实现了 100% 的真正并行执行，从而为自我进化、高效且可扩展的智能体推理树立了新标准。

T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground
T-pro 2.0：一个高效的俄语混合推理模型与测试平台
我们推出 T-pro 2.0，这是一个开放权重的俄语大语言模型 (LLM)，专注于混合推理并实现了高效推断。该模型支持直接回答和推理轨迹生成，通过采用一个针对西里尔字符优化的密集 Tokenizer 以及一个经过适配的 EAGLE 推测解码流水线，有效降低了延迟。为了促进可复现和可扩展的研究，我们在 Hugging Face 上发布了模型权重、T-Wix 500k 指令数据集、T-Math 推理基准以及 EAGLE 权重。这些资源使用户能够研究俄语推理任务，并可对模型及推断流水线进行扩展或适配。一个公开的 Web 演示提供了推理模式与非推理模式，并展示了我们的推断技术栈在不同任务上所带来的加速效果。因此，T-pro 2.0 作为一个开放易用的系统，可用于构建和评估高效、实用的俄语大语言模型应用。

TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows
TwinFlow: 基于自对抗流的大模型一步生成方法
近期，大型多模态生成模型在图像和视频等多模态生成任务上展现出卓越能力。这些模型通常基于扩散模型和流匹配等多步框架构建，其固有的多步推理过程（通常需要 40-100 次函数评估 (NFEs)）限制了推理效率。尽管已有多种少步推理方法旨在加速，但现有方案仍存在明显不足。主流的基于蒸馏的方法，如渐进蒸馏和一致性蒸馏，要么需要迭代的蒸馏流程，要么在极低步数（< 4-NFE）下性能显著下降。同时，将对抗训练融入蒸馏过程（例如 DMD/DMD2 和 SANA-Sprint）以提升性能，会因引入额外的辅助训练模型而导致训练不稳定、复杂度增加以及高昂的 GPU 内存开销。为此，我们提出了 TwinFlow，一个用于训练一步生成模型的简洁高效框架。该框架无需依赖固定的预训练教师模型，且在训练过程中避免了使用标准的对抗网络，因而非常适合构建大规模高效模型。在文生图任务上，我们的方法仅需 1-NFE 即可获得 0.83 的 GenEval 分数，性能优于 SANA-Sprint（基于 GAN 损失的框架）和 RCGM（基于一致性的框架）等强基线。尤为重要的是，我们通过在 Qwen-Image-20B 上进行全参数训练，验证了 TwinFlow 的可扩展性，并将其成功转换为一个高效的少步生成器。仅使用 1-NFE，我们的方法在 GenEval 和 DPG-Bench 基准测试上的性能即可与原版 100-NFE 模型相媲美，在质量仅有轻微下降的同时，将计算成本降低了 100×100\times100×。项目页面见 zhenglin-cheng.com/twinflow。

StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation
StereoWorld: 几何感知的单目到立体视频生成
随着 XR 设备的日益普及，市场对高质量立体视频的需求强劲增长，但其制作过程依然成本高昂且易产生伪影。为应对这一挑战，我们提出了 StereoWorld，这是一个端到端的框架，它通过改造一个预训练的视频生成器，实现了高保真的单目到立体视频生成。我们的框架使模型能够以输入的单目视频为条件，同时利用几何感知的正则化对生成过程进行显式监督，从而确保三维结构的保真度。此外，我们还集成了一个时空分块方案，以实现高效的高分辨率视频合成。为了支持大规模训练与评估，我们构建了一个高清立体视频数据集，其中包含超过 1100 万帧视频，其内容均与自然的人类瞳孔间距离 (IPD) 对齐。大量实验表明，StereoWorld 的性能显著优于现有方法，能够生成具有卓越视觉保真度和几何一致性的立体视频。项目网页地址为 ke-xing.github.io/StereoWorld…

Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs
超越实数：旋转位置编码的虚数扩展及其在长上下文大语言模型中的应用
旋转位置编码 (RoPE) 通过在复平面上对查询和键向量施加旋转，已成为大语言模型 (LLMs) 中编码序列顺序的标准方案。然而，在标准的实现中，注意力分数的计算仅使用了复数值点积的实部。这种简化丢弃了包含宝贵相位信息的虚部，可能导致对建模长上下文依赖至关重要的关系细节丢失。本文提出一种扩展方法，重新纳入了这一被丢弃的虚部。我们的方法利用完整的复数值表示，构建了一个双分量的注意力分数。我们从理论和实验上证明，该方法通过保留更多的位置信息，提升了对长上下文依赖的建模能力。此外，在一系列长上下文语言建模基准测试上的评估表明，相较于标准 RoPE，我们的方法能持续提升模型性能，且随着上下文长度的增加，其优势愈发显著。代码开源地址：github.com/OpenMOSS/ro…

Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality
保持源视频真实感：实现电影级质量的高保真人脸交换
视频人脸交换在电影和娱乐制作中至关重要。然而，对于长且复杂的视频序列，实现高保真度和时间一致性仍是一个重大挑战。受近期参考引导图像编辑技术进展的启发，我们探索能否类似地利用源视频中丰富的视觉属性，来同时提升视频人脸交换的保真度与时间连贯性。基于此，本文提出了首个视频参考引导的人脸交换模型——LivingSwap。我们的方法以关键帧作为条件信号，引入目标身份特征，从而实现灵活可控的编辑。通过结合关键帧条件与视频参考引导，模型能够进行时序拼接，确保在长视频序列中稳定保持身份并实现高保真重建。为了解决参考引导训练数据稀缺的问题，我们构建了一个配对的人脸交换数据集 Face2Face，并通过反转数据对来确保可靠的真值监督。大量实验表明，我们的方法取得了最先进的结果，能够将目标身份与源视频的表情、光照和运动无缝融合，同时显著减少了生产流程中的人工工作量。项目网页：aim-uofa.github.io/LivingSwap

## 每周AI论文速递（251215-251219）

Kling-Omni Technical Report
Kling-Omni 技术报告
我们提出了 Kling-Omni，一个通用的生成式框架，旨在直接从多模态视觉语言输入合成高保真视频。基于端到端的设计理念，Kling-Omni 打破了多样化视频生成、编辑和智能推理任务之间的功能壁垒，将其整合为一个统一的整体系统。与割裂的流水线方法不同，Kling-Omni 支持多种用户输入，包括文本指令、参考图像和视频上下文，并将其转化为统一的多模态表示，从而实现电影级画质和高度智能的视频内容创作。为了支撑这些能力，我们构建了一个全面的数据系统，作为多模态视频创作的基础。该框架还通过高效的大规模预训练策略和优化的推理基础设施得到进一步强化。全面的评估表明，Kling-Omni 在上下文生成、基于推理的编辑以及多模态指令遵循方面展现出卓越的性能。Kling-Omni 不仅仅是一个内容创作工具，我们相信它是迈向能够感知、推理、生成并与动态复杂世界交互的多模态世界模拟器的关键进展。

Step-GUI Technical Report
Step-GUI 技术报告
多模态大语言模型 (Multimodal Large Language Model) 的最新进展为图形用户界面 (GUI) 自动化带来了前所未有的机遇。然而，一个核心挑战依然存在：如何在保证标注可靠性的前提下，高效获取高质量的训练数据？我们提出了一种由校准步骤奖励系统 (Calibrated Step Reward System) 驱动的自演进训练流水线。该流水线通过轨迹级校准，将模型生成的操作序列转化为可靠的训练信号，从而以降低 10 至 100 倍的成本实现了超过 90% 的标注准确率。基于此流水线，我们推出了 Step-GUI 模型系列 (4B/8B 参数)。该系列模型在保持强大通用能力的同时，实现了业界领先的 GUI 性能 (8B 模型: AndroidWorld 80.2%, OSWorld 48.5%, ScreenShot-Pro 62.6%)。随着 GUI 智能体能力的增强，实际部署要求能在异构设备间提供标准化接口，并保护用户隐私。为此，我们提出了 GUI-MCP，这是首个专为 GUI 自动化设计的模型上下文协议 (Model Context Protocol)。它采用分层架构，结合了底层的原子操作与将高层任务委派给本地专家模型的能力，从而实现了高隐私保护执行——敏感数据全程保留在设备本地。最后，为了评估智能体处理真实日常使用场景的能力，我们引入了 AndroidDaily 基准测试。该测试基于真实的移动设备使用模式构建，涵盖了高频日常场景，包含 3146 个静态动作和 235 个端到端任务 (8B 模型: 静态动作准确率 89.91%, 端到端任务成功率 52.50%)。我们的工作推动了实用型 GUI 智能体的发展，并展现了其在日常数字交互中进行实际部署的巨大潜力。

MMGR: Multi-Modal Generative Reasoning
MMGR: 多模态生成式推理
视频基础模型 (Video foundation models) 能够生成视觉逼真且时序连贯的内容，但其作为世界模拟器 (world simulators) 的可靠性，取决于它们是否捕捉到了物理、逻辑和空间约束。现有指标，如弗雷歇视频距离 (Frechet Video Distance, FVD)，侧重于感知质量，却忽略了推理错误 (reasoning failures)，包括对因果关系、物理定律和全局一致性的违背。我们提出了 MMGR (多模态生成式推理评估与基准，Multi-Modal Generative Reasoning Evaluation and Benchmark)，这是一个基于五种推理能力的系统性评估框架：物理推理、逻辑推理、3D 空间推理、2D 空间推理和时序推理。MMGR 在三个领域评估生成式推理 (Generative Reasoning)：抽象推理 (Abstract Reasoning，包括 ARC-AGI 和数独)、具身导航 (Embodied Navigation，包括真实世界的 3D 导航与定位) 以及物理常识 (Physical Commonsense，包括运动和组合交互)。MMGR 采用细粒度指标，要求视频和图像生成具备整体正确性。我们对领先的视频模型 (Veo-3, Sora-2, Wan-2.2) 和图像模型 (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image) 进行了基准测试，结果揭示了跨领域的显著性能差距。模型在物理常识任务上取得了中等成功，但在抽象推理上表现不佳 (在 ARC-AGI 上的准确率低于 10%)，并且在具身环境中的长时程空间规划方面存在困难。我们的分析指出了当前模型的关键局限，包括过度依赖感知数据、全局状态一致性较弱，以及优化目标更倾向于奖励视觉合理性而非因果正确性。MMGR 提供了一个统一的诊断基准，并为开发具备推理意识的生成式世界模型指明了一条路径。

EgoX: Egocentric Video Generation from a Single Exocentric Video
EgoX: 从单段第三人称视频生成第一人称视频
第一人称视角感知使人类能够直接从自身视角体验和理解世界。将第三人称视角视频转换为第一人称视角视频，为沉浸式理解开辟了新途径。然而，由于相机姿态变化剧烈且视图重叠区域极小，该任务仍极具挑战性。它要求模型在忠实保留可见内容的同时，以几何一致的方式合成不可见区域。为此，我们提出了 EgoX，一个从单段第三人称输入视频生成第一人称视频的新框架。EgoX 通过轻量化的低秩适配 (LoRA) 技术，利用大规模视频扩散模型的预训练时空知识，并引入了一种统一的调节策略。该策略通过沿宽度和通道维度拼接的方式，融合了第三人称与第一人称视角先验。此外，我们提出了一种几何引导的自注意力机制，能够选择性地关注空间相关区域，从而确保了几何一致性并实现了高视觉保真度。我们的方法能够生成连贯且逼真的第一人称视角视频，并在未见过的及真实场景视频上表现出了良好的可扩展性与鲁棒性。

Memory in the Age of AI Agents
AI 智能体时代的记忆
记忆已成为基于基础模型的智能体的一项核心能力，并且这一地位将持续保持。随着智能体记忆研究的迅速扩展并吸引前所未有的关注，该领域也日益呈现出碎片化趋势。现有关于智能体记忆的研究，其动机、实现方式和评估协议往往存在显著差异，而定义松散的记忆术语的激增进一步加剧了概念上的模糊性。诸如长/短期记忆之类的传统分类法已被证明不足以涵盖当代智能体记忆系统的多样性。本文旨在勾勒当前智能体记忆研究的最新全景。我们首先明确界定智能体记忆的范畴，并将其与大语言模型记忆、检索增强生成 (RAG) 以及上下文工程等相关概念区分开来。接着，我们通过形式、功能和动态这三个统一的视角来审视智能体记忆。从形式视角，我们识别出智能体记忆的三种主要实现方式：Token 级记忆、参数化记忆和潜在记忆。从功能视角，我们提出了一个更细粒度的分类法，区分事实性记忆、经验性记忆和工作记忆。从动态视角，我们分析了记忆如何随时间形成、演化与检索。为支持实际开发，我们汇编了关于记忆基准测试和开源框架的全面总结。在整合现有成果之外，我们还阐述了对新兴研究前沿的前瞻性看法，包括记忆自动化、强化学习集成、多模态记忆、多智能体记忆以及可信度问题。我们希望本综述不仅能作为现有研究的参考，更能为将记忆重新思考为未来智能体智能设计中的一等原语提供概念基础。

QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management
QwenLong-L1.5: 面向长上下文推理与记忆管理的后训练方案
我们推出 QwenLong-L1.5，这是一个通过系统性后训练创新实现卓越长上下文推理能力的模型。QwenLong-L1.5 的关键技术突破如下：(1) 长上下文数据合成流水线：我们开发了一套系统性的合成框架，用于生成需要基于全局分散证据进行多跳关联的挑战性推理任务。通过将文档解构为原子事实及其底层关系，并以编程方式构建可验证的推理问题，我们的方法能够大规模生成高质量训练数据，从而实现了从简单检索任务到真正长程推理能力的实质性跨越。(2) 面向长上下文训练的稳定强化学习：为克服长上下文强化学习 (RL) 中的核心不稳定性，我们引入了结合任务特定优势估计的任务平衡采样以缓解奖励偏差，并提出了自适应熵控制策略优化 (AEPO)，动态调节探索与利用之间的权衡。(3) 面向超长上下文的记忆增强架构：我们认识到，即使扩展的上下文窗口也无法容纳任意长的序列。为此，我们开发了一个包含多阶段融合强化学习训练的记忆管理框架，该框架将单次推理与基于记忆的迭代处理无缝集成，以处理超过 4M Token 的任务。基于 Qwen3-30B-A3B-Thinking 模型，QwenLong-L1.5 在长上下文推理基准测试中取得了与 GPT-5 和 Gemini-2.5-Pro 相当的性能，平均超越其基线 9.90 分。在超长任务 (1M~4M Token) 上，QwenLong-L1.5 的记忆智能体框架相比智能体基线实现了 9.48 分的性能提升。此外，所获得的长上下文推理能力也带来了在科学推理、记忆工具使用以及扩展对话等通用领域性能的增强。

Towards Scalable Pre-training of Visual Tokenizers for Generation
面向生成任务的可扩展视觉分词器预训练
视觉分词器 (例如，VAEs) 的潜在空间质量对现代生成模型至关重要。然而，标准的基于重建的训练范式所产生的潜在空间偏向于编码低级信息，这导致了一个基础性缺陷：更高的像素级精度并不能带来更高质量的生成结果。这意味着，将大量计算资源投入到视觉分词器预训练中，难以有效转化为生成性能的提升。我们将此界定为“预训练缩放问题”，并指出必须做出转变：一个潜在空间要想对生成任务有效，就必须能简洁地表征高级语义。为此，我们提出了 VTP，一个统一的视觉分词器预训练框架，率先对图像-文本对比损失、自监督损失和重建损失进行联合优化。我们的大规模研究得出了两个主要结论：(1) 理解是生成的关键驱动力；(2) VTP 展现出优越得多的缩放特性，其生成性能能随预训练投入的计算量、参数量和数据集大小而有效提升。经过大规模预训练后，我们的分词器取得了具有竞争力的性能指标 (在 ImageNet 上达到 78.2 的零样本精度和 0.36 的 rFID) ，并且在生成任务上的收敛速度比先进的蒸馏方法快 4.1 倍。更重要的是，它具备良好的可扩展性：在不改变标准 DiT 训练配置的情况下，仅通过在预训练 VTP 时投入更多 FLOPS，就能为下游生成任务带来 65.8% 的 FID 提升；相比之下，传统自编码器仅使用其 1/10 的计算量时，性能便已早早陷入停滞。我们的预训练模型发布于 github.com/MiniMax-AI/…

ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding
ReFusion: 一种具有并行自回归解码能力的扩散大语言模型
自回归模型 (Autoregressive Models, ARMs) 受限于其缓慢的顺序推理速度。掩码扩散模型 (Masked Diffusion Models, MDMs) 虽提供了并行的替代方案，但也面临两个关键缺陷：一是因无法使用键值 (Key-Value, KV) 缓存而导致的高计算开销，二是在学习难以处理的 Token 组合空间上的依赖关系时，会导致生成内容不连贯。为克服这些限制，我们提出了 ReFusion，这是一种新颖的掩码扩散模型。它通过将并行解码从 Token 级别提升至更高的槽位 (slot) 级别（每个槽位是一个固定长度的连续子序列），从而实现了卓越的性能与效率。其核心是一个迭代的“规划与填充”解码过程：首先，一个基于扩散的规划步骤识别出一组弱依赖的槽位；随后，一个自回归填充步骤并行解码这些选定的槽位。这种基于槽位的设计具有双重优势：它通过一个统一的因果框架实现了完整的 KV 缓存重用，同时将学习复杂度从巨大的 Token 组合空间降低到了可管理的槽位级排列空间。在七个多样化基准上的广泛实验表明，ReFusion 不仅以 34% 的性能提升和平均超过 18 倍的加速比显著超越了先前的 MDMs，而且在保持平均 2.33 倍加速比优势的同时，弥合了与强大 ARMs 之间的性能差距。

Adaptation of Agentic AI
智能体 AI 的适配
最先进的智能体 AI (Agentic AI) 系统构建于基础模型之上，这些模型能够通过适配来进行规划、推理，并与外部工具交互，以执行日益复杂和专门化的任务。随着此类系统能力和应用范围的增长，适配已成为提升其性能、可靠性和泛化能力的核心机制。本文旨在将这一快速发展的研究领域统一为一个系统性框架，该框架同时涵盖智能体适配与工具适配。我们进一步将智能体适配分解为工具执行信号触发型和智能体输出信号触发型，将工具适配分解为智能体无关型和智能体监督型。我们论证了该框架有助于厘清智能体 AI 中各种适配策略的设计空间，明确揭示其权衡取舍，并为系统设计过程中策略的选择或切换提供实践指导。随后，我们回顾了各类别中的代表性方法，分析了其优势与局限，并着重指出了关键的开放性挑战与未来机遇。总而言之，本文旨在为致力于构建更强大、高效、可靠的智能体 AI 系统的研究人员与从业者，提供一个坚实的理论基石和清晰的实践路线图。

LongVie 2: Multimodal Controllable Ultra-Long Video World Model
LongVie 2: 多模态可控超长视频世界模型
在预训练视频生成系统基础上构建视频世界模型，是迈向通用时空智能的关键且富有挑战性的一步。一个理想的世界模型应具备三个核心特性：可控性、长期视觉质量与时间一致性。为此，我们采用了一种渐进式策略：首先提升可控性，进而扩展到长期、高质量的视频生成。本文提出了 LongVie 2，这是一个端到端的自回归框架，其训练包含三个阶段：(1) 多模态引导，通过融合密集与稀疏的控制信号，提供隐式的世界层面监督以增强可控性；(2) 输入帧的退化感知训练，旨在缩小训练与长期推理之间的差异，从而维持高视觉质量；(3) 历史上下文引导，通过对齐相邻视频片段间的上下文信息来确保时间一致性。此外，我们推出了 LongVGenBench，一个包含 100 段高分辨率、时长一分钟视频的综合评测基准，涵盖了多样化的真实世界与合成场景。大量实验证明，LongVie 2 在长程可控性、时间连贯性与视觉保真度方面均达到了最先进水平，并能支持持续生成长达五分钟的视频，这标志着我们在构建统一视频世界模型的征程上迈出了坚实的一步。

WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling
WorldPlay: 面向实时交互式世界建模的长期几何一致性
本文提出了 WorldPlay，这是一个流式视频扩散模型，能够实现具有长期几何一致性的实时交互式世界建模，从而解决了当前方法所面临的速度与内存之间的权衡问题。WorldPlay 基于三项关键创新。1) 我们采用了一种双重动作表示，以实现对用户键盘和鼠标输入的鲁棒动作控制。2) 为了确保长期一致性，我们提出的重构上下文内存能够动态地从历史帧重建上下文，并利用时间重构技术使几何上重要但历史久远的帧保持可访问性，从而有效缓解了内存衰减问题。3) 我们还提出了上下文强制，这是一种专为内存感知模型设计的新型蒸馏方法。通过对齐教师模型与学生模型之间的内存上下文，该方法保留了学生模型利用长程信息的能力，在实现实时生成速度的同时，防止了误差漂移。综上所述，WorldPlay 能够以 24 FPS 的帧率实时生成长序列的 720p 流式视频，具有卓越的一致性，其性能优于现有技术，并在多样化的场景中表现出强大的泛化能力。项目页面与在线演示请访问：3d-models.hunyuan.tencent.com/world/ 和 3d.hunyuan.tencent.com/sceneTo3D。

Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?
视频真实性测试：AI 生成的 ASMR 视频能欺骗视觉语言模型和人类吗？
视频生成技术的最新进展已能产出极为生动、常与真实视频难以区分的内容，这使得 AI 生成视频检测成为一个新兴的社会挑战。现有的 AIGC (AI-Generated Content) 检测基准大多针对无音频视频进行评估，覆盖宽泛的叙事领域，且仅侧重于分类任务。然而，当前最先进的视频生成模型能否制作出沉浸式、音画同步的视频，从而可靠地欺骗人类和视觉语言模型 (Vision-Language Models, VLMs)，仍是一个悬而未决的问题。为此，我们提出了视频真实性测试 (Video Reality Test)，这是一个基于 ASMR (Autonomous Sensory Meridian Response) 源构建的视频基准套件，用于在强视听关联条件下测试感知真实性，其特点包括以下两个维度：(i) 沉浸式 ASMR 音视频源。该基准基于精心筛选的真实 ASMR 视频构建，专注于细粒度的动作-物体交互，并在物体、动作和背景方面具有多样性。(ii) 同行评审式评估。采用一种对抗性的创作者-评审者协议：视频生成模型扮演创作者，旨在欺骗评审者；而 VLMs 则扮演评审者，致力于识别虚假内容。我们的实验结果表明：表现最佳的创作者模型 Veo3.1-Fast 甚至能成功欺骗大多数 VLM 评审者：最强的评审者模型 (Gemini 2.5-Pro) 仅达到 56% 的准确率 (随机基线为 50%)，远低于人类专家 81.25% 的准确率。添加音频有助于提升真假辨别能力，但诸如水印等表面线索仍会显著误导模型。这些发现界定了当前视频生成真实性的边界，并揭示了 VLMs 在感知保真度与视听一致性方面的局限性。我们的代码可在 github.com/video-reali… 获取。

Next-Embedding Prediction Makes Strong Vision Learners
下一嵌入预测构建强大的视觉学习模型
受自然语言中生成式预训练成功的启发，我们探究同样的原理能否构建出强大的自监督视觉学习模型。我们的方法不是训练模型输出用于下游任务的特征，而是训练其生成嵌入 (embedding) 来直接执行预测任务。本工作探索了从学习表征 (representation) 到学习预测模型这一转变。具体而言，模型学习在给定历史补丁嵌入的条件下预测未来的补丁嵌入，该方法结合了因果掩码 (causal masking) 和停止梯度 (stop gradient)，我们称之为下一嵌入预测自回归 (Next-Embedding Predictive Autoregression, NEPA)。我们证明，在 ImageNet-1k 上仅以下一嵌入预测为学习目标预训练一个简单的 Transformer 模型即可取得良好效果——无需像素重建、离散 Token、对比损失或任何任务特定的头部 (head)。该方案保持了架构的简洁性和可扩展性，无需引入额外的设计复杂度。NEPA 在多项任务上表现强劲：使用 ViT-B 和 ViT-L 骨干网络进行微调后，在 ImageNet-1K 上分别达到了 83.8% 和 85.3% 的 top-1 准确率，并能有效迁移至 ADE20K 数据集的语义分割任务。我们相信，基于嵌入的生成式预训练为视觉自监督学习提供了一种简单、可扩展且可能模态无关 (modality-agnostic) 的替代方案。

LLaDA2.0: Scaling Up Diffusion Language Models to 100B
LLaDA2.0: 将扩散语言模型规模扩展至 100B 参数
本文提出了 LLaDA2.0——一系列通过系统化转换自回归 (Auto-Regressive, AR) 模型而构建的离散扩散大语言模型 (Discrete Diffusion Large Language Model, dLLM)，其总参数量扩展至 100B，从而为前沿级别的模型部署确立了新范式。LLaDA2.0 并非成本高昂的从头训练，而是遵循知识继承、渐进适应与注重效率的设计原则，并采用一种新颖的、基于三阶段块级 WSD 的训练方案，将预训练的 AR 模型无缝转换为 dLLM。该方案包括：块扩散中逐步增大块尺寸（预热阶段）、大规模全序列扩散（稳定阶段）以及回退至紧凑尺寸的块扩散（衰减阶段）。结合监督微调 (SFT) 和直接偏好优化 (DPO) 进行训练后对齐，我们得到了 LLaDA2.0-mini (16B) 和 LLaDA2.0-flash (100B)，这是两个为实际部署优化的、经过指令调优的混合专家 (Mixture-of-Experts, MoE) 变体。通过保持并行解码的优势，这些模型在达到前沿规模时，能提供卓越的性能与效率。两个模型均已开源。

Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows
Finch: 面向以电子表格为核心的企业工作流的财务与会计基准测试
我们提出了一个财务与会计基准测试 (Finch)，用于评估 AI 智能体在真实企业级专业工作流上的性能。这些工作流融合了数据录入、结构化、格式化、网络搜索、跨文件检索、计算、建模、验证、翻译、可视化及报告等多种任务。Finch 的数据源包括安然公司 (包含来自 150 名员工的 15,000 份电子表格和 50 万封电子邮件) 及其他金融机构的真实工作环境，完整保留了实际工作中跨多模态工件 (文本、表格、公式、图表、代码和图像) 的杂乱特性，并覆盖了预算、交易和资产管理等多个领域。
我们提出了一种结合大语言模型 (LLM) 辅助发现与专家标注的工作流构建方法：(1) 从真实的电子邮件线程和电子表格文件版本历史中，通过 LLM 辅助并经由专家验证来推导工作流；(2) 对工作流进行细致的专家标注，此过程耗费了超过 700 小时的领域专家工时。最终，我们构建了 172 个复合工作流，包含 384 项任务，涉及 1,710 个电子表格 (总计 2,700 万个单元格) 以及 PDF 等其他文件，从而捕捉了真实企业工作所固有的混乱性、长期性、知识密集性与协作性。
我们对包括 GPT 5.1、Claude Sonnet 4.5、Gemini 3 Pro、Grok 4 和 Qwen 3 Max 在内的前沿 AI 系统进行了人工与自动化评估。结果显示，GPT 5.1 Pro 总计耗时 48 小时，仅能通过 38.4% 的工作流，而 Claude Sonnet 4.5 的通过率仅为 25.0%。进一步的全面案例分析揭示了真实企业工作流给 AI 智能体带来的具体挑战。

## 每周AI论文速递（251222-251226）

DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI
DataFlow: 面向以数据为中心 AI 时代的统一数据准备与工作流自动化 LLM 驱动框架
大语言模型 (LLMs) 对高质量数据的需求快速增长，这使得对可扩展、可靠且语义丰富的数据准备管道的需求变得尤为迫切。然而，当前实践仍主要依赖临时脚本和定义松散的工作流，它们缺乏原则性的抽象，阻碍了可复现性，并对模型在环 (model-in-the-loop) 的数据生成支持有限。为应对这些挑战，我们提出了 DataFlow，一个统一且可扩展的 LLM 驱动数据准备框架。DataFlow 采用系统级抽象设计，实现了模块化、可复用和可组合的数据转换，并提供了类似 PyTorch 风格的管道构建 API，用以构建可调试和可优化的数据流。该框架包含近 200 个可复用操作符和六个领域通用管道，覆盖文本、数学推理、代码、Text-to-SQL、智能体驱动的检索增强生成 (Agent RAG) 以及大规模知识提取。为进一步提升易用性，我们引入了 DataFlow-Agent，它能够通过操作符合成、管道规划和迭代验证，自动将自然语言描述转换为可执行的管道。在六个代表性用例中，DataFlow 均能一致地提升下游 LLM 性能。我们的数学、代码和文本管道性能超越了精心构建的人工数据集和专门的合成基线：在 Text-to-SQL 任务上，其执行准确率较 SynSQL 最高提升 3%；在代码基准测试上平均提升 7%；在 MATH、GSM8K 和 AIME 基准上取得了 1 到 3 个百分点的性能增益。此外，由 DataFlow 生成的统一万样本 (10K) 数据集，使得基础模型的性能超越了在百万级 (1M) Infinity-Instruct 数据上训练的同类模型。这些结果表明，DataFlow 为可靠、可复现和可扩展的 LLM 数据准备提供了一个实用且高性能的底层支持，并为未来以数据为中心的 AI 发展奠定了系统级基础。

Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows
通过科学家工作流对齐评估大语言模型的科学通用智能
尽管科学 AI 领域取得了进展，但关于科学通用智能 (Scientific General Intelligence, SGI) —— 即自主构思、探索并跨科学领域进行推理的能力 —— 仍缺乏一个连贯的框架。我们提出了一个基于实践探究模型 (Practical Inquiry Model, PIM: 审议、构思、行动、感知) 的可操作 SGI 定义，并通过四个与科学家工作流对齐的任务来具体实现这一定义：深度研究、想法生成、干/湿实验 (dry/wet experiments) 以及实验推理。SGI-Bench 基准包含 1000 多个由专家精心策划的跨学科样本，其灵感来源于《科学》杂志提出的 125 个重大科学问题，可用于系统评估最先进的大语言模型。评估结果揭示了多方面的差距：尽管在步骤层面与人类工作流对齐，但深度研究任务的精确匹配率仍然很低 (10--20%)；生成的想法缺乏可行性和细节；干实验任务中代码可执行性高，但执行结果的准确性低；湿实验方案的步骤序列保真度低；并且在多模态比较推理方面持续面临挑战。我们进一步引入了测试时强化学习 (Test-Time Reinforcement Learning, TTRL)，该方法在模型推理阶段优化基于检索增强的新颖性奖励，从而能在不依赖参考答案的情况下提升生成假设的新颖性。综上所述，我们基于 PIM 的定义、以工作流为中心的基准测试以及实证分析，为开发能够真正推动科学发现进程的 AI 系统奠定了基础。

SemanticGen: Video Generation in Semantic Space
SemanticGen：语义空间视频生成
当前最先进的视频生成模型通常学习视频在 VAE (Variational Autoencoder) 潜在空间中的分布，并通过 VAE 解码器将其映射到像素空间。这种方法虽然能够生成高质量视频，但存在收敛速度慢的问题，并且在生成长视频时计算开销巨大。本文提出 SemanticGen，一种新颖的解决方案，通过在语义空间中进行视频生成来应对这些挑战。我们的核心观点是：由于视频本身存在固有冗余，生成过程应当始于一个紧凑的高层语义空间以进行全局规划与结构设计，随后再补充高频细节，而非直接使用双向注意力对海量的低层视频 Token 进行建模。SemanticGen 采用两阶段生成流程：第一阶段，一个扩散模型生成紧凑的语义视频特征，这些特征定义了视频的全局布局；第二阶段，另一个扩散模型以这些语义特征为条件，生成 VAE 潜在表示，进而产生最终视频输出。我们观察到，相较于在 VAE 潜在空间中生成，在语义空间中进行生成能实现更快的收敛速度。此外，我们的方法在扩展到长视频生成任务时，依然保持高效且计算成本可控。大量实验表明，SemanticGen 能够生成高质量视频，其性能优于当前最优方法和多个强基线模型。
Step-DeepResearch Technical Report
Step-DeepResearch 技术报告
随着大语言模型 (LLM) 向自主 AI 智能体 (AI Agent) 演进，深度研究 (Deep Research) 已成为一项关键的评估维度。然而，现有的学术基准（如 BrowseComp）往往难以满足现实世界对开放式探索研究的需求，这类研究需要强大的意图识别、长期决策和跨来源验证能力。为此，我们推出了 Step-DeepResearch，一个高性价比的端到端智能体。我们提出了一种基于原子化能力 (Atomic Capabilities) 的数据合成策略，用以强化任务规划和报告撰写能力，并结合了从智能体行为训练、监督微调 (SFT) 到强化学习 (RL) 的渐进式训练路径。该方法通过一个检查表式评估器 (Checklist-style Judger) 得到增强，显著提升了系统的鲁棒性。此外，为了填补中文领域在深度研究评估方面的空白，我们针对真实的深度研究场景建立了 ADR-Bench 基准。实验结果表明，Step-DeepResearch (32B) 在 Scale AI Research Rubrics 评估中取得了 61.4% 的得分。在 ADR-Bench 上，其性能显著优于同类模型，并能与 OpenAI、Gemini DeepResearch 等最先进 (SOTA) 的闭源模型相媲美。这些发现证明，通过精细化的训练，中等规模的模型能够以业界领先的性价比实现专家级的能力。

TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times
TurboDiffusion: 将视频扩散模型加速 100-200 倍
我们提出了 TurboDiffusion，这是一个视频生成加速框架，能够在保持视频质量的同时，将端到端的扩散模型生成过程加速 100 至 200 倍。TurboDiffusion 主要通过以下几个组件实现加速：(1) 注意力机制加速：TurboDiffusion 采用低比特 SageAttention 和可训练的稀疏线性注意力 (Sparse-Linear Attention, SLA) 来加速注意力计算。(2) 步数蒸馏：TurboDiffusion 使用 rCM 方法进行高效的步数蒸馏。(3) W8A8 量化：TurboDiffusion 将模型参数和激活值量化为 8 位 (W8A8)，以加速线性层运算并减少模型体积。此外，该框架还集成了其他多项工程优化技术。
我们在 Wan2.2-I2V-14B-720P、Wan2.1-T2V-1.3B-480P、Wan2.1-T2V-14B-720P 以及 Wan2.1-T2V-14B-480P 模型上进行了实验。结果表明，即使在单块 RTX 5090 GPU 上，TurboDiffusion 也能实现 100-200 倍的视频生成加速，同时生成视频的质量与原始方法相当。包含模型检查点及易于使用代码的 GitHub 仓库地址为：github.com/thu-ml/Turb…

PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence
PhysBrain: 人类自我中心数据作为从视觉语言模型到物理智能的桥梁
机器人的泛化能力依赖于物理智能：即在自我中心感知与行动条件下，对状态变化、密集接触交互以及长时程规划进行推理的能力。然而，大多数视觉语言模型主要基于第三人称数据进行训练，这为人形机器人带来了根本性的视角失配。由于成本高昂且多样性有限，扩展机器人自我中心数据的采集仍不现实；而大规模的人类自我中心视频则提供了一个可扩展的替代方案，它们天然地捕捉了丰富的交互语境与因果结构。关键挑战在于如何将原始的自我中心视频转化为结构化且可靠的具身训练监督信号。为此，我们提出了一个 Egocentric2Embodiment 转换流程，该流程将第一人称视频转化为具有强制证据基础和时序一致性的、多层次且模式驱动的视觉问答监督信号，从而能够大规模构建 Egocentric2Embodiment 数据集 (E2E-3M)。通过在 E2E-3M 数据集上进行训练，我们得到了一个具备自我中心感知能力的具身大脑，称为 PhysBrain。PhysBrain 在自我中心理解方面表现出显著提升，尤其是在 EgoThink 任务上的规划能力。它提供了一个具备自我中心感知能力的初始化状态，使得视觉语言-动作模型的微调过程样本效率更高，并在 SimplerEnv 上取得了更高的成功率 (53.9%)，这证明了从人类自我中心监督信号到下游机器人控制的有效迁移。

Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding
Robust-R1：基于退化感知推理的鲁棒视觉理解
在极端现实世界的视觉退化条件下，多模态大语言模型 (MLLM) 难以维持可靠的性能，这限制了其实际应用的鲁棒性。现有的鲁棒多模态大语言模型主要依赖于隐式训练或适应方法，这些方法仅侧重于提升视觉编码器的泛化能力，导致模型可解释性有限且优化过程相对孤立。为克服这些局限，我们提出了 Robust-R1，这是一个通过结构化推理链来显式建模视觉退化的新型框架。我们的方法整合了三个核心部分：(i) 为奠定退化感知推理基础而进行的监督微调，(ii) 为实现精准退化参数感知的奖励驱动对齐，以及 (iii) 能够适应退化强度的动态推理深度缩放。为支持此方法，我们构建了一个包含 1.1 万个样本的专用数据集，其中涵盖了在四个关键现实视觉处理阶段合成的真实退化。每个样本均标注有结构化推理链，链中连接了退化参数、感知影响、原始语义推理链及最终结论。全面的评估结果表明，Robust-R1 具备最先进的鲁棒性：它在现实世界退化基准 R-Bench 上的表现优于所有通用及鲁棒基线模型，并且在 MMMB、MMStar 和 RealWorldQA 基准上面临多强度对抗性退化时，依然保持了卓越的抗退化性能。

The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding
棱镜假说：通过统一自编码调和语义与像素表示
跨模态的深度表示本质上是相互交织的。本文中，我们系统地分析了多种语义与像素编码器的频谱特性。有趣的是，我们的研究揭示了一个极具启发性且鲜有探索的对应关系：编码器的特征频谱与其功能角色密切相关——语义编码器主要捕获编码抽象含义的低频分量，而像素编码器则额外保留了传达细粒度细节的高频信息。这一发现提供了一个统一的视角，将编码器行为与其底层频谱结构关联起来。我们将其定义为棱镜假说，即每种数据模态都可被视为自然世界在共享特征频谱上的投影，其作用类似于棱镜。基于此见解，我们提出了统一自编码 (UAE)，该模型通过一种创新的频带调制器来调和语义结构与像素细节，使二者能够无缝共存。在 ImageNet 和 MS-COCO 基准上进行的大量实验表明，我们的 UAE 能够有效地将语义抽象与像素级保真度统一到单一潜在空间中，并取得了最先进的性能。

Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies
自底向上策略优化：语言模型策略隐含内部策略
现有的强化学习 (RL) 方法将大语言模型 (LLMs) 视为单一的统一策略，忽略了其内部机制。因此，理解策略在不同层和模块间的演变过程，对于实现更具针对性的优化以及揭示复杂的推理机制至关重要。本文通过利用 Transformer 残差流的固有划分，以及隐藏状态与解嵌入矩阵的合成结果与最终可采样策略之间的等价性，对语言模型策略进行分解。这种分解揭示了内部层策略 (Internal Layer Policies)，其对应于各独立层的贡献；以及内部模块策略 (Internal Modular Policies)，其与每层中的自注意力机制和前馈网络 (FFN) 组件相关联。通过分析内部策略的熵，我们发现：(a) 早期层保持高熵以支持探索，顶层则收敛至接近零的熵以实现精细化，且收敛模式因模型系列不同而有所差异。(b) Llama 的预测空间在最后一层迅速收敛，而 Qwen 系列模型，尤其是 Qwen3，则展现出一种更接近人类、渐进结构化的推理模式。受这些发现启发，我们提出了自底向上策略优化 (Bottom-up Policy Optimization, BuPO)，这是一种新颖的 RL 范式，可在训练早期直接优化内部层策略。通过在底层对齐训练目标，BuPO 重构了基础推理能力，并取得了卓越的性能。在复杂推理基准上进行的大量实验证明了我们方法的有效性。我们的代码可在 github.com/Trae1ounG/B… 获取。

When Reasoning Meets Its Laws
当推理遇见其定律
尽管大推理模型 (Large Reasoning Models, LRMs) 性能卓越，但其推理行为常常有违直觉，导致推理能力未能达到最优。为了从理论层面形式化所期望的推理行为，本文提出了推理定律 (Laws of Reasoning, LoRe) 这一统一框架，用以刻画大推理模型内在的推理模式。我们首先提出了计算定律，其核心假设是推理计算量应与问题复杂度呈线性关系。除了计算量，我们还通过补充的准确率定律对推理定律进行了扩展。由于问题复杂度在实践中难以量化，我们借助该定律的两个可检验属性——单调性与组合性——来验证这些假设。为此，我们引入了 LoRe-Bench 基准测试，用于系统性地评估大推理模型的这两个可处理属性。评估结果表明，大多数推理模型具备合理的单调性，但缺乏组合性。针对此问题，我们开发了一种有效的微调方法，以强制模型满足计算定律的组合性要求。大量的实证研究表明，更好地遵循计算定律能够在多个基准测试上持续提升模型的推理性能，并揭示出不同属性与定律之间的协同效应。项目页面：lore-project.github.io/

Latent Implicit Visual Reasoning
潜在隐式视觉推理
尽管大型多模态模型（LMMs）已取得显著进展，但其本质上仍以文本为中心，将语言作为核心推理模态。因此，它们在处理以视觉为主的推理任务时能力有限。近期的一些方法尝试通过利用辅助图像、深度图或图像裁剪来监督中间视觉步骤，以解决此问题。然而，这些策略为“有用”的视觉抽象形态设定了限制性先验，带来了高昂的标注成本，且跨任务泛化能力较差。为克服这一关键局限，我们提出了一种任务无关的机制，它能训练 LMMs 自主发现并利用视觉推理标记，而无需任何显式监督。这些标记具有全局注意力，并能以任务自适应的方式对图像进行重新编码，从而使模型能够提取相关的视觉信息，无需依赖人工设计的监督信号。我们的方法性能优于直接微调，在多种视觉中心任务上——包括那些中间抽象难以明确定义的任务——均取得了最先进的成果，同时也能很好地泛化至多任务指令微调场景。

LongVideoAgent: 基于多智能体的长视频推理
LongVideoAgent: 基于多智能体的长视频推理
多模态大语言模型以及利用工具进行长视频问答的系统所取得的进展，展现了处理长达数小时视频内容并进行推理的潜力。然而，现有方法大多仍将视频内容压缩为有损摘要，或依赖于功能有限的工具集，这导致时间定位能力被削弱，且容易遗漏细粒度线索。为此，我们提出一个多智能体框架：一个主控大语言模型负责协调一个定位智能体来锁定与问题相关的视频片段，以及一个视觉智能体来提取有针对性的文本化视觉观察。主控智能体在预设的步骤限制内进行规划，并通过强化学习进行训练，旨在实现简洁、准确且高效的多智能体协作。该设计通过定位机制使主控智能体能专注于相关片段，利用视觉细节补充字幕信息，并生成可解释的推理轨迹。在我们新提出的 LongTVQA 和 LongTVQA+ 数据集（这两个剧集级数据集由 TVQA/TVQA+ 聚合而成）上，我们的多智能体系统性能显著优于多个强大的非智能体基线模型。实验还表明，强化学习能进一步强化已训练智能体的推理与规划能力。代码与数据将在 longvideoagent.github.io/ 发布。

Region-Constraint In-Context Generation for Instructional Video Editing
面向教学视频编辑的区域约束上下文生成
上下文生成 (In-context generation) 范式近期在教学图像编辑领域展现出强大能力，兼具数据高效性与合成高质量的特点。然而，将这种上下文学习应用于基于指令的视频编辑并非易事。若不指定编辑区域，结果可能出现编辑区域不准确，以及在去噪过程中编辑区域与非编辑区域之间发生 Token 干扰的问题。为解决这些问题，我们提出了 ReCo，一种新的教学视频编辑范式，其创新之处在于深入探究了上下文生成过程中编辑区域与非编辑区域之间的约束建模。技术上，ReCo 将源视频与目标视频沿空间（宽度）维度拼接，进行联合去噪。为校准视频扩散学习，ReCo 利用了两项正则化项：潜在正则化与注意力正则化，它们分别作用于单步反向扩散去噪后的潜在表示 (latents) 和注意力图 (attention maps)。潜在正则化旨在增大源视频与目标视频之间编辑区域的潜在差异，同时减小非编辑区域的差异，从而强化对编辑区域的修改，并抑制非编辑区域意外内容的生成。注意力正则化则抑制目标视频编辑区域中的 Token 对源视频对应区域 Token 的注意力，以此减轻目标视频生成新对象时来自源视频对应 Token 的干扰。此外，我们提出了一个大规模高质量的视频编辑数据集 ReCo-Data，包含 50 万对指令-视频样本，以支持模型训练。在四项主流基于指令的视频编辑任务上进行的大量实验，验证了我们所提方案的优越性。

Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience
Seed-Prover 1.5：通过经验学习精通本科级定理证明
近期，大语言模型 (LLM) 在生成严谨数学证明方面取得了重大进展。然而，利用大语言模型在形式化语言 (如 Lean) 中进行定理证明仍然面临挑战且计算开销巨大，尤其是在处理本科及以上难度的问题时。本文提出了 Seed-Prover 1.5，这是一个通过大规模智能体强化学习训练的形式化定理证明模型，并配套一个高效的测试时扩展工作流。该模型在强化学习过程中，通过与 Lean 等工具进行广泛交互，持续积累经验，从而显著提升了形式化定理证明的能力与效率。此外，结合自然语言证明领域的最新进展，我们的测试时扩展工作流有效弥合了自然语言与形式化语言之间的鸿沟。与现有最先进方法相比，Seed-Prover 1.5 在更小的计算预算下实现了更优的性能：它解决了 88% 的 PutnamBench (本科级)、80% 的 Fate-H (研究生级) 以及 33% 的 Fate-X (博士级) 问题。尤为突出的是，利用本系统，我们在 9 小时内解决了 2025 年普特南数学竞赛 12 道题目中的 11 道。我们的研究表明，由高质量形式化反馈驱动的经验学习规模化扩展，在形式化数学推理领域拥有巨大的发展潜力。

Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models
学习进行 4D 推理：视觉语言模型的动态空间理解
视觉语言模型 (VLM) 在通用理解任务上表现出色，但在动态空间推理 (DSR) 方面仍显薄弱。DSR 指的是对物体几何形状及其在三维空间中随时间演变的关系进行推理。这种薄弱主要归因于可扩展的 4D 感知训练资源匮乏。为弥合在数据集、基准和模型方面的这一差距，我们推出了 DSR 套件。首先，我们提出一种自动化流水线，能够从真实场景视频中为 DSR 任务生成多项选择题对。该流水线利用现代视觉基础模型，提取丰富的几何与运动信息，包括相机位姿、局部点云、物体掩膜、朝向以及三维轨迹。这些几何线索可用于构建用于模型训练的 DSR-Train 数据集，以及经过人工进一步精炼、用于评估的 DSR-Bench 基准。与先前研究相比，我们的数据强调以下特性：(i) 源自真实场景视频，(ii) 具备物体级和场景级的三维信息要求，(iii) 包含视点变换，(iv) 涉及多物体交互，以及 (v) 提供细粒度、分步骤的答案。除了数据贡献，我们还提出一个轻量级的几何选择模块 (GSM)，用于将几何先验无缝集成到 VLM 中。该模块能压缩问题语义，并从预训练的 4D 重建先验中提取与问题相关的知识，将其编码为一组紧凑的几何 token。这种有针对性的知识提取避免了无关知识对模型的干扰。实验表明，将 DSR-Train 和 GSM 集成到 Qwen2.5-VL-7B 模型中，能显著提升其动态空间推理能力，同时保持了在通用视频理解基准上的准确率。

## 每周AI论文速递（251229-260102）

mHC: Manifold-Constrained Hyper-Connections
mHC: 流形约束的超连接

近来，以超连接 (HC) 为代表的研究，通过扩展残差流宽度并多样化连接模式，对过去十年间确立的、普遍存在的残差连接范式进行了拓展。尽管这带来了显著的性能提升，但连接模式的多样化从根本上损害了残差连接固有的恒等映射特性，进而导致严重的训练不稳定性、受限的可扩展性，并产生了显著的内存访问开销。为应对这些挑战，我们提出了流形约束的超连接 (mHC)。这是一个通用框架，它将 HC 的残差连接空间投影到特定流形上，以恢复恒等映射特性，同时融合了严格的基础设施优化以确保效率。实证实验表明，mHC 能有效进行大规模训练，带来切实的性能提升与卓越的可扩展性。我们预计，mHC 作为 HC 的一种灵活且实用的扩展，将有助于更深入地理解拓扑架构设计，并为基础模型的演进提供有前景的方向。

Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding
用于改进长上下文理解的 Mindscape-Aware 检索增强生成

人类理解长而复杂的文本，依赖于对内容的整体语义表征。这种全局视角有助于组织先验知识、解释新信息，并整合分散在文档中的证据，这正体现了心理学中所揭示的人类 Mindscape-Aware（心智景观感知）能力。当前的检索增强生成 (RAG) 系统缺乏这种指导，因此在处理长上下文任务时面临困难。本文提出了 Mindscape-Aware RAG (MiA-RAG)，这是首个为基于大语言模型的 RAG 系统赋予显式全局上下文感知能力的方法。MiA-RAG 通过分层摘要构建一个全局语义表征（即心智景观），并以此为基础指导检索和生成过程。这使得检索器能够形成信息更丰富的查询嵌入，同时使生成器能够在连贯的全局上下文中对检索到的证据进行推理。我们在多种长上下文和双语基准测试上评估了 MiA-RAG 在基于证据的理解和全局语义整合方面的性能。结果表明，MiA-RAG consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.

InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion
InsertAnywhere: 融合4D场景几何与扩散模型以实现逼真的视频对象插入

基于扩散模型的视频生成技术近期取得了显著进展，为可控视频编辑带来了新的可能。然而，由于对4D场景的理解有限，以及对遮挡和光照效应的处理不足，实现逼真的视频对象插入 (VOI) 仍然面临挑战。本文提出了 InsertAnywhere，这是一个新的 VOI 框架，能够实现几何一致的对象放置和外观忠实的视频合成。我们的方法首先采用一个4D感知的掩码生成模块，该模块重建场景几何，并在视频帧间传播用户指定的对象位置，同时确保时间连贯性和遮挡一致性。在此空间基础之上，我们扩展了一个基于扩散的视频生成模型，以联合合成插入的对象及其周围的局部变化（如光照和阴影）。为了进行有监督训练，我们引入了 ROSE++，这是一个光照感知的合成数据集，通过将 ROSE 对象移除数据集转换为三元组（包含对象移除后的视频、对象存在时的视频以及由 VLM 生成的参考图像）而构建。大量实验表明，我们的框架能够在多样化的真实世界场景中生成几何合理且视觉连贯的对象插入效果，其性能显著优于现有的研究模型和商业模型。

Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss
通过辅助损失耦合混合专家模型中的专家与路由器

混合专家 (Mixture-of-Experts, MoE) 模型缺乏明确的约束来确保路由器的决策与专家的能力良好匹配，这最终会限制模型性能。为解决此问题，我们提出了专家-路由器耦合 (expert-router coupling, ERC) 损失，这是一种轻量级辅助损失，旨在将路由器的决策与专家能力紧密耦合。我们的方法将每个专家的路由器嵌入 (router embedding) 视为分配给该专家的 Token 的代理 Token (proxy token)，并将扰动后的路由器嵌入输入专家以获取其内部激活 (internal activations)。ERC 损失对这些激活施加了两项约束：(1) 每个专家对其自身代理 Token 的激活必须高于对其他任何专家代理 Token 的激活。(2) 每个代理 Token 在其对应专家处激发的激活必须强于在其他任何专家处激发的激活。这些约束共同作用，确保每个路由器嵌入能准确表征其对应专家的能力，同时使每个专家专注于处理实际被路由至它的 Token。ERC 损失计算高效，仅需处理 n^2 个激活（n 为专家数量）。这意味着一个与批次大小无关的固定开销，不同于先前那些计算成本随 Token 数量（通常每批次达数百万）增长的耦合方法。通过对参数量从 30亿到 150亿的 MoE-LLMs 进行预训练，并基于数万亿 Token 进行广泛分析，我们验证了 ERC 损失的有效性。此外，ERC 损失还能在训练过程中对专家的专业化程度进行灵活控制和定量追踪，从而为理解 MoE 模型提供了宝贵洞见。

Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models
Youtu-LLM: 释放轻量级大语言模型的原生智能体潜力

我们推出 Youtu-LLM，这是一个轻量级但功能强大的语言模型，它成功兼顾了高计算效率与原生智能体智能。与通常依赖知识蒸馏的小型模型不同，Youtu-LLM (1.96B) 采用从头预训练的方式，旨在系统性地发展其推理与规划能力。其关键技术进展如下：(1) 支持长上下文的紧凑架构：该模型基于密集的多潜在注意力 (MLA) 架构和一种新颖的 STEM 导向词汇表构建，支持长达 128k 的上下文窗口。这一设计使其能够在极小的内存开销下实现稳健的长上下文推理与状态追踪，非常适合长程智能体任务和推理任务。(2) 结构化的 "常识-STEM-智能体" 课程学习：我们构建了一个约 11T token 的大规模语料库，并采用多阶段训练策略。通过逐步将预训练数据的分布从通用常识转向复杂的 STEM 及智能体任务，我们确保模型获得的是深层的认知能力，而非浅层的任务对齐。(3) 可扩展的智能体中期训练：针对智能体中期训练，我们采用了多样化的数据构建方案，在数学、代码和工具使用等领域合成了丰富多样的行动轨迹。这些高质量数据使模型能够有效地内化规划与反思行为。广泛的评估表明，Youtu-LLM 为参数量小于 20 亿的大语言模型树立了新的性能标杆。在通用基准测试中，其性能可与更大的模型相媲美；而在智能体专项任务上，它则显著超越了现有的 SOTA 基线模型，这证明轻量级模型同样可以具备强大的内在智能体能力。

Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling
基于超图记忆改进多步 RAG 以进行长上下文复杂关系建模

多步检索增强生成 (RAG) 已成为一项广泛采用的策略，用于增强大语言模型 (LLM) 在需要全局理解和深度推理任务上的性能。许多 RAG 系统集成了工作记忆模块以整合检索到的信息。然而，现有的记忆设计主要充当被动存储器，仅用于积累孤立的事实，以压缩冗长输入并通过演绎生成新的子查询。这种静态特性忽略了原始事实间关键的高阶关联，而这些事实的组合往往能为后续步骤提供更强的指导。因此，其表示能力以及对多步推理和知识演进的影响有限，导致在长上下文处理中出现推理碎片化和全局理解能力薄弱的问题。我们提出了 HGMem，一种基于超图的记忆机制，它将记忆的概念从简单的存储扩展为一种动态、富有表现力的结构，以支持复杂推理和全局理解。在我们的方法中，记忆被表示为一个超图，其超边对应不同的记忆单元，从而能够在记忆内部逐步形成高阶交互。该机制围绕核心问题连接事实与思路，演进为一个一体化、情境化的知识结构，为后续步骤的深度推理提供有力支撑。我们在多个专为测试全局理解能力设计的挑战性数据集上评估了 HGMem。大量实验和深入分析表明，我们的方法能持续提升多步 RAG 的性能，并在多种任务上显著优于各强基线系统。

LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation
LiveTalk: 通过改进的同策略蒸馏实现实时多模态交互式视频扩散

利用扩散模型进行实时视频生成，对于构建通用多模态交互式 AI 系统至关重要。然而，扩散模型通过迭代过程、结合双向注意力对所有视频帧进行同步去噪，这阻碍了实时交互。虽然现有的蒸馏方法可以使模型具备自回归特性并减少采样步数以缓解此问题，但这些方法主要集中于文生视频任务，导致人机交互显得不自然且效率低下。本文旨在实现一种基于多模态上下文（包括文本、图像和音频）的实时交互式视频扩散模型，以弥合这一差距。我们观察到，领先的同策略蒸馏方法 Self Forcing 在多模态条件输入下会面临挑战（出现闪烁、黑帧和质量下降等视觉伪影）。为此，我们研究了一种改进的蒸馏方案，该方案着重优化条件输入的质量，以及同策略优化的初始化和调度策略。在 HDTF、AVSpeech 和 CelebV-HQ 等多模态条件（音频、图像和文本）驱动的虚拟形象视频生成基准测试中，我们蒸馏得到的模型，在推理成本和延迟降低 20 倍的前提下，其视觉质量与相似或更大规模的全步长双向基线模型相当。此外，我们将该模型与音频语言模型以及长视频推理技术 Anchor-Heavy Identity Sinks 相结合，构建了 LiveTalk——一个实时多模态交互式虚拟形象系统。在我们精心构建的多轮交互基准上进行的系统级评估显示，LiveTalk 在多轮视频连贯性和内容质量方面均优于 Sora2、Veo3 等最先进的模型，同时将响应延迟从 1-2 分钟大幅降低至实时生成水平，从而实现了流畅无缝的人机多模态交互。

Yume-1.5: A Text-Controlled Interactive World Generation Model
Yume-1.5: 一个文本控制的交互式世界生成模型

近期的一些方法展现了利用扩散模型生成交互式、可探索世界的潜力。然而，这些方法大多面临关键挑战，例如参数量过大、依赖冗长的推理步骤以及历史上下文快速增长，这些问题严重限制了实时性能，并且缺乏文本控制生成能力。为解决这些挑战，我们提出了 \method，这是一个新颖的框架，用于从单张图像或文本提示生成逼真、交互且连续的世界。\method 通过一个精心设计的框架实现此目标，该框架支持基于键盘对生成的世界进行探索。该框架包含三个核心组件：(1) 一个集成了统一上下文压缩与线性注意力的长视频生成框架；(2) 一个由双向注意力蒸馏和增强文本嵌入方案驱动的实时流式加速策略；(3) 一种用于生成世界事件的文本控制方法。相关代码库已提供在补充材料中。

Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem
任其流动：基于 ROCK 与 ROLL 的智能体式构建，在开放智能体学习生态系统中打造 ROME 模型

智能体式构建 (Agentic crafting) 要求大语言模型 (LLM) 在现实环境中通过多轮操作来执行任务，具体包括采取行动、观察结果并迭代优化其生成的制品。尽管该能力至关重要，但开源社区目前仍缺乏一个系统化、端到端的生态系统来简化智能体的开发流程。为此，我们提出了智能体学习生态系统 (Agentic Learning Ecosystem, ALE)，这是一个旨在优化智能体大语言模型生产流水线的基础设施。ALE 包含三个核心组件：ROLL，一个用于权重优化的后训练 (post-training) 框架；ROCK，一个用于生成训练轨迹的沙盒环境管理器；以及 iFlow CLI，一个用于高效上下文工程 (context engineering) 的智能体框架。我们发布了 ROME (ROME is Obviously an Agentic Model)，这是一个基于 ALE 构建、并在超过一百万条轨迹上训练而成的开源智能体模型。我们的方法包含用于合成复杂行为的数据组合协议 (data composition protocols)，以及一种新颖的策略优化算法——基于交互的策略对齐 (Interaction-based Policy Alignment, IPA)。IPA 算法在语义交互块而非单个 Token 上分配信用，从而提升了长视野 (long-horizon) 训练的稳定性。在实证评估中，我们在一个结构化设置中对 ROME 进行了测试，并推出了 Terminal Bench Pro 基准测试，该基准在规模和污染控制方面均有改进。ROME 在 SWE-bench Verified 和 Terminal Bench 等多个基准测试中均展现出强劲性能，这证明了 ALE 基础设施的有效性。

## 每周AI论文速递（260105-260109）

GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization
GDPO: 面向多奖励RL优化的组奖励解耦归一化策略优化
随着语言模型能力日益增强，用户不仅期望其提供准确的响应，还希望它们能在多样化的场景中表现出符合不同人类偏好的行为。为此，强化学习 (RL) 训练框架已开始整合多个奖励信号，每个奖励对应一种特定偏好，以引导模型产生这些期望行为。然而，近期研究默认在多奖励设置下直接采用组相对策略优化 (GRPO)，而未深入探究其适用性。本文证明，直接应用GRPO对不同rollout奖励组合进行归一化，会导致这些组合的优势值坍缩为相同数值，从而降低训练信号的分辨率，导致收敛结果次优，甚至在部分情况下引发早期训练失败。为此，我们提出了组奖励解耦归一化策略优化 (GDPO)。这一新策略优化方法通过解耦各奖励的归一化过程，解决了上述问题，能更真实地保留奖励间的相对差异，从而实现更精确的多奖励优化，并大幅提升训练稳定性。我们在工具调用、数学推理和代码推理三个任务上对比了GDPO与GRPO，评估了包括正确性指标（准确率、错误率）和约束遵循指标（格式、长度）。在所有实验设置下，GDPO均一致优于GRPO，证明了其在多奖励强化学习优化中的有效性和泛化能力。

NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos
NeoVerse: 利用真实世界单目视频增强 4D 世界模型
本文提出 NeoVerse，一个多功能 4D 世界模型，能够执行 4D 重建、新视角轨迹视频生成以及丰富的下游任务。我们首先指出，当前 4D 世界建模方法普遍存在可扩展性局限，其根源在于依赖昂贵且专用的多视图 4D 数据，或训练预处理流程繁琐。相比之下，NeoVerse 基于一个核心设计理念，使得整个流程能够轻松扩展至多样化的真实世界单目视频。具体而言，NeoVerse 具备免姿态前馈 4D 重建、在线单目退化模式模拟以及其他协调一致的技术。这些设计使 NeoVerse 具备了多功能性以及对多种领域的泛化能力。同时，NeoVerse 在标准重建与生成基准测试中取得了最先进的性能。项目页面详见 neoverse-4d.github.io。

Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization
Youtu-Agent: 通过自动化生成与混合策略优化提升智能体生产力
现有的大语言模型 (LLM) 智能体框架面临两大挑战：配置成本高昂与能力固化。构建高质量智能体通常需要在工具集成和提示工程上投入大量人工，而已部署的智能体若不进行代价高昂的微调，则难以适应动态环境。为解决这些问题，我们提出了 Youtu-Agent，这是一个专为 LLM 智能体自动化生成与持续进化设计的模块化框架。Youtu-Agent 具备结构化配置系统，实现了执行环境、工具包与上下文管理的解耦，从而支持灵活复用与自动化组装。我们引入了两种生成范式：面向标准任务的 工作流 模式，以及面向复杂、非规范化需求的 元智能体 模式，后者能够自动生成工具代码、提示词及配置。此外，Youtu-Agent 构建了一套混合策略优化系统：(1) 智能体实践 模块，使智能体能够通过上下文内优化积累经验、提升性能，且无需更新模型参数；(2) 智能体强化学习 模块，可与分布式训练框架集成，支持以端到端、大规模的方式对任意 Youtu-Agent 进行可扩展且稳定的强化学习。实验表明，使用开源权重模型时，Youtu-Agent 在 WebWalkerQA (71.47%) 和 GAIA (72.8%) 基准上取得了领先性能。我们的自动化生成流程工具合成成功率超过 81%，而实践模块在 AIME 2024 和 2025 上分别将性能提升了 2.7% 和 5.4%。此外，我们的智能体强化学习训练在 7B 参数规模的 LLM 上实现了 40% 的加速，同时性能持续稳定提升，在数学及通用/多跳问答基准测试中，编码/推理与搜索能力分别最高提升了 35% 和 21%。

InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields
InfiniDepth: 基于神经隐式场的任意分辨率细粒度深度估计
现有的深度估计方法本质上受限于在离散的图像网格上预测深度。这种表示形式限制了其向任意输出分辨率的扩展能力，并影响了几何细节的还原。本文提出了 InfiniDepth，该方法将深度表示为神经隐式场。通过一个简单而有效的局部隐式解码器，我们可以在连续的二维坐标处查询深度值，从而实现任意分辨率下的细粒度深度估计。为了更全面地评估本方法的性能，我们从五款不同的游戏中构建了一个高质量的 4K 合成基准数据集，该数据集涵盖了具有丰富几何与外观细节的多样化场景。大量实验表明，无论是在合成数据还是真实世界数据的基准测试上，InfiniDepth 在相对深度估计和度量深度估计任务中均达到了最先进的性能，尤其在精细细节区域的表现尤为突出。此外，该方法也能显著提升大视角变化下新视图合成任务的效果，所生成的结果质量更高，且空洞和伪影更少。

LTX-2: Efficient Joint Audio-Visual Foundation Model
LTX-2: 高效的联合视听基础模型
当前的文本到视频扩散模型虽能生成高质量的视频序列，但通常是无声的，缺乏音频所能提供的语义、情感及氛围线索。为此，我们提出了 LTX-2，这是一个开源的基础模型，能够以统一的方式生成高质量且时间同步的视听内容。LTX-2 采用非对称双流 Transformer 架构，其中视频流包含 140 亿参数，音频流包含 50 亿参数。两个流通过双向视听交叉注意力层进行耦合，该层包含时间位置嵌入以及用于共享时间步条件化的跨模态 AdaLN。此架构不仅实现了统一视听模型的高效训练与推理，还为视频生成分配了比音频生成更多的模型容量。
我们采用了多语言文本编码器，以支持对更广泛提示词的理解，并引入了一种模态感知的无分类器引导机制，从而提升了视听对齐效果与可控性。除了生成语音，LTX-2 还能生成丰富、连贯的音频轨道，这些音频能够贴合每个场景的角色、环境、风格与情感，并包含自然的背景音和拟音效果。在我们的评估中，该模型在开源系统中实现了最先进的视听质量与提示跟随性，同时仅需专有模型一小部分的计算成本和推理时间，便能达到与之相当的效果。所有模型权重和代码均已开源发布。

Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting
熵自适应微调：解决自信冲突以缓解遗忘
监督微调 (Supervised Fine-Tuning, SFT) 是领域适应的标准范式，但它常常以灾难性遗忘为代价。与此形成鲜明对比的是，基于策略的强化学习 (Reinforcement Learning, RL) 能有效保留模型的通用能力。我们探究了这种差异，并发现了一个根本性的分布不匹配问题：RL 与模型的内在信念保持一致，而 SFT 则迫使模型拟合外部监督信号。这种不匹配通常表现为一种"自信冲突" (Confident Conflicts) 的 Token，其特征是预测概率低但熵值也低。在这种情况下，模型对其自身预测高度自信，却被迫学习与之相悖的真实标签，从而引发破坏性的梯度更新。为解决此问题，我们提出了熵自适应微调 (Entropy-Adaptive Fine-Tuning, EAFT)。与仅依赖预测概率的方法不同，EAFT 利用 Token 级别的熵作为门控机制，以区分认知不确定性和知识冲突。这使得模型能够从不确定的样本中学习，同时抑制来自冲突数据的梯度。我们在数学、医学和 AI 智能体领域，对 Qwen 和 GLM 系列模型 (参数规模从 4B 到 32B) 进行了广泛实验，结果证实了我们的假设。EAFT 在始终达到与标准 SFT 相当的下游任务性能的同时，显著缓解了通用能力的衰退。

K-EXAONE Technical Report
K-EXAONE 技术报告
本技术报告介绍了 K-EXAONE，这是一个由 LG AI Research 开发的大规模多语言大语言模型。K-EXAONE 基于专家混合 (Mixture-of-Experts, MoE) 架构构建，总参数量为 236B，推理时激活参数量为 23B。它支持 256K Token 的上下文窗口，并涵盖六种语言：韩语、英语、西班牙语、德语、日语和越南语。我们在一个涵盖推理、智能体能力、通用能力、韩语能力及多语言能力的综合基准测试套件上对 K-EXAONE 进行了评估。在这些评估中，K-EXAONE 展现出了与同类规模的开源模型相当的性能。K-EXAONE 旨在通过推进人工智能技术来创造更美好的生活，其定位是一个强大的闭源 AI 基础模型，适用于广泛的工业与科研应用。

Evolving Programmatic Skill Networks
演化式程序化技能网络
我们研究在开放域具身环境中持续的技能获取问题，智能体需要构建、优化并重用其不断增长的可执行技能库。我们提出了程序化技能网络（PSN），该框架中的技能是可执行的符号程序，它们构成一个组合网络，并通过经验不断演化。PSN 定义了三个由大语言模型实例化的核心机制：(1) 用于对技能组合进行结构化故障定位的 REFLECT，(2) 具备成熟度感知更新门控的渐进式优化，该机制能稳定可靠技能，同时为不确定技能保持可塑性，以及 (3) 在回滚验证下的规范结构重构，以维持网络紧凑性。我们进一步指出，PSN 的学习动态在结构上与神经网络训练存在相似性。在 MineDojo 和 Crafter 环境上的实验表明，该方法在开放域任务分布上具有强大的技能重用能力、快速适应能力和优异的泛化性能。\footnote{我们计划开源代码。

Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits
大语言模型能预测自身的失败吗？基于内部路径的自我感知
大语言模型 (LLMs) 能够生成流畅且复杂的输出，但常常无法识别自身的错误和幻觉。现有方法通常依赖于外部评判器、多样本一致性或基于文本的自我批判，这些方法要么会产生额外的计算开销，要么与真实正确性的关联较弱。我们探讨一个问题：大语言模型能否通过检查推理过程中的内部状态来预测自身的失败？我们提出了 Gnosis，一种轻量级的自我感知机制，它使参数冻结的 LLMs 能够通过解码其隐藏状态和注意力模式的信号，进行内在的自我验证。Gnosis 被动地观察内部轨迹，将其压缩为固定资源占用的描述符，并以极低的推理开销预测正确性，仅增加约 500 万个参数，且其运行与序列长度无关。在数学推理、开放域问答和学术知识基准测试中，在参数规模从 17 亿到 200 亿不等的多个冻结骨干模型上，Gnosis 在准确性和校准度方面均持续优于性能强劲的内部基线模型和规模庞大的外部评判器。此外，它能够零样本泛化到部分（不完整）的生成结果，从而实现对失败生成路径的早期检测，并进行计算感知的控制。这些结果表明，可靠的正确性线索本就存在于生成过程之中，无需外部监督即可高效提取。

NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation
NextFlow: 统一序列建模赋能多模态理解与生成
我们提出了 NextFlow，这是一个统一的仅解码器自回归 Transformer 模型，在 6 万亿交织的文本-图像离散 Token 上训练而成。通过在统一的自回归架构内利用统一的视觉表示，NextFlow 原生具备了多模态理解与生成能力，实现了图像编辑、交织内容生成以及视频生成等功能。鉴于不同模态的本质差异——文本具有严格的顺序性，而图像则具有内在的层次性——我们为文本保留了下一 Token 预测，但对视觉生成采用了下一尺度预测。这有别于传统的光栅扫描方法，使得生成 1024x1024 分辨率图像仅需 5 秒，比同类自回归 (AR) 模型快数个数量级。我们通过一套稳健的训练方案解决了多尺度生成的不稳定性问题。此外，我们还引入了一种用于强化学习的前缀调优 (prefix-tuning) 策略。实验表明，NextFlow 在统一模型中取得了最先进的性能，并且在视觉质量上可与专门的扩散 (diffusion) 基线模型相匹敌。

MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization
MOSS Transcribe Diarize: 带说话人角色分离的精准转录
带说话人角色和时间戳的转录 (Speaker-Attributed, Time-Stamped Transcription, SATS) 旨在转录语音内容，并精确确定每位说话人的发言时间点，这对于会议转录尤其有价值。现有的 SATS 系统很少采用端到端方案，并且普遍存在上下文窗口有限、长距离说话人记忆能力弱以及无法输出时间戳等局限。为了克服这些不足，我们提出了 MOSS Transcribe Diarize，这是一个统一的多模态大语言模型，能够以端到端的方式联合完成带说话人角色和时间戳的转录。该模型在大量真实场景数据上进行训练，并配备了可处理长达 90 分钟音频输入的 128k 上下文窗口，因此具有良好的可扩展性和强大的泛化能力。在全面的评估中，它在多个公开及内部基准测试上的表现均优于当前最先进的商业系统。

Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation
Avatar Forcing：面向自然对话的实时交互式头部数字人生成
说话头生成技术旨在从静态肖像中创建逼真的数字人 (Avatar) ，以用于虚拟交流与内容创作。然而，现有模型尚无法营造出真正交互式交流的体验，其生成的响应往往是单向的，缺乏情感参与感。我们为实现真正交互式的数字人，识别出两个关键挑战：一是在因果约束条件下实时生成运动；二是在无需额外标注数据的情况下，学习富有表现力且鲜活自然的反应。为应对这些挑战，我们提出了 Avatar Forcing，这是一个用于交互式头部数字人生成的新框架，它通过扩散驱动 (Diffusion Forcing) 对实时用户-数字人交互进行建模。该设计使得数字人能够以低延迟处理实时多模态输入（包括用户的音频和动作），从而对语音、点头、笑声等言语与非言语线索做出即时反应。此外，我们引入了一种直接偏好优化方法，该方法利用通过丢弃用户条件构建的合成负样本，实现了对富有表现力交互的无标签学习。实验结果表明，我们的框架能够实现低延迟（约500毫秒）的实时交互，相比基线模型获得了6.8倍的加速，并生成了反应灵敏且富有表现力的数字人运动，在超过80%的评估中优于基线模型。

Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation
抑制幻觉：通过反事实视频生成增强 MLLMs 的视频理解能力
多模态大语言模型 (Multimodal Large Language Models, MLLMs) 在视频理解方面取得了显著进展。然而，它们存在一个关键缺陷：过度依赖语言先验，这可能导致视觉上无根据的幻觉，尤其是在处理违背常识的反事实视频时。这一局限源于文本与视频之间固有的数据不平衡，而由于收集和标注反事实数据成本高昂，该问题难以解决。为此，我们提出了 DualityForge，一个新颖的反事实数据合成框架。该框架利用可控的、基于扩散模型的视频编辑技术，将真实世界视频转化为反事实场景。通过将结构化的上下文信息嵌入视频编辑和问答生成过程，该框架能自动生成高质量的问答对以及用于对比训练的原始视频与编辑后视频配对。基于此，我们构建了 DualityVidQA，一个旨在减少 MLLM 幻觉的大规模视频数据集。此外，为充分利用配对数据的对比特性，我们提出了 Duality 归一化优势训练 (Duality-Normalized Advantage Training, DNA-Train)。这是一个两阶段的 SFT-RL 训练机制，其中强化学习阶段应用了成对的 ℓ1\ell_1ℓ1​ 优势归一化，从而实现更稳定、高效的策略优化。在 DualityVidQA-Test 上的实验表明，我们的方法能显著降低模型在反事实视频上的幻觉，相比 Qwen2.5-VL-7B 基线获得了 24.0% 的相对提升。此外，我们的方法在幻觉评测和通用基准测试上均取得了显著进步，显示出强大的泛化能力。我们将开源数据集和代码。

Recursive Language Models
递归语言模型
我们从推理时扩展的角度，研究如何让大语言模型 (LLMs) 能够处理任意长度的提示。我们提出了递归语言模型 (RLMs)，这是一种通用的推理策略。它将长提示视为外部环境的一部分，使大语言模型能够以编程方式对提示片段进行检查、分解，并递归地调用自身进行处理。我们发现，在四个不同的长上下文任务中，RLMs 能够成功处理长度超出模型原始上下文窗口两个数量级的输入。并且，即使是对于较短的提示，其输出质量也显著优于基础大语言模型和常见的长上下文扩展框架，同时每次查询的成本与之相当（甚至更低）。

DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer
DreamID-V：通过扩散 Transformer 弥合图像到视频鸿沟以实现高保真人脸交换
视频人脸交换 (VFS) 旨在将源身份无缝注入目标视频，同时精确保持原始的姿态、表情、光照、背景及动态信息。现有方法在维持时序一致性的同时，往往难以兼顾身份相似度与属性保留。为应对这一挑战，我们提出了一个综合性框架，旨在将图像人脸交换 (IFS) 的优势无缝迁移至视频领域。我们首先引入了一种新颖的数据流水线 SyncID-Pipe，它预训练了一个身份锚定视频合成器，并将其与 IFS 模型相结合，构建用于显式监督的双向 ID 四元组。基于此配对数据，我们提出了首个基于扩散 Transformer 的框架 DreamID-V，其核心是一个模态感知条件模块，用于区分性地注入多模态条件。同时，我们提出了一种合成到真实的课程学习机制以及一种身份一致性强化学习策略，以提升在复杂场景下的视觉真实感与身份一致性。针对现有基准测试有限的问题，我们引入了 IDBench-V，这是一个涵盖多样化场景的综合基准测试集。大量实验表明，DreamID-V 性能优于现有最先进方法，并展现出出色的通用性，能够无缝适配各类与人脸交换相关的任务。

## 每周AI论文速递（260112-260116）

Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning
观看、推理与搜索：面向智能体视频推理的开放网络视频深度研究基准
在现实世界的视频问答场景中，视频本身通常仅提供局部视觉线索，而可验证的答案广泛分布于开放网络。因此，模型需要协同完成跨帧线索提取、迭代式检索以及基于多跳推理的验证。为弥补这一差距，我们构建了首个视频深度研究基准 VideoDR。VideoDR 的核心是视频条件化的开放域视频问答，要求模型执行跨帧视觉锚点提取、交互式网络检索，并对视频与网络联合证据进行多跳推理。通过严格的人工标注与质量控制，我们构建了涵盖六个语义领域的高质量视频深度研究样本集。我们评估了在流程式 (Workflow) 与智能体式 (Agentic) 两种范式下的多个闭源及开源多模态大语言模型。结果表明，智能体式 (Agentic) 范式并非总是优于流程式 (Workflow) 范式：其性能提升取决于模型在长链检索过程中维持初始视频锚点的能力。进一步分析指出，目标漂移 (goal drift) 和长程一致性 (long-horizon consistency) 是核心瓶颈。总之，VideoDR 为研究开放网络环境下的视频智能体提供了一个系统性基准，并揭示了下一代视频深度研究智能体所面临的关键挑战。

BabyVision: Visual Reasoning Beyond Language
BabyVision: 超越语言的视觉推理
人类在掌握语言能力之前很久便已发展出核心视觉技能，然而，当代的多模态大语言模型 (Multimodal LLMs, MLLMs) 仍严重依赖语言先验来弥补其薄弱的视觉理解能力。我们发现了一个关键事实：最先进的多模态大语言模型在人类（即使是3岁儿童）都能轻松解决的基本视觉任务上持续表现不佳。为了系统地探究这一差距，我们提出了 BabyVision 基准，旨在评估多模态大语言模型独立于语言知识的核心视觉能力。BabyVision 涵盖广泛的任务，包含388个测试项，划分为四个关键类别下的22个子类。实证结果与人工评估表明，领先的多模态大语言模型性能显著低于人类基线。Gemini3-Pro-Preview 的得分为49.7，落后于6岁儿童的表现，且远低于成人平均分94.1。这些结果表明，尽管当前的多模态大语言模型在知识密集型评估中表现出色，但它们仍然缺乏基本的视觉原语。BabyVision 的进展是迈向人类水平视觉感知与推理能力的一步。我们还通过提出 BabyVision-Gen 和自动评估工具包，探索了利用生成模型解决视觉推理问题。我们的代码和基准数据已在 github.com/UniPat-AI/B… 发布，以供复现。

Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization
基于地图思考：用于地理定位的强化并行地图增强智能体
图像地理定位任务旨在利用视觉线索，预测图像在地球上的拍摄位置。现有的大型视觉语言模型 (LVLM) 方法利用了世界知识、思维链 (Chain-of-Thought) 推理和智能体 (Agent) 能力，但忽略了人类常用的一种策略——使用地图。在本工作中，我们首先赋予模型 基于地图思考 的能力，并将其形式化为一个地图中的智能体循环。我们为此开发了一个两阶段优化方案：首先是智能体强化学习 (RL)，随后是并行测试时缩放 (TTS)。RL 阶段旨在增强模型的智能体能力，以提高其采样效率；而并行 TTS 阶段则允许模型在做出最终预测前并行探索多条候选路径，这对地理定位任务至关重要。为了在最新且真实（非合成）的图像上评估我们的方法，我们进一步提出了 MAPBench，这是一个完全基于真实世界图像的综合性地理定位训练与评估基准。实验结果表明，我们的方法在大多数指标上优于现有的开源和闭源模型。具体而言，与启用 Google 搜索/地图定位模式的 Gemini-3-Pro 相比，我们的方法将 Acc@500m 指标从 8.0% 显著提升至 22.1%。

STEP3-VL-10B Technical Report
STEP3-VL-10B 技术报告
我们推出 STEP3-VL-10B，这是一个轻量级开源基础模型，旨在重新权衡紧凑效率与前沿多模态智能能力。STEP3-VL-10B 通过两项战略转变实现：首先，在 1.2T 多模态 Token 上采用统一且全部参数可训练的预训练策略，将语言对齐的感知编码器与 Qwen3-8B 解码器集成，以建立内在的视觉-语言协同；其次，采用一个大规模后训练流程，包含超过 1000 次强化学习迭代。关键在于，我们实施了并行协调推理 (PaCoRe) 来扩展测试时的计算规模，将资源分配给可扩展的感知推理，以探索并整合多样化的视觉假设。因此，尽管其模型规模紧凑，仅为 10B 参数，STEP3-VL-10B 的性能却媲美甚至超越了规模大 10 到 20 倍的模型（例如 GLM-4.6V-106B、Qwen3-VL-235B）以及顶级的闭源旗舰模型，如 Gemini 2.5 Pro 和 Seed-1.5-VL。它实现了同类最佳的绩效，在 MMBench 上取得 92.2% 的得分，在 MMMU 上取得 80.11% 的得分；同时在复杂推理任务中表现卓越，在 AIME2025 上达到 94.43%，在 MathVision 上达到 75.95%。我们发布了完整的模型套件，旨在为社区提供一个强大、高效且可复现的基准。

Urban Socio-Semantic Segmentation with Vision-Language Reasoning
基于视觉语言推理的城市社会语义分割
城市作为人类活动的中心，其区域包含丰富的语义实体。从卫星影像中分割这些多样的实体，对众多下游应用至关重要。当前先进的分割模型能够可靠地分割由物理属性定义的实体 (例如，建筑物、水体) ，但在处理社会语义类别 (例如，学校、公园) 时仍面临挑战。本研究通过视觉语言模型 (Vision-Language Model) 的推理能力，实现了社会语义分割。为此，我们提出了一个名为 SocioSeg 的城市社会语义分割数据集，该新资源包含卫星影像、数字地图以及按层级结构组织的社会语义实体的像素级标注。此外，我们提出了一种名为 SocioReasoner 的新型视觉语言推理框架，它通过跨模态识别与多阶段推理，模拟人类识别与标注社会语义实体的过程。我们采用强化学习来优化这一不可微的流程，从而有效激发视觉语言模型固有的推理能力。实验结果表明，我们的方法相较于现有最优模型取得了性能提升，并展现出强大的零样本泛化能力。我们的数据集与代码已公开于 github.com/AMAP-ML/Soc…

Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs
奖励稀缺性：面向大语言模型创造性问题解决的独特性感知强化学习
强化学习已成为大语言模型后训练，尤其是针对复杂推理任务的核心范式。然而，该方法常面临探索崩溃问题：模型策略过早地收敛于少数几种主导的推理模式。这虽然能提升 pass@1 指标，却限制了推理路径层面的多样性，从而抑制了 pass@k 指标的进一步增益。我们认为，问题的根源在于现有方法侧重于对局部 Token 行为进行正则化，而非鼓励解决方案集合的多样性。为此，我们提出了独特性感知强化学习。该方法定义了一个推理路径层面的优化目标，明确奖励那些采用罕见高级策略的正确解决方案。具体而言，我们利用一个基于大语言模型的评判器，将针对同一问题生成的不同推理路径按其高级解决策略（忽略表面差异）进行聚类，并依据聚类规模反比地重新调整策略优势值。如此一来，正确且新颖的策略将获得比冗余策略更高的奖励。在数学、物理和医学推理等多个基准测试上的实验表明，我们的方法能够在大规模采样预算下持续提升 pass@kkk 指标，并增大 pass@kkk 曲线下面积，同时不损害 pass@1 性能。该方法有效维持了探索过程，并能在更大规模上发掘出更多样化的解决方案策略。

DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation
DeepResearchEval：一个用于深度研究任务构建与智能体评估的自动化框架
深度研究系统已广泛应用于多步骤网络研究、分析与多源信息综合，但其评估仍面临挑战。现有基准测试往往需要大量标注来构建任务，依赖静态的评估维度，或在引文缺失时无法可靠地验证事实。为填补这些空白，我们提出了 DeepResearchEval，一个用于深度研究任务构建与智能体 (Agentic) 评估的自动化框架。在任务构建方面，我们设计了一个角色 (Persona) 驱动的流程，能够基于多样化的用户画像生成真实、复杂的研究任务，并应用一个两阶段过滤器（任务资格 (Task Qualification) 与搜索必要性 (Search Necessity)）来筛选出仅那些需要整合多源证据并进行外部检索的任务。在评估方面，我们提出了一个智能体流程，包含两个组件：一是自适应点式质量评估 (Adaptive Point-wise Quality Evaluation)，它能根据每个生成的任务动态推导出任务特定的评估维度、标准及权重；二是主动事实核查 (Active Fact-Checking)，它能通过网页搜索自主提取并验证报告中的陈述，即便在引文缺失的情况下也能进行。

Controlled Self-Evolution for Algorithmic Code Optimization
面向算法代码优化的受控自进化
自进化方法通过迭代式的“生成-验证-精炼”循环来提升代码生成质量。然而，现有方法探索效率低下，难以在有限预算内发现复杂度更优的解决方案。这种低效源于几个瓶颈：初始化偏差会将进化过程限制在较差的解空间区域；随机操作缺乏反馈引导，不可控；且跨任务的经验未能得到充分利用。为应对这些挑战，我们提出了受控自进化 (Controlled Self-Evolution, CSE)，其包含三个核心组件。多样化规划初始化通过生成结构各异的算法策略，以实现对解空间的广泛覆盖。遗传进化则采用反馈引导机制替代随机操作，从而支持有针对性的变异与组合式交叉。分层进化记忆能够在任务间与任务内两个层面，同时捕获成功与失败的经验。在EffiBench-X基准上的实验表明，CSE在使用不同大语言模型 (LLM) 骨干时，均稳定优于所有基线方法。此外，CSE在进化早期即展现出更高的效率，并能在此后的整个进化过程中持续改进。我们的代码已公开于 github.com/QuantaAlpha…

MMFormalizer: Multimodal Autoformalization in the Wild
MMFormalizer: 真实场景下的多模态自动形式化
自动形式化 (Autoformalization) 旨在将自然语言描述的数学内容转化为形式化语句，以实现机器推理。然而，在真实物理世界的开放场景下，由于其多模态特性，自动形式化面临着根本性挑战：物理学问题常常需要从视觉元素中推断出隐藏的约束条件 (例如质量或能量)。为此，我们提出了 MMFormalizer，它通过整合来自真实世界数学和物理领域的实体进行自适应基础 (adaptive grounding)，从而将自动形式化的范畴从纯文本扩展到了多模态。MMFormalizer 通过递归基础 (recursive grounding) 和公理组合 (axiom composition)，从感知基础 (perceptually grounded) 的基元出发，递归地构建形式化命题。其自适应的递归终止机制确保每一个抽象概念都有视觉证据支撑，并基于维度或公理基础 (dimensional or axiomatic grounding)。我们在一个新构建的基准测试 PhyX-AF 上评估了 MMFormalizer。该基准包含从 MathVerse、PhyX、综合几何 (Synthetic Geometry) 和解析几何 (Analytic Geometry) 中精心挑选的 115 个样本，涵盖了多样化的多模态自动形式化任务。结果表明，GPT-5 和 Gemini-3-Pro 等前沿模型在形式化编译和语义准确性方面取得了最高分，其中 GPT-5 在物理推理任务上表现尤为出色，而几何领域仍然是挑战最大的方向。总体而言，MMFormalizer 为统一的多模态自动形式化提供了一个可扩展的框架，有效连接了感知与形式推理。据我们所知，这是首个能够处理经典力学 (基于哈密顿量推导)、相对论、量子力学和热力学的多模态自动形式化方法。更多详细信息请访问我们的项目页面：MMFormalizer.github.io。

MAXS: Meta-Adaptive Exploration with LLM Agents
MAXS: 基于大语言模型智能体的元自适应探索
大语言模型 (LLM) 智能体通过多工具协作，具备内在的推理能力。然而，在智能体推理过程中，现有方法通常存在两个问题：(i) 由于缺乏前瞻性，导致局部短视的生成；(ii) 轨迹不稳定，即早期的微小错误可能演变为发散的推理路径。这些问题使得难以在全局有效性与计算效率之间取得平衡。为解决这两个问题，我们提出了 MAXS (Meta-Adaptive Exploration with LLM Agents，项目地址：github.com/exoskeleton…) ，这是一个基于 LLM 智能体的元自适应推理框架，能够灵活集成工具执行与推理规划。MAXS 采用前瞻策略，将推理路径向前推演若干步，以估计工具使用的优势值，并结合步骤一致性方差与步骤间趋势斜率，共同筛选出稳定、一致且高价值的推理步骤。此外，我们引入了轨迹收敛机制，一旦路径一致性达成，便停止进一步推演，从而控制计算成本，实现在多工具推理中平衡资源效率与全局有效性。我们在三个基础模型 (MiMo-VL-7B、Qwen2.5-VL-7B、Qwen2.5-VL-32B) 和五个数据集上进行了大量实验，结果表明 MAXS 在性能与推理效率上均持续优于现有方法。进一步的分析证实了我们前瞻策略与工具使用机制的有效性。

A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation
A^3-Bench：基于锚点与吸引子激活的记忆驱动科学推理基准
科学推理不仅依赖于逻辑推断，也需要激活先验知识与经验结构。记忆能够有效复用知识，提升推理的一致性与稳定性。然而，现有基准主要评估最终答案或逐步推理的连贯性，忽略了人类推理所依赖的 记忆驱动 机制，该机制通过激活锚点 (Anchor) 和吸引子 (Attractor)，并将其整合到多步推理中来实现。为填补这一空白，我们提出了 A³-Bench (a3-bench.github.io)，这是一个基于锚点与吸引子激活 (Anchor and Attractor Activation) 理论、旨在通过双尺度记忆驱动激活来评估科学推理的基准。
首先，我们采用 SAPM 流程 (即主题、锚点与吸引子、问题及记忆形成) 对跨领域的 2,198 个科学推理问题进行了标注。其次，我们引入了一个利用锚点和吸引子的双尺度记忆评估框架，并提出了 AAUI (锚点-吸引子利用指数) 指标以量化记忆激活率。最后，通过对多种基础模型及推理范式的实验，我们验证了 A³-Bench 的有效性，分析了记忆激活如何影响推理性能，从而深入理解了记忆驱动的科学推理机制。

PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning
PaCoRe: 学习通过并行协调推理扩展测试时计算量
我们提出了并行协调推理 (PaCoRe)，这是一个旨在克服当代语言模型核心局限性的训练与推理框架：即模型无法在固定上下文窗口下，将测试时计算量 (Test-Time Compute, TTC) 扩展到远超顺序推理的程度。PaCoRe 摒弃了传统的顺序范式，转而通过多轮消息传递架构协调的大规模并行探索来驱动 TTC。在每一轮中，系统启动多个并行推理轨迹，将其发现压缩成受上下文长度限制的消息，然后综合这些消息来指导下一轮推理，并最终生成答案。通过大规模、基于结果的强化学习进行端到端训练，模型掌握了 PaCoRe 所需的信息综合能力，能够将有效 TTC 扩展到数百万 token 的规模，同时不突破上下文长度限制。该方法在多个不同领域都带来了显著提升，尤其在数学推理方面超越了前沿系统：一个 80 亿参数的模型在 HMMT 2025 数据集上达到了 94.5% 的准确率，通过将有效 TTC 扩展至约两百万 token，超越了 GPT-5 的 93.2%。我们开源了模型检查点、训练数据以及完整的推理流水线，以促进后续研究。

MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences
MemGovern: 通过从经过治理的人类经验中学习增强代码智能体
尽管自主软件工程 (SWE) 智能体正在重塑编程范式，但它们目前存在一个“封闭世界”的局限：它们试图从零开始或仅依赖本地上下文来修复缺陷，而忽略了 GitHub 等平台上可用的海量历史人类经验。然而，现实世界中的问题跟踪数据往往是非结构化和碎片化的，这阻碍了智能体有效利用这些开放世界的经验。本文提出了 MemGovern 框架，旨在治理原始 GitHub 数据，并将其转化为智能体可操作的体验记忆。MemGovern 通过经验治理流程，将人类经验转换为便于智能体使用的经验卡片，并引入了一种智能体驱动的经验搜索策略，从而实现基于逻辑的人类专业知识检索。通过生成 13.5 万个经过治理的经验卡片，MemGovern 带来了显著的性能提升，在 SWE-bench Verified 基准测试中将问题解决率提高了 4.65%。作为一种插件式方案，MemGovern 为构建适配智能体的记忆基础设施提供了一种解决方案。

Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning
面向推理的协作式多智能体测试时强化学习
多智能体系统已发展成为许多实际应用中由大语言模型驱动的实用协作者，其鲁棒性得益于多样性和交叉验证。然而，多智能体强化学习 (MARL) 训练资源密集且不稳定：智能体间的协同适应会导致环境非平稳性，且奖励信号通常稀疏且方差高。为此，我们提出了 多智能体测试时强化学习 (MATTRL) 框架，该框架在推理阶段将结构化的文本经验注入多智能体的决策审议中。MATTRL 组建一个多专家团队进行多轮讨论，检索并整合测试时经验，最终达成共识以做出决策。我们还研究了信用分配机制，用于构建轮级经验池并将其重新注入对话流程。在医学、数学和教育等领域的多个挑战性基准测试上，MATTRL 的平均准确率相较于多智能体基线提升了 3.67%，相较于相应的单智能体基线提升了 8.67%。消融研究分析了不同的信用分配方案，并详细比较了它们对训练结果的影响。MATTRL 为无需额外调优、且能有效应对分布偏移的多智能体推理，提供了一条稳定、高效且有效的路径。

Motion Attribution for Video Generation
视频生成中的运动归因
尽管视频生成模型发展迅速，但数据如何影响运动仍不明确。我们提出了 Motive (MOTIon attribution for Video gEneration)，这是一个运动中心、基于梯度的数据归因框架，能够适应现代大规模高质量视频数据集和模型。我们利用该框架研究哪些微调片段会改善或损害时序动态。Motive 通过运动加权损失掩码将时序动态与静态外观分离，实现了高效且可扩展的运动特定影响力计算。在文本到视频模型上，Motive 能够识别出对运动有强烈影响的片段，并以此指导数据筛选工作，从而提升时间一致性与物理合理性。使用 Motive 精选出的高影响力数据进行微调，我们的方法在 VBench 基准上同时提升了运动平滑度与动态程度，相较于预训练基础模型，获得了 74.1% 的人类偏好胜率。据我们所知，这是首个在视频生成模型中归因于运动而非视觉外观的框架，并利用该归因结果来构建微调数据集。

Solar Open Technical Report
Solar Open 技术报告
我们介绍了 Solar Open，这是一个拥有 1020 亿参数、面向低资源语言的双语专家混合 (Mixture-of-Experts， MoE) 大语言模型。Solar Open 展示了一种通过解决三个相互关联的挑战来构建具有竞争力大语言模型的系统性方法。首先，为了应对低资源语言数据稀缺的问题以实现有效训练，我们合成了 4.5 万亿个高质量、领域特定且面向强化学习 (RL) 的 Token。其次，我们通过课程学习 (Curriculum Learning) 来协调这些数据，在总计 20 万亿 Token 的数据上，联合优化其构成、质量阈值和领域覆盖范围。第三，为了通过可扩展的强化学习来获得推理能力，我们应用了我们提出的 SnapPO 框架进行高效优化。在英语和韩语的各项基准测试中，Solar Open 都取得了具有竞争力的性能，这证明了该方法对于推动低资源语言 AI 发展的有效性。

VIBE: Visual Instruction Based Editor
VIBE: 基于视觉指令的编辑器
基于指令的图像编辑是生成式 AI (Generative AI) 中发展最迅速的领域之一。过去一年，该领域迈上了新台阶，涌现出数十个开源模型以及能力强大的商业系统。然而，目前仅有少数开源方法能达到实用级质量。此外，作为这些管线主流选择的扩散主干网络，通常体积庞大且计算成本高昂，难以适配许多部署和研究场景；其广泛使用的版本通常包含 60 亿至 200 亿参数。本文提出了一种紧凑、高吞吐的基于指令的图像编辑管线，它采用一个现代的 20 亿参数 Qwen3-VL 模型来指导编辑过程，并使用一个 16 亿参数的扩散模型 Sana1.5 进行图像生成。我们在架构、数据处理、训练配置和评估等方面的设计决策，均以低成本推理和严格的源一致性为目标，同时确保在此规模可行的主要编辑类别上保持高质量。在 ImgEdit 和 GEdit 基准测试上的评估表明，所提方法的性能匹配甚至超越了参数量大数倍、推理成本显著更高的基线模型，并且在需要保留输入图像的编辑任务上表现尤为突出，例如属性调整、对象移除、背景编辑和针对性替换。该模型可适配 24 GB 的 GPU 显存，在 NVIDIA H100 上以 BF16 精度生成高达 2K 分辨率的编辑图像仅需约 4 秒，且无需任何额外的推理优化或模型蒸馏。

Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning
面向卓越长链思维推理的分布对齐序列蒸馏
在本报告中，我们介绍了 DASD-4B-Thinking，这是一个轻量级但能力强大、完全开源的推理模型。在数学、科学推理和代码生成等具有挑战性的基准测试中，它在同等规模的开源模型中实现了 SOTA 性能，甚至优于一些更大的模型。我们首先批判性地重新审视了社区中广泛采用的一种蒸馏范式：基于教师模型生成回答进行监督微调 (SFT)，也称为序列级蒸馏 (Sequence-Level Distillation)。尽管近期一系列遵循此方案的工作展现了显著的效率和强大的实证性能，但它们主要立足于 SFT 的视角。因此，这些方法将重点放在了设计 SFT 数据过滤的启发式规则上，而在很大程度上忽略了蒸馏本身的核心原则——即让学生模型学习教师模型的完整输出分布，从而继承其泛化能力。具体而言，我们指出了当前实践中的三个关键局限：i) 对教师模型序列级分布的表征不足；ii) 教师模型的输出分布与学生模型的学习能力之间存在错配；iii) 教师强制训练 (Teacher-Forced Training) 与自回归推理 (Autoregressive Inference) 之间的差异导致的暴露偏差。总而言之，这些不足反映了在整个蒸馏过程中系统性缺乏明确的师生交互，使得蒸馏的精髓未能得到充分利用。为了解决这些问题，我们提出了几项方法学创新，它们共同构成了一个增强的序列级蒸馏训练流程。值得注意的是，DASD-4B-Thinking 仅使用 44.8 万训练样本就取得了有竞争力的结果——这比大多数现有开源工作所使用的样本量少了一个数量级。为了支持社区研究，我们公开发布了模型和训练数据集。

KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions
KnowMe-Bench: 用于终身数字伴侣的人物理解基准测试
现有的长期记忆基准测试大多使用多轮对话或合成的用户历史，这使得检索性能并不能完美地代表模型的人物理解能力。我们提出了 \BenchName，这是一个基于长篇自传叙事构建的可公开发布的基准测试。在这些叙事中，人物的行动、背景和内心思想为推断其稳定的动机和决策原则提供了丰富的证据。\BenchName~将每个叙事重构为一个具有闪回感知和时间锚定的序列，并通过一系列与证据关联的问题来评估模型，这些问题涵盖事实回忆、主观状态归因和原则级推理。在不同来源的叙事上，检索增强系统主要提升了事实准确性，但在需要时间定位的解释和更高层次的推理方面，错误仍然存在，这凸显了对超越检索的记忆机制的需求。我们的数据发布于 \href{KnowMeBench}{github.com/QuantaAlpha…

CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature
CaricatureGS: 基于高斯曲率夸张化3D高斯泼溅人脸
本文提出了一种用于人脸的照片级真实感、可控3D漫画化框架。我们首先采用一种基于内在高斯曲率的表面夸张技术，但当其与纹理结合时，渲染结果往往过于平滑。为解决此问题，我们求助于3D高斯泼溅 (3D Gaussian Splatting, 3DGS)，该技术近期已被证明能生成逼真的自由视点化身。给定一个多视角图像序列，我们提取FLAME网格，求解曲率加权的泊松方程，从而得到其夸张形式。然而，直接对3DGS中的高斯泼溅进行变形效果不佳，因此需要通过局部仿射变换将每一帧图像扭曲为其对应的夸张2D表示，以合成伪真值漫画图像。随后，我们设计了一种交替使用真实图像和合成图像进行监督的训练方案，使得单个高斯泼溅集合能够同时表征自然状态和夸张状态的化身。该方案提升了保真度，支持局部编辑，并允许对漫画夸张程度进行连续控制。为实现实时变形，我们引入了原始表面与夸张表面之间的高效插值方法，并进一步分析表明，该方法与闭式解之间的偏差是有界的。在定量与定性评估中，我们的方法均优于现有工作，能够生成几何可控、具有照片级真实感的漫画化身。

ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking
ArenaRL: 通过基于锦标赛的相对排名实现开放式智能体的强化学习规模化
强化学习已显著提升了大语言模型智能体在结果可验证任务上的性能，但在解空间广阔的开放式智能体任务（例如复杂旅行规划）上仍举步维艰。由于此类任务缺乏客观的真值，当前的强化学习算法主要依赖于为单个响应分配标量分数的奖励模型。我们认为，这种逐点评分法存在固有的区分度崩溃问题：奖励模型难以辨别不同轨迹间的细微优势，导致组内分数被压缩至一个狭窄区间。因此，有效的奖励信号被奖励模型本身的噪声所主导，进而引发优化停滞。为解决此问题，我们提出了 ArenaRL，这是一种将评估方式从逐点标量评分转变为组内相对排名的强化学习范式。ArenaRL 引入了一种感知任务过程的成对评估机制，采用多级评分标准为轨迹分配细粒度的相对分数。此外，我们构建了一个组内对抗竞技场，并设计了一套基于锦标赛的排名方案，以获取稳定的优势信号。实证结果表明，所构建的采用种子排位的单败淘汰制，在仅需 O(N) 复杂度的情况下，其优势估计精度与需要 O(N^2) 复杂度的完全成对比较几乎相当，从而在效率与精度之间达到了最优平衡。再者，为弥补开放式智能体缺乏全流程评测基准的不足，我们构建了 Open-Travel 和 Open-DeepResearch 两个高质量基准，它们具备覆盖监督微调、强化训练及多维评估的完整流程。大量实验表明，ArenaRL 显著优于标准强化学习基线，能够使大语言模型智能体为复杂的现实世界任务生成更为稳健的解决方案。

The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning
思想的分子结构：映射长链思维推理的拓扑结构
大语言模型 (LLMs) 常常难以通过模仿人类或非长链思维 (Long CoT) 大语言模型来学习有效的长链思维 (Long CoT) 推理。为探究其原因，我们提出，在统一视角下，有效且可学习的长链思维轨迹具有类似分子的稳定结构，这些结构由三种相互作用构成：深度推理 (类共价键) 、自我反思 (类氢键) 和自我探索 (类范德华力) 。对蒸馏轨迹的分析表明，这些结构源自长链思维微调过程，而非对关键词的简单模仿。我们引入了有效语义异构体的概念，并证明只有那些能促进快速熵收敛的化学键才能支持稳定的长链思维学习，而不同结构之间的竞争则会损害训练效果。基于这些发现，我们提出了 Mole-Syn，一种基于分布转移图的方法，用于指导合成有效的长链思维结构，从而在多个基准测试中提升模型性能并增强强化学习的稳定性。

User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale
面向用户的大规模多轮对话生成与工具使用
近期，将大型推理模型 (Large Reasoning Models, LRMs) 作为自主智能体的范式转变，极大地提升了对复杂多轮工具使用能力的需求。然而，现有数据集与数据生成方法受限于静态、预定义的工具集，难以应对开放式人机协作场景的复杂性。为此，我们首先构建了一个框架，用于大规模自动化生成面向任务的多轮对话。该框架利用基于 LRM 的模拟器动态生成高价值、领域特定的工具，以解决指定任务。但我们发现，纯粹面向任务的设计往往导致"仅限任务解决"的轨迹，即智能体以最少的交互完成目标，无法生成现实场景中常见的多轮次对话。为弥补这一不足，我们转向了面向用户的模拟范式。通过将任务生成与一个专用的用户模拟器解耦——该模拟器模仿人类行为规则，如增量式提出请求和逐轮提供反馈——我们能够促成更真实、更扩展的多轮对话，从而反映现实世界问题解决的迭代特性。我们的生成流水线作为一个多功能、即插即用的模块运行，可从任意状态启动生成，确保了在产出扩展工具使用数据时的高可扩展性。此外，通过支持在单条轨迹内完成多个任务，该流程能够生成高密度数据集，以反映现实世界人机交互的多方面需求。

Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning
Fast-ThinkAct: 基于可语言化潜在规划的高效视觉-语言-动作推理
视觉-语言-动作 (Vision-Language-Action, VLA) 任务要求对复杂视觉场景进行推理，并在动态环境中执行自适应的动作。尽管近期关于推理型 VLA 的研究表明，显式的思维链 (Chain-of-Thought, CoT) 能够提升泛化能力，但其冗长的推理轨迹导致了较高的推理延迟。我们提出了 Fast-ThinkAct，一个高效的推理框架，它通过可语言化的潜在推理，实现了紧凑且高性能的规划。Fast-ThinkAct 通过从教师模型进行知识蒸馏，学习利用潜在 CoT 进行高效推理。该方法由一个偏好引导的目标驱动，旨在对齐操作轨迹，从而迁移用于具身控制的语言和视觉规划能力。这实现了推理增强的策略学习，能够有效地将紧凑的推理与动作执行相衔接。在多种具身操作与推理基准上进行的大量实验表明，Fast-ThinkAct 取得了优异的性能，与最先进的推理型 VLA 相比，推理延迟最高可降低 89.3%，同时仍能有效进行长时程规划、少样本适应和故障恢复

## 每周AI论文速递（260119-260123）
Agentic Reasoning for Large Language Models
面向大语言模型的智能体推理
推理是支撑推断、问题解决与决策制定的基本认知过程。尽管大语言模型 (LLMs) 在封闭环境设定下展现出强大的推理能力，但在开放、动态的环境中却表现欠佳。智能体推理 (Agentic Reasoning) 标志着一种范式转变，它将大语言模型重构为能够通过持续交互进行规划、行动和学习的自主智能体。本综述从三个互补的维度来梳理智能体推理。首先，我们通过三个层级来刻画环境动态：基础智能体推理，它在稳定环境中建立核心的单智能体能力，包括规划、工具使用和搜索；自我进化智能体推理，它研究智能体如何通过反馈、记忆和适应来完善这些能力；以及集体多智能体推理，它将智能延伸至涉及协调、知识共享和共同目标的协作场景。在这些层级中，我们区分了上下文推理（通过结构化编排来扩展测试时的交互）与训练后推理（通过强化学习和监督微调来优化行为）。我们进一步回顾了跨越现实世界应用与基准测试的代表性智能体推理框架，涵盖科学、机器人、医疗保健、自主研究与数学等领域。本综述将各类智能体推理方法综合成一个连接思维与行动的统一路线图，并概述了开放的挑战与未来方向，包括个性化、长周期交互、世界模型建模、可扩展的多智能体训练以及实际部署的治理机制。

Your Group-Relative Advantage Is Biased
群体相对优势估计存在偏差
基于验证器奖励的强化学习 (Reinforcement Learning from Verifier Rewards, RLVR) 已成为对大语言模型进行推理任务后训练的一种广泛应用方法，其中基于群体的方法，如 GRPO 及其变体，得到了广泛采用。这些方法依赖于群体相对优势估计 (group-relative advantage estimation) 来避免使用学习到的评论家，但其理论性质仍不明确。
本工作揭示了基于群体的强化学习的一个根本问题：群体相对优势估计量相对于真实（期望）优势存在固有偏差。我们首次进行了理论分析，证明该估计量会系统性地低估困难提示的优势，同时高估简单提示的优势，从而导致探索与利用的失衡。为解决此问题，我们提出了历史感知自适应难度加权 (History-Aware Adaptive Difficulty Weighting, HA-DW)，这是一种自适应重加权方案，能够根据一个动态变化的难度锚点和训练过程动态来调整优势估计。在五个数学推理基准测试上的理论分析与实验均表明，将 HA-DW 集成到 GRPO 及其变体中能持续提升性能。我们的结果表明，纠正有偏差的优势估计对于实现稳健、高效的 RLVR 训练至关重要。

Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization
Being-H0.5: 面向跨形态泛化的以人为中心机器人学习规模化
我们提出了 Being-H0.5，这是一个基础视觉-语言-动作（Vision-Language-Action, VLA）模型，旨在实现跨多样化机器人平台的鲁棒跨形态泛化。针对现有 VLA 模型常受限于形态异构性与数据稀缺的问题，我们提出了一种以人为中心的学习范式，将人类交互轨迹视为物理交互的通用“母语”。为此，我们推出了 UniHand-2.0，这是迄今为止规模最大的具身预训练方案，包含了跨越 30 种不同机器人形态的超过 35,000 小时多模态数据。我们的方法引入了一个统一动作空间，将异构的机器人控制映射到语义对齐的槽位，从而使低资源机器人能够从人类数据和高资源平台中自举学习技能。基于此以人为中心的基础，我们设计了一个统一的序列建模与多任务预训练范式，以弥合人类演示与机器人执行之间的差距。在架构上，Being-H0.5 采用了一种混合 Transformer（Mixture-of-Transformers）设计，其核心是新颖的混合流（Mixture-of-Flow, MoF）框架，用于将共享的运动基元与专门的形态特定专家解耦。最后，为确保跨形态策略在现实世界中的稳定性，我们引入了流形保持门控（Manifold-Preserving Gating）以应对感知变化下的鲁棒性挑战，以及通用异步分块（Universal Async Chunking）以实现跨不同延迟与控制特性的机器人形态的通用分块控制。实验结果表明，Being-H0.5 在模拟基准测试（如 LIBERO (98.9%) 和 RoboCasa (53.9%)）上取得了最先进的性能，同时在五个机器人平台上展现出强大的跨形态能力。

EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience
EvoCUA: 通过从可扩展合成经验中学习演进计算机使用智能体
原生计算机使用智能体 (CUA) 的发展是多模态 AI 领域的一次重大飞跃。然而，其潜力目前受限于静态数据扩展的瓶颈。主要依赖对静态数据集进行被动模仿的现有范式，难以捕捉长程计算机任务中固有的复杂因果动态。本文中，我们提出了 EvoCUA，一个原生计算机使用智能体模型。与静态模仿不同，EvoCUA 将数据生成与策略优化整合为一个自我维持的演进循环。为缓解数据稀缺问题，我们开发了一个可验证的合成引擎，能自主生成多样化任务并附带可执行的验证器。为实现大规模经验获取，我们设计了一个可扩展的基础设施，可协调数万个异步沙盒模拟运行。基于这些大规模轨迹，我们提出了一种迭代演进学习策略，以高效吸收这些经验。该机制通过识别能力边界来动态调节策略更新——强化成功的行为模式，同时通过错误分析与自我纠正将失败轨迹转化为丰富的监督信号。在 OSWorld 基准测试上的实证评估表明，EvoCUA 实现了 56.7% 的成功率，创造了新的开源模型最佳性能。值得注意的是，EvoCUA 显著优于此前最佳的开源模型 OpenCUA-72B (45.0%)，并且超越了领先的闭源权重模型，如 UI-TARS-2 (53.1%)。关键的是，我们的结果证明了该方法的泛化能力：这种由经验学习驱动的演进范式，在不同规模的基础模型上均能实现一致的性能提升，从而为增强原生智能体能力开辟了一条稳健且可扩展的路径。

ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development
ABC-Bench: 面向真实世界开发环境的智能体后端编码基准测试
大语言模型 (LLMs) 向自主 AI 智能体 (AI Agent) 的演进，已将人工智能 (AI) 编码的范畴从局部代码生成，扩展至复杂的、仓库级别的、由执行驱动的问题求解。然而，现有的基准测试主要评估静态上下文中的代码逻辑，忽视了真实世界工程项目中动态的、全流程的需求，尤其是在需要严格环境配置与服务部署的后端开发领域。为弥补这一不足，我们提出了 ABC-Bench，这是一个专为在真实、可执行的工作流中评估智能体后端编码能力而设计的基准测试。通过一个可扩展的自动化流水线，我们从开源仓库中构建了 224 个实际任务，涵盖 8 种编程语言和 19 个框架。与以往的评估不同，ABC-Bench 规定智能体必须管理从仓库探索到部署容器化服务的完整开发生命周期，并且要通过外部的端到端 API 测试。我们的大量评估结果表明，即便是最先进的模型，在面对这些综合性任务时也难以提供稳定可靠的性能，这凸显了当前模型能力与实际后端工程需求之间存在的显著差距。我们的代码开源在 github.com/OpenMOSS/AB…

HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding
HERMES: 将 KV 缓存作为分层内存以实现高效流式视频理解
多模态大语言模型 (Multimodal Large Language Models, MLLMs) 的最新进展，在离线视频理解任务上取得了显著进步。然而，将其能力扩展至流式视频输入仍面临挑战，因为现有模型难以在保持稳定理解性能的同时，兼顾实时响应与较低的 GPU 内存开销。为应对这一挑战，我们提出了 HERMES，一种新颖的免训练架构，旨在实现对视频流的实时、准确理解。基于对注意力机制的机理探究，我们将 KV 缓存 (KV Cache) 概念化为一个分层内存框架，该框架能以多种粒度封装视频信息。在推理过程中，HERMES 通过重用紧凑的 KV 缓存，在有限资源下实现了高效的流式理解。值得注意的是，HERMES 在用户查询到达时无需任何辅助计算，从而确保了连续视频流交互的实时响应能力，其首次令牌生成时间 (Time To First Token, TTFT) 比之前的 SOTA 方法快 10 倍。即使与均匀采样相比，视频令牌数量减少了高达 68%，HERMES 在所有基准测试中仍取得了相当或更优的准确率，在流式数据集上的性能提升最高达 11.4%。

Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey
基于大语言模型的软件工程问题解决：进展与前沿综述
问题解决是现实软件开发中一项不可或缺的复杂软件工程任务，现已成为人工智能面临的一项重大挑战。SWE-bench等基准测试的建立表明，此项任务对大语言模型而言极具难度，从而极大地推动了自主编码智能体的演进。本文对这一新兴领域进行了系统性综述。首先，我们考察了数据构建流程，包括自动收集与合成方法。接着，我们全面分析了相关方法，范围从具备模块化组件的免训练框架，到基于训练的技术（如监督微调和强化学习）。随后，我们对数据质量和智能体行为进行了批判性分析，并探讨了实际应用。最后，我们指出了当前面临的关键挑战，并展望了未来有前景的研究方向。我们在 github.com/DeepSoftwar… 维护了一个开源仓库，作为该领域的动态资源。

The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models
灵活性陷阱：为何任意顺序会限制扩散语言模型的推理潜力
扩散大语言模型 (Diffusion Large Language Models, dLLMs) 打破了传统大语言模型严格的从左到右约束，允许以任意顺序生成 Token。直观上看，这种灵活性意味着其解空间严格包含了固定的自回归轨迹，理论上能为数学和编码等通用任务释放更强大的推理潜力。因此，许多研究都利用强化学习 (Reinforcement Learning, RL) 来挖掘 dLLMs 的推理能力。本文揭示了一个反直觉的事实：在当前形式下，任意顺序生成非但没有扩大，反而缩小了 dLLMs 的推理边界。我们发现，dLLMs 倾向于利用这种顺序灵活性来规避对探索至关重要的高不确定性 Token，从而导致解空间过早坍缩。这一发现挑战了现有 dLLMs 强化学习方法的前提，这些方法通常为了保持这种灵活性而引入了相当大的复杂性，例如处理组合轨迹和难解的似然问题。我们证明，通过有意放弃任意顺序，转而应用标准的组相对策略优化 (Group Relative Policy Optimization, GRPO)，可以更有效地激发推理能力。我们的方法 JustGRPO 设计极其简洁，却效果惊人（例如，在 GSM8K 上达到 89.1% 的准确率），同时完全保留了 dLLMs 的并行解码能力。项目页面：nzl-thu.github.io/the-flexibi…

RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation
RubricHub: 通过自动化由粗到细生成构建的全面高区分度评分标准数据集
具有可验证奖励的强化学习 (Reinforcement Learning with Verifiable Rewards, RLVR) 在数学等推理密集型领域已取得重大进展。然而，由于缺乏真实标签，优化开放式生成任务仍面临挑战。基于评分标准的评估虽为验证提供了一种结构化替代方案，但现有方法受限于可扩展性瓶颈和粗糙的评判标准，导致了监督性能瓶颈。为解决此问题，我们提出了一种自动化的由粗到细评分标准生成框架。该框架协同利用原则引导的合成、多模型聚合与难度演化，能够生成全面且高区分度的评判标准，从而捕捉生成内容中的细微差别。基于此框架，我们发布了 RubricHub，这是一个大规模（约 11 万条）且覆盖多领域的数据集。我们通过一个两阶段的后训练流程验证了其有效性，该流程包含基于评分标准的拒绝采样微调 (Rubric-based Rejection Sampling Fine-Tuning, RuFT) 和强化学习 (Rubric-based Reinforcement Learning, RuRL)。实验结果表明，RubricHub 能显著提升模型性能：经后训练的 Qwen3-14B 模型在 HealthBench 基准上取得了最先进 (state-of-the-art, SOTA) 的性能（69.3 分），超越了 GPT-5 等专有的前沿模型。相关代码与数据即将发布。

LLM-in-Sandbox Elicits General Agentic Intelligence
LLM-in-Sandbox 激发通用智能体能力
我们提出了 LLM-in-Sandbox 方法，使大语言模型能够在代码沙盒（即虚拟计算机）内进行探索，从而在非代码领域激发通用智能。我们首先证明，强大的大语言模型无需额外训练，即可展现出利用代码沙盒处理非代码任务的泛化能力。例如，大语言模型能够自主访问外部资源以获取新知识，利用文件系统处理长上下文，并执行脚本来满足特定格式要求。我们进一步表明，这些 AI 智能体能力可以通过 LLM-in-Sandbox 强化学习（LLM-in-Sandbox-RL）得到增强，该方法仅使用非智能体行为数据来训练模型进行沙盒探索。实验表明，LLM-in-Sandbox 在免训练和训练后两种设置下，均能实现稳健的泛化，其能力覆盖数学、物理、化学、生物医学、长上下文理解及指令遵循等多个领域。最后，我们从计算和系统两个角度分析了 LLM-in-Sandbox 的效率，并将其开源为一个 Python 软件包，以促进实际应用部署。

BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries
BayesianVLA: 基于潜在动作查询的视觉-语言-动作模型贝叶斯分解
视觉-语言-动作 (Vision-Language-Action, VLA) 模型在机器人操作任务中展现出潜力，但其泛化能力常受限于新指令或复杂的多任务场景。我们指出当前训练范式存在一个关键缺陷：目标驱动的数据收集导致了数据集偏差。在此类数据集中，仅凭视觉观察就足以高度预测出语言指令，致使指令与动作之间的条件互信息趋于零，我们将此现象称为信息坍缩 (Information Collapse)。其结果是，模型退化为仅依赖视觉的策略，忽略了语言约束，从而在分布外 (Out-of-Distribution, OOD) 场景中失效。为解决此问题，我们提出了 BayesianVLA，这是一个通过贝叶斯分解来确保模型遵循指令的新框架。通过引入可学习的潜在动作查询 (Latent Action Queries)，我们构建了一个双分支架构，分别估计仅视觉先验 p(a∣v)p(a \mid v)p(a∣v) 和语言条件后验 π(a∣v,ℓ)π(a \mid v, \ell)π(a∣v,ℓ)。随后，我们优化策略以最大化动作与指令之间的条件点互信息 (Pointwise Mutual Information, PMI)。该目标有效地抑制了视觉捷径，并奖励那些能明确体现语言指令的动作。BayesianVLA 无需额外数据即可显著提升泛化性能。在 SimplerEnv 和 RoboCasa 上进行的大量实验证明了其显著的性能提升，其中在极具挑战性的 OOD SimplerEnv 基准测试上实现了 11.3% 的性能增益，验证了我们的方法能够稳健地将语言关联到动作。

Toward Efficient Agents: Memory, Tool learning, and Planning
迈向高效智能体：记忆、工具学习与规划
近年来，将大语言模型扩展为智能体 (AI Agent) 系统的研究兴趣日益浓厚。尽管智能体的有效性在持续提升，但对于实际部署至关重要的效率却常被忽视。因此，本文从智能体的三个核心组件——记忆、工具学习和规划——出发，研究其效率问题，并综合考虑延迟、Token 消耗、步骤数等成本。为了对智能体系统本身的效率进行全面研究，我们回顾了近期的一系列方法。这些方法在具体实现上各异，但在高级设计原则上往往趋同，包括但不限于：通过压缩和管理来限制上下文、设计强化学习奖励以最小化工具调用，以及采用受控搜索机制来提升效率。我们将对这些原则进行详细讨论。
相应地，我们从两个互补的维度来刻画效率：一是在固定成本预算下比较其有效性；二是在达到可比有效性水平时比较其成本消耗。这种权衡关系也可以从有效性与成本之间的帕累托前沿 (Pareto frontier) 来理解。基于此视角，我们还审视了面向效率的基准评测：通过总结针对上述组件的评估方案，并整合来自基准研究和方法论文献中常报告的各项效率指标。
此外，我们讨论了该领域面临的关键挑战与未来研究方向，旨在为相关研究提供有价值的见解。

MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents
MMDeepResearch-Bench：多模态深度研究智能体基准
深度研究智能体 (Deep Research Agents, DRAs) 通过多步骤搜索与信息合成来生成包含丰富引用的报告。然而，现有基准主要面向纯文本场景或短格式多模态问答，缺乏对端到端多模态证据使用的评估。为此，我们提出了 MMDeepResearch-Bench (MMDR-Bench)，这是一个包含 21 个领域、共计 140 项由专家精心设计任务的基准。每项任务提供一个图文数据包 (image-text bundle)，用于评估模型的多模态理解能力以及基于引用的报告生成能力。与以往的设置相比，MMDR-Bench 强调具备明确证据使用的报告式合成，要求模型必须将视觉内容与有来源支撑的论断相关联，并在叙述、引用和视觉参考之间保持一致性。我们进一步提出了一套统一且可解释的评估流程：用于评估报告质量的公式化-LLM自适应评估 (Formula-LLM Adaptive Evaluation, FLAE)，用于评估引用与证据对齐的可信检索对齐引用评估 (Trustworthy Retrieval-Aligned Citation Evaluation, TRACE)，以及用于检查文本-视觉一致性的多模态支持对齐完整性检查 (Multimodal Support-Aligned Integrity Check, MOSAIC)。每个评估环节都能产生细粒度的指标，支持进行超越单一总分的精细化错误诊断。我们在 25 个前沿模型上进行了实验，结果揭示了生成质量、引用规范性与多模态基础之间的系统性权衡。这些发现突出表明，仅能生成流畅的文本并不能保证对证据的忠实使用，并且多模态一致性仍然是深度研究智能体面临的一个关键瓶颈。

The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents
毒苹果效应：通过 AI 智能体技术扩张对中介市场进行战略操纵
AI 智能体 (AI Agents) 融入经济市场，从根本上改变了战略互动的格局。我们在三个经典的博弈论场景中，研究了扩展可用技术集合所带来的经济影响，这些场景包括：议价 (资源分配)、谈判 (非对称信息交易) 以及说服 (战略信息传递)。研究发现，仅仅增加可供选择的 AI 智能体，就能显著改变均衡收益和监管结果，这常常会激励监管机构主动开发和发布新技术。与之相对，我们识别出一种被称为“毒苹果”效应的战略现象：某个智能体可能会发布一项新技术，而这项技术最终既不会被其自身采用，也不会被其对手采用，其唯一目的在于操纵监管机构，使其选择有利于该智能体的市场设计方案。这种战略性发布行为，以牺牲对手利益和违背监管机构公平目标为代价，提升了发布者自身的福利。我们的研究结果表明，静态的监管框架易受技术扩张的操纵，因此需要能够适应 AI 能力动态发展的市场设计。



