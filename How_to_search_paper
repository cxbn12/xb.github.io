1) AI 主流期刊/会议（按领域分层）

计算机视觉（CV）
	•	顶会：CVPR / ICCV / ECCV
	•	顶刊：TPAMI（IEEE Transactions on Pattern Analysis and Machine Intelligence）

机器学习（ML）
	•	顶会：NeurIPS / ICML / ICLR
	•	顶刊：JMLR（Journal of Machine Learning Research）
	•	近年也常用“开放期刊轨道”：TMLR（Transactions on Machine Learning Research，JMLR 系）

人工智能综合（AI）
	•	老牌强刊：AIJ（Artificial Intelligence, Elsevier）
	•	顶会（偏 AI 综合/应用）：AAAI / IJCAI（你没列，但也很主流）

机器人（Robotics）
	•	顶会：ICRA / IROS
	•	顶刊：T-RO（IEEE Transactions on Robotics）等

⸻

2) 各 venue 怎么“只搜该 venue 的 VQA/MLLM 论文”

A) CVPR/ICCV/ECCV：用 CVF Open Access 最干净

入口：openaccess.thecvf.com
检索建议（Google/Scholar 都能套）：
	•	site:openaccess.thecvf.com CVPR2025 VQA
	•	site:openaccess.thecvf.com CVPR2024 "Question Answering"
	•	site:openaccess.thecvf.com ICCV2025 VQA
	•	关键词扩展：VQA, "visual question answering", "question answering", KBVQA, DocVQA, TextVQA, VideoQA, Audio-Visual Question Answering, Embodied Question Answering

B) NeurIPS：优先用 OpenReview accept tabs 或 NeurIPS 官方 virtual
	•	OpenReview（你截图那个）：NeurIPS 2025 → Accept(oral/spotlight/poster) 内搜索，结果=主会论文（最权威）  ￼
	•	NeurIPS Proceedings（更适合已归档年份，例如 2024）：proceedings.neurips.cc  ￼
	•	NeurIPS Virtual（也能筛）：neurips.cc/virtual/2025/papers.html  ￼

C) ICML：用 PMLR Proceedings
	•	site:proceedings.mlr.press ICML 2024 "visual question answering"
（PMLR 对“是否 ICML 正刊”标注很清晰）  ￼

D) ICRA：主会论文可用 IEEE/官方 proceedings（但检索入口分散）

你也可以用 OpenReview（部分 workshop/track 在 OpenReview 维护）  ￼
如果你要“ICRA 主会全量 VQA”，建议用 ICRA 官方/IEEE 入口配合关键词搜索（有时会受 robots 限制）。

E) TPAMI / AIJ / JMLR
	•	TPAMI：主要在 IEEE Xplore（有 robots 限制时要换入口/用 Google + DOI），我这边直接访问 IEEE Xplore 被 robots 拦截了（所以我不能在这里给你直接可引用的全量清单）。
	•	JMLR：JMLR 主刊不太会“专门发 VQA”，但 TMLR（OpenReview 体系）可以直接在 JMLR/TMLR 页面搜到 VQA 相关论文条目。比如 TMLR 页面里就有明确标题含 VQA 的论文列表项  ￼
	•	AIJ：通常用出版社页面+Google 限定检索（AIJ 的 VQA 论文数量一般不会像 CVPR/ICCV 那么多）。

⸻

3) 先给你一份“已确认 venue”的 2024–2025 VQA/MLLM 相关样例清单

CVPR 2024（CVF Open Access）
	•	MoReVQA: Exploring Modular Reasoning Models for Video Question Answering  ￼
	•	OpenEQA: Embodied Question Answering in the Era of Foundation Models  ￼
	•	VTQA: Visual Text Question Answering via Entity Alignment…  ￼
	•	CoG-DQA: Chain-of-Guiding Learning … for Diagram Question Answering  ￼
	•	Grounded Question-Answering in Long Egocentric Videos  ￼

CVPR 2025（CVF Open Access）
	•	Marten: Visual Question Answering with Mask Generation for Multi-modal Document…  ￼
	•	AVQACL: Audio-Visual Question Answering Continual Learning benchmark  ￼
	•	FRAMES-VQA: Benchmarking Fine-Tuning Robustness…  ￼
	•	Separation of Powers: KBVQA… segregating knowledge from observation  ￼

ICCV 2025（CVF Open Access）
	•	ReasonVQA: Multi-hop Reasoning Benchmark with Structural Knowledge for VQA  ￼
	•	ToolVQA: Multi-step Reasoning VQA with External Tools  ￼
	•	Acknowledging Focus Ambiguity in Visual Questions (VQ-FocusAmbiguity)  ￼
	•	（你截图里那个 ICCVW 医疗 VQA agent 框架）A Dynamic Agent Framework for LLM Reasoning for Medical & Visual QA (ICCVW 2025)  ￼

NeurIPS 2024（Proceedings）
	•	LOVA3: Learning tO Visual Question Answering, Asking and Assessment (MLLM 训练任务设计)  ￼
	•	Overcoming Biases for Audio-Visual Question Answering  ￼
	•	CVQA: Culturally-diverse Multilingual VQA dataset  ￼

NeurIPS 2025（OpenReview Accept）
	•	Wiki-PRF: Knowledge-based Visual Question Answer with Multimodal RAG（NeurIPS 2025 poster，OpenReview 页面会显示 Published/track）  ￼
	•	CoralVQA（NeurIPS 2025 Datasets & Benchmarks track 页面能看到）  ￼
	•	NeurIPS 2025 Virtual 列表里也能检索到 VQA benchmark（例如 WearVQA）  ￼

ICML 2024（PMLR，VQA 作为评测任务出现）
	•	Improving Context Understanding in Multimodal LLMs via Multimodal Composition Learning（文中包含 VQA 文本生成评测）  ￼

ICRA 2024（示例：VQA in Robotic Surgery）
	•	GitHub 项目页明确写了 “accepted by IEEE ICRA 2024” 的 VQA in Robotic Surgery 工作（可用来反向定位论文）  ￼

JMLR/TMLR（不是“会议”，但你问到 JMLR）
	•	TMLR 页面里可直接看到标题含 “Visual Question Answering” 的论文条目（例如 D3: Data Diversity Design… for Systematic Generalization in VQA）  ￼

⸻

4) 你接下来要我“真正抽取 NeurIPS-only 2024–2025（从 accept tabs）”怎么做？

我可以做，但需要你给我其中一个：
	•	你 NeurIPS OpenReview 的 链接（比如 NeurIPS 2024/2025 的 accept(poster) 页面 URL），或
	•	你已经在页面里用搜索框搜到 VQA 的结果列表截图（多几页），或
	•	你希望的关键词范围（只要 VQA/visual question answering，还是包含 question answering / VideoQA / AVQA / KBVQA）

因为 OpenReview 的 accept 列表很长，“VQA/MLLM 边界”你可以设严格或宽松；我可以按你现在的使用习惯：只取标题/摘要明确是 VQA/QA 的（最干净）。
